{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression With PyTorch\n",
    "\n",
    "## About Logistic Regression\n",
    "\n",
    "### Logistic Regression Basics\n",
    "\n",
    "*** Classification Algorithm ***\n",
    "\n",
    "* Example : Spam Vs Not Spam\n",
    "    * Input : Bunch of words\n",
    "    * Output : Probability spam or not\n",
    "    \n",
    "*** Basic Comparison ***\n",
    "\n",
    "* Linear Regression\n",
    "    * Output : Numeric Value given inputs\n",
    "* Logistic Regression\n",
    "    * Output : probability [0,1] given input belonging to a class\n",
    "\n",
    "*** Input/Output comparison ***\n",
    "\n",
    "*** - Linear Regression Multiplication ***\n",
    "    * input :[1]\n",
    "        * Output :[2]\n",
    "    \n",
    "    * input :[2]\n",
    "        * Output : [4]\n",
    "* Here we are trying to model the relationship  y=2x\n",
    "\n",
    "*** - Logistic Regression Spam ***\n",
    "    * p: probability it's spam\n",
    "\n",
    "    * Input : 'Sign up to get 1 million tonight'\n",
    "        * Output p = 0.8\n",
    "    * Input : 'This is the receipt from your recent purchase from flipkart'\n",
    "        * Output p = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems of Linear Regresssion\n",
    "\n",
    "* Example \n",
    "    * Fever \n",
    "    * Input : temperature\n",
    "    * Output : ferver or no fever \n",
    "    \n",
    "* Remember \n",
    "    * Linear regression : minimize error between points and line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21405d5e5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1 ,5, 10, 10, 25, 50, 70, 75, 100]\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1 ]\n",
    "\n",
    "colors = np.random.rand(len(x))\n",
    "plt.plot(np.unique(x),np.poly1d(np.polyfit(x,y,1))(np.unique(x)))\n",
    "plt.ylabel(\"Fever\")\n",
    "plt.xlabel(\"Temperature \")\n",
    "plt.scatter(x,y , c=colors ,alpha =0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Linear Regression Probelm 1 ***\n",
    "\n",
    "Fever value can go negative(below 0) and positive(above 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8VfW57/HPkxlCGELCTAhDABEEIaLV4shkbaWtVXE6tdba29ZOR2099/T2eNpz7gWcW0fq2OHU2slDT2VSGZxQQMEBkxAIQ5gSAmSe93P/2Js0xkBCzM7O8H2/Xnmx11q/rDyLleSbtdaz1zJ3R0REBCAq0gWIiEjnoVAQEZEGCgUREWmgUBARkQYKBRERaaBQEBGRBgoFERFpoFAQEZEGCgUREWkQE+kCTlVKSoqnp6dHugwRkS5l8+bNh909taVxXS4U0tPT2bRpU6TLEBHpUsxsd2vG6fSRiIg0UCiIiEgDhYKIiDRQKIiISAOFgoiINFAoiIhIA4WCiIg0UCiIiHRC1XX1PP16Hq9uL+zQr9vl3rwmItKdBQLOsq37uWdVNvlHK7n+nDRmZbT4RuR2o1AQEekE3J11OYUsXpHNRwdKmDS0L8/eNIXzM1I6tA6FgohIhG3Ze4xFyz9iw84jjEzuxYMLp/GFM4YRFWUdXotCQUQkQnYWlnHPqmxefP8gAxPjuOsLk7j27FHExUTucq9CQUSkgxWUVPHAy9v5w8a9xMdE8f1LMvjG+WPoEx/5X8mRr0BEpIcoqarl8XU7ePK1POrqnevPTuPWizNITYqPdGkNFAoiImFWVVvPbzfs5qE1uRyrqOXyqcO4be54Rg1MjHRpn6BQEBEJk/qA89d393H/6hz2HatkVkYKP54/kcnD+0W6tBNSKIiItDN3Z012AYuXZ5N9qJQpw/ux+Ioz+GwHt5e2hUJBRKQdbd59lMXLs3h71xFGDezNQ9eeyecmD41Ie2lbKBRERNpBbkEpS1Zks2rbIVL6xPPzBaezcGYasdFd625CYQsFM3sK+DxQ4O6Tm1l+HfDj0GQZ8C133xquekREwuFAcSUPrN7OHzfvpXdcDLfNGc9Nnx1NYidoL22LcFb9DPAQ8OsTLM8DLnD3o2Z2KbAUODuM9YiItJviiloeWZfLM6/vIuDOV89N59aLxjGwT+dpL22LsIWCu683s/STLH+j0eQGYES4ahERaS9VtfU8+8YuHl6TS2l1HV+cNpx/njOekcm9I11au+gsxzdfB5ZHuggRkROpqw/wl3f2cf9LORworuLCCan8aN5EJg3rG+nS2lXEQ8HMLiIYCp89yZhbgFsA0tLSOqgyEZFge+nqbYdYsjKb3IIypo7sz31XTeMzYwdGurSwiGgomNkZwBPApe5edKJx7r6U4DUHMjMzvYPKE5Ee7u28IyxekcXm3UcZk5LIY9dPZ97pQzDrGu2lbRGxUDCzNOAvwA3unhOpOkREmso+WMqSFVm8nFXAoKR4/u+XpnBV5ghiulh7aVuEsyX198CFQIqZ5QP/BsQCuPtjwE+BgcAjodStc/fMcNUjItKSfccquX91Dn9+J58+8THcMW8CN503ml5x0ZEurcOEs/vomhaW3wzcHK6vLyLSWkfLa3hkbS7PvrkbHG7+7Gi+feE4BiTGRbq0DhfxC80iIpFSWVPPU6/n8djaHZTV1HHF9BH8cM54hvfvFenSIkahICI9Tl19gOc35fPASzkUlFYz+7RB3DFvIhOGJEW6tIhTKIhIj+HurPjgIHevymZnYTnT0/rz0LXTmTk6OdKldRoKBRHpEd7cUcSiFVls3XuMcYP6sPSGGcyZNLhbt5e2hUJBRLq1bftLWLIyi7XZhQzpm8CSK87gy9OH94j20rZQKIhIt7T3SAX3rc7hhS37SIqP4V8unchXz00nIbbntJe2hUJBRLqVorJqHlqTy+827MEMvnn+WL51wVj69Y6NdGldgkJBRLqF8uo6nnwtj6Xrd1JRU8eVM0bygzkZDO3Xc9tL20KhICJdWm19gOfe3sODL+dyuKyauZMG86P5Exg3SO2lbaFQEJEuKRBw/v7+Ae5dlc2uogpmpifz+A0zmDFqQKRL69IUCiLS5byee5hFy7N4f18xEwYn8dSNmVw0YZDaS9uBQkFEuowP9hWzeEUWr24/zPD+vbj3yql88czhREcpDNqLQkFEOr3dReXcsyqHv23dT//esfzkstO4/pxRai8NA4WCiHRahaXVPPTKdn731h5ioo3vXDSWb14wlr4Jai8NF4WCiHQ6ZdV1LF2/kyde3Ul1XYCrzxrJ9y/JYHDfhEiX1u0pFESk06ipC/Bfb+3ml6/kUlRew+emDOG2uRMYm9on0qX1GAoFEYm4QMD523v7uWdVNnuPVHLOmGSevPQ0po3sH+nSehyFgohEjLuzfvthFi/PYtuBEk4b2pdnvjaZC8anqr00QhQKIhIRW/ceY/GKLN7YUcSIAb144OppXD51GFFqL40ohYKIdKi8w+XcszKbv79/gOTEOP7tC5O49uw04mPUXtoZKBREpEMUlFTx4MvbeW7jXuJjovjeJRl8Y9ZoktRe2qmELRTM7Cng80CBu09uZrkBDwKfAyqAG939nXDV053Uez2Hqg5SE6gmOS6FvrF9P9X66mrryM85QHVFNYPTU+mf2o+6QD37KwuoDtQwOGEgfWPV/SFtU1JVy9J1O3nytTxq6wNcd3Ya3704g9Sk+EiX1qkVl1exr/AYsbExjBrUn7jYjvkbPpxf5RngIeDXJ1h+KZAR+jgbeDT0r5xESW0xqw+uoLSuBADHmdxvKpkDZrbpwlzRgaP86b6/UXy4lOOfPeWLk9g35ShldeUN485OPoOzB56hi3/SatV19fzmzd08vCaXoxW1fGHqMG6bM570lMRIl9bpvbltFys35gCOOyQmxHH97OkMS+kX9q8dtlBw9/Vmln6SIQuAX7u7AxvMrL+ZDXX3A+Gqqatzd9YXrqUyUEFyXAoAAQ/w3rEtDEkYwsjeo055fX97dCVV5dUMGZUKQF1dHct2ryF96ChGDh4CBI9M3izayrDeg0jrPbR9N0q6nfqA88K7+7hvdQ77jlXy2XEp/Hj+RKaMCP8vtO5g3+Filr+VxaABfYiJDl5nKSmv4vdrtvD9L88K+2NEI3lNYTiwt9F0fmieQuEESutKKawuIDl2YMO8KIsiISqB7aU5pxwKRQeOUphfxKCRKQ3zahPrqe8XoHJ/OQwOzou2aOKjYskuyVMoyAm5O2uzC1m8Iousg6VMHt6XRVdMYVZGaqRL61I+3HWQmOiohkAA6JuYwMGiUvYXlZA2KLzv3YhkKDR3HsKbHWh2C3ALQFpaWjhr6tScAIZ94hSOWRT1Xn/K6wvUf3J9jmMYXv/xXRFlRl0bvob0DO/sOcqi5Vm8nXeEUQN788trzuSyKUPVXtoG9fWB5k/TGgQCgbB//UiGQj4wstH0CGB/cwPdfSmwFCAzM7PZ4OgJkmL60je2L+V15STGBM/LujuV9RWM7TPulNc3cNgAkpITKTtWTp/+wfUlVMVBuZOY8Y8LywF3KutrGJ+U3i7bId1HbkEZd6/MYuWHh0jpE8fPF5zO1WelERcT3lMc3dmEtEG8sW03gYA3hGpFdQ3xsTEMG/jpmkpaI5KhsAy41cyeI3iBuVjXE04uyqI4P/UiVh18kaLqw5gZAQKMSRzLqMTRp7y+6OhoLvvmXP583984uLuQKDMCgQCz0qdzLLWKQ1VHMAsGz6R+Y0lPHB6GrZKu6GBxFQ+8lMPzm/bSKzaaH84ez82zRpMYry73T2v0kGQ+M2kUG7btbjhiiImO4uqLpnVIB5IFr/OGYcVmvwcuBFKAQ8C/AbEA7v5YqCX1IWA+wZbUr7n7ppbWm5mZ6Zs2tTisW6uqr2RPxR6q6itJjR/E4IQhRFnb/zIrL6kg9908qsqqGJ4xlGHjhlAZqCavLJ/KQDXDeqUyLEFPtRIorqjl0XU7ePr1PALuXHf2KG69eBwpfdRe2p7cnX2Hi8k7eIT42BgmjBxEv8RPd4dYM9vs7pktjgtXKISLQkGk41XV1vPrN3fx8JodlFTVsmDqMG6bO4GRyb0jXZq0UmtDQcd6InJC9QHnz+/kc//qHA4UV3HB+FR+NH8Cpw9Te2l3pVAQkU9wd176qIAlK7LYXlDG1BH9uPeqqZw7NqXlT5YuTaEgIh+zcdcRFi/PYtPuo4xJSeSR66Zz6eQhuqbUQygURASAnEOlLFmRxUsfFTAoKZ7//NJkrsocSWyY30ErnYtCQaSH23+skvtX5/Dnd/JJjIvhjnkT+Np56fSO06+Hnkh7XaSHOlZRwyNrd/DMG7vA4abzRvOdi8YxIDEu0qVJBCkURHqYypp6nn4jj0fX7qCsuo4vnzmCH87JYMQAtZeKQkGkx6irD/DHzfk88FIOh0qquWTiIO6YP4GJQ8J/6wTpOhQKIt2cu7Pyw4MsWZnNzsJypqf155fXTGfm6ORIlyadkEJBpBvbsLOIRcuz2LL3GGNTE3n8hhnMnTRY7aVyQgoFkW7oowMlLFmRxZrsQob0TWDxFVO4YvqIsD+gRbo+hYJIN7L3SAX3r87hr1v2kRQfw52XTuTGc9NJiI1u+ZNFUCiIdAtHymt46JVcfrthN2Zwy/lj+PYF4+jXOzbSpUkXo1AQ6cIqaup48tU8lq7fSXlNHVfOGMkP5mQwtF+vSJcmXZRCQaQLqq0P8IeNe3nw5e0UllYzZ9JgfjRvAhmDkyJdmnRxCgWRLsTd+fv7B7h3VQ55h8s5K30Aj10/nRmj1F4q7UOhINJFvJF7mEUrsngvv5jxg/vw5FczuXiinogn7UuhINLJfbCvmMUrsnh1+2GG9Uvgniun8qUzhxMdpTCQ9qdQEOmk9hRVcM+qbJZt3U//3rH85LLTuP6cUWovlbBSKIh0MofLqnnolVx+99ZuoqOMb184lm9eMJZ+vdReKuGnUBDpJMqq63ji1Z38av1OquoCXJU5kh/MzmBw34RIlyY9SFhDwczmAw8C0cAT7r6oyfI04Fmgf2jMne7+YjhrEulsauoC/P7tPfzi5e0Ulddw6eQh3D5vAmNT+0S6NOmBwhYKZhYNPAzMAfKBjWa2zN23NRr2E+B5d3/UzCYBLwLp4apJpDMJBJy/vbefe1flsOdIBeeMSeaJ+RM5M21ApEuTHiycRwozgVx33wlgZs8BC4DGoeDA8Zu59wP2h7EekU7B3Xl1+2EWr8jiw/0lTBySxDNfO4sLxqeqvVQiLpyhMBzY22g6Hzi7yZi7gFVm9l0gEZgdxnpEIu69/GMsXpHF67lFjBjQi/uvnsqCqcOJUnupdBLhDIXmvsu9yfQ1wDPufq+ZfQb4jZlNdvfAx1ZkdgtwC0BaWlpYihUJp7zD5dyzKpu/v3eA5MQ4fvr5SVx3ThrxMWovlc4lnKGQD4xsND2CT54e+jowH8Dd3zSzBCAFKGg8yN2XAksBMjMzmwaLSKdVUFrFL17eznNv7yUuJorvXTyOb5w/hqQEtZdK5xTOUNgIZJjZaGAfsBC4tsmYPcAlwDNmdhqQABSGsSaRDlFaVcvS9Tt54tU8ausDXDMzje9eMo5BSWovlc4tbKHg7nVmdiuwkmC76VPu/qGZ/QzY5O7LgNuAX5nZDwmeWrrR3XUkIF1WdV09v92wh4fX5HKkvIbPnzGU2+dOID0lMdKlibRKWN+nEHrPwYtN5v200ettwHnhrEGkI9QHnP/eso97V+Ww71gl540byJ3zT2PKiH6RLk3klOgdzSKfgruzNqeQxcuzyDpYyuThfVl0xRRmZaRGujSRNlEoiLTRu3uOsmh5Fm/lHSEtuTe/uOZMPj9lqNpLpUtTKIicoh2FZdy9IpsVHx4kpU8cP1twOgvPSiMuJirSpYl8agoFkVY6WFzFgy/n8PymfBJiovjh7PHcPGs0ifH6MZLuQ9/NIi0orqzlsXU7ePr1POoDzg3njOLWi8eR0ic+0qWJtDuFgsgJVNXW85s3d/PQmlyKK2tZMG0Yt82ZQNrA3pEuTSRsFAoiTdQHnL+8k8/9q3PYX1zF+eNT+dG8CUwervZS6f5aDAUziwLec/fJHVCPSMS4Oy9/VMCSlVnkHCpj6oh+3HPlVM4dlxLp0kQ6TIuh4O4BM9tqZmnuvqcjihLpaJt3H2HR8iw27jrK6JREHrluOpdOHqJbWUuP09rTR0OBD83sbaD8+Ex3vzwsVYl0kO2HSlmyMpvV2w6RmhTPf35pMldljiQ2Wu2l0jO1NhT+PaxViHSw/ccqeeClHP60OZ/EuBhunzuemz47mt5xuswmPVurfgLcfZ2ZjQIy3P0lM+tN8CZ3Il3KsYoaHl27g6ff2AUOXztvNN+5aBzJiXGRLk2kU2hVKJjZNwg+5CYZGEvwqWqPEbzttUinV1Vbz9Ov7+LRtbmUVtfxpTOH889zxjNigNpLRRpr7bHydwg+c/ktAHffbmaDwlaVSDupqw/wp835PPDSdg6WVHHxxEH8aP4EJg7p2/Ini/RArQ2FanevOd6JYWYxfPLRmiKdhruz8sND3L0yix2F5ZyZ1p8HF07j7DEDI12aSKfW2lBYZ2b/G+hlZnOAbwN/C19ZIm331s4iFq3I4t09xxibmshj189g3umD1V4q0gqtDYU7CT5P+X3gmwQfnPNEuIoSaYusgyUsWZHNK1kFDO4bz6IvT+ErM0YQo/ZSkVZrbSgsAH7t7r8KZzEibZF/tIL7Vufw13f3kRQfw4/nT+TGc9PpFacGOZFT1dpQuBx4wMzWA88BK929LnxlibTsSHkND6/J5Tdv7gaDW2aN4VsXjqV/b7WXirRVa9+n8DUziwUuBa4FHjGz1e5+c1irE2lGRU0dT72Wx+PrdlJeU8dXZozgB7PHM6x/r0iXJtLltfrtm+5ea2bLCXYd9SJ4SkmhIB2mtj7A85v28sBL2yksrWbOpMHcMW8C4wcnRbo0kW6jtW9emw8sBC4C1hK8yHxV+MoS+Qd358X3D3LPqmzyDpeTOWoAj143ncz05EiXJtLttPZI4UaC1xK+6e7VrV15KEweJHhLjCfcfVEzY64C7iJ4BLLV3a9t7fql+3tjx2EWL89ia34x4wf34Yl/yuSS0wapvVQkTFp7TWFh6N5Hs4CXzKwXEOPupSf6HDOLBh4G5gD5wEYzW+bu2xqNyQD+BTjP3Y/qXdJy3If7i1m8Ipv1OYUM65fA3V85gy9PH0F0lMJAJJzaeu+jEbR876OZQK677wyt4zmC1yG2NRrzDeBhdz8K4O4Fp7oB0r3sPVLBvauyeWHLfvr1iuVfP3caN3xmFAmxai8V6QjhvPfRcGBvo+l84OwmY8YDmNnrBE8x3eXuK5quyMxuIRhKpKWltbJk6UqKyqr55Su5/O6t3URHGd+6cCz/64Kx9OsVG+nSRHqUcN77qLnj/KafEwNkABcSPPp41cwmu/uxj32S+1JgKUBmZqbuudSNlFfX8cSreSxdv4OqugBXZY7kB7MzGNw3IdKlifRI4bz3UT4wstH0CGB/M2M2uHstkGdm2QRDYmMr65IuqqYuwHMb9/CLl7dzuKyG+acP4fZ5Exg3qE+kSxPp0cJ576ONQIaZjQb2EWxpbdpZ9AJwDfCMmaUQPJ20s5U1SRcUCDj/8/4B7l2Vze6iCs4enczSf5rI9LQBkS5NRGghFMwszd33uHsA+FXoo1Xcvc7MbgVWErxe8JS7f2hmPwM2ufuy0LK5ZrYNqAfucPeitm6MdG6vbi9k8YosPthXwsQhSTz9tbO4cHyq2ktFOhFzP/EpejN7x92nh17/2d2v6LDKTiAzM9M3bdoU6TLkFLyfX8ziFVm8lnuY4f17cdvc8SyYNlztpSIdyMw2u3tmS+NaOn3U+Kd2zKcrSXqaXYfLuWdVNv/z3gEG9I7l/3x+Etefk0Z8jNpLRTqrlkLBT/Ba5IQKSqv45cu5/P7tPcRGR/Hdi8fxjfPH0DdB7aUinV1LoTDVzEoIHjH0Cr0mNO3urgfdSoPSqlp+tX4nT7yWR01dgIUzR/K9SzIYlKT2UpGu4qSh4O46zpcWVdfV87sNe3hoTS5Hymu47Iyh3D53AqNTEiNdmoicolbfOlukqUDA+e+t+7h3VQ75Rys5d+xA7rx0ImeM6B/p0kSkjRQKcsrcnXU5hSxekc1HB0o4fVhf/u+XpjArI0XtpSJdnEJBTsmWvcdYtPwjNuw8Qlpybx5cOI0vnDGMKLWXinQLCgVplR2FZdyzMpvlHxxkYGIc/3756VwzM424mKhIlyYi7UihICd1qKSKB17azvOb9pIQE8UPZmdw86wx9InXt45Id6SfbGlWSVUtj6/bwZOv5VEfcG44ZxS3XjyOlD7xkS5NRMJIoSAfU1Vbz2837OahNbkcq6jl8qnDuH3uBNIG9o50aSLSARQKAkB9wHnh3X3ctzqHfccqmZWRwo/nT2Ty8H6RLk1EOpBCoYdzd9ZkF7B4eTbZh0o5Y0Q/lnzlDM4blxLp0kQkAhQKPdjm3UdZvDyLt3cdIX1gbx669kwumzJU7zUQ6cEUCj1QbkEpS1Zks2rbIVL6xPPzL05m4VkjiY1We6lIT6dQ6EEOFFfywOrt/HHzXnrHxXDbnPHc9NnRJKq9VERC9NugByiuqOWRdbk88/ou3OHGc0dz68XjSE6Mi3RpItLJKBS6saraep55YxePrMmltLqOL00bzg/njGdkstpLRaR5CoVuqK4+wJ/fyef+1ds5WFLFRRNS+dH8iZw2VI+/EJGTUyh0I+7Oqm2HuHtlNrkFZUwb2Z8HFk7jnDEDI12aiHQRCoVu4u28Iyxa/hHv7DnGmJREHrt+OvNOH6L2UhE5JWENBTObDzwIRANPuPuiE4z7CvBH4Cx33xTOmrqb7IOlLFmRxctZBQxKiuf/fXkKV84YQYzaS0WkDcIWCmYWDTwMzAHygY1mtszdtzUZlwR8D3grXLV0R/uOVXLfqhz+8m4+feJjuGPeBG46bzS94vQEVRFpu3AeKcwEct19J4CZPQcsALY1GfdzYAlwexhr6TaOltfw8Jpcfr1hNwDfmDWGb10wlgFqLxWRdhDOUBgO7G00nQ+c3XiAmZ0JjHT3/zGzE4aCmd0C3AKQlpYWhlI7v4qaOp5+fRePrd1BWU0dV0wfwQ/njGd4/16RLk1EupFwhkJzVzi9YaFZFHA/cGNLK3L3pcBSgMzMTG9heLdSVx/g+U35PPBSDgWl1cw+bRB3zJvIhCFJkS5NRLqhcIZCPjCy0fQIYH+j6SRgMrA21CEzBFhmZpfrYnOwvXTFBwe5e2U2Ow+XM2PUAB6+bjpnpSdHujQR6cbCGQobgQwzGw3sAxYC1x5f6O7FQMP9mc1sLXC7AgHe3FHEohVZbN17jIxBfVh6wwzmTBqs9lIRCbuwhYK715nZrcBKgi2pT7n7h2b2M2CTuy8L19fuqrbtL2HJyizWZhcytF8CS644gy9PH672UhHpMGF9n4K7vwi82GTeT08w9sJw1tKZ7T1SwX2rc3hhyz76JsTyL5dO5KvnppMQq/ZSEelYekdzBBWVVfPQmlx+t2EPZvDN88fyrQvG0q93bKRLE5EeSqEQARU1dTz5ah6Pr99JRU0dV2WO5PuzMxjaT+2lIhJZCoUOVFsf4LmNe3nwpe0cLqtm3umDuWPeBMYNUnupiHQOCoUOEAg4L35wgHtWZrOrqIKZ6ck8fsMMZowaEOnSREQ+RqEQZq/nHmbxiizeyy9m4pAknroxk4smDFJ7qYh0SgqFMPlgXzGLV2Tx6vbDDO/fi3uvnMoXzxxOdJTCQEQ6L4VCO9tdVM69q3JYtnU/A3rH8pPLTuP6c0apvVREugSFQjs5XFbNL1/ezu/e2kNMtPGdi8byzQvG0jdB7aUi0nUoFD6lsuo6frV+J0+8upOqugALzxrJ9y/JYFDfhEiXJiJyyhQKbVRTF+C/3trNL1/Jpai8hsumDOW2ueMZk9on0qWJiLSZQuEUBQLO397bz72rcthzpILPjBnIjy+dyLSR/SNdmojIp6ZQaCV3Z/32wyxensW2AyWcNrQvz940k/MzUtReKiLdhkKhFbbuPcbiFVm8saOIkcm9eODqaVw+dRhRai8VkW5GoXASeYfLuWdlNn9//wDJiXHc9YVJXHv2KOJidCtrEemeFArNKCip4sGXt/Pcxr3Ex0TxvUsy+Mas0SSpvVREujmFQiMlVbUsXbeTJ1/Lo7Y+wHVnp/HdizNITYqPdGkiIh1CoQBU19Xz2w17eOiV7RytqOULU4dx25zxpKckRro0EZEO1aNDoT7gvPDuPu5bncO+Y5XMykjhR/MmMmVEv0iXJiISET0yFNydNdkFLFmRTdbBUiYP78uiK6YwKyM10qWJiERUjwyFp17fxc//ZxujBvbml9ecyWVThqq9VESEHhoKC6YNIy7auPqsNLWXiog0EtbfiGY238yyzSzXzO5sZvk/m9k2M3vPzF42s1HhrOe4lD7x3PCZdAWCiEgTYfutaGbRwMPApcAk4Bozm9Rk2LtAprufAfwJWBKuekREpGXh/FN5JpDr7jvdvQZ4DljQeIC7r3H3itDkBmBEGOsREZEWhDMUhgN7G03nh+adyNeB5c0tMLNbzGyTmW0qLCxsxxJFRKSxcIZCc+083uxAs+uBTODu5pa7+1J3z3T3zNRUtY2KiIRLOLuP8oGRjaZHAPubDjKz2cC/Ahe4e3UY6xERkRaE80hhI5BhZqPNLA5YCCxrPMDMzgQeBy5394Iw1iIiIq0QtlBw9zrgVmAl8BHwvLt/aGY/M7PLQ8PuBvoAfzSzLWa27ASrExGRDhDWN6+5+4vAi03m/bTR69nh/PoiInJq9O4tERFpoFAQEZEGCgUREWmgUBARkQYKBRERaaBQEBGRBgoFERFpoFAQEZEGCgUREWmgUBARkQYKBRERaaBQEBGRBgoFERFpoFAQEZEGCgUREWmgUBARkQYKBRERaaBQEBGRBgoFERFpoFAQEZEGPTYU3P2k0wCBQKDd1i8i0hXEhHPlZjYfeBCIBp5w90VNlscDvwZmAEXA1e6+K1zqea2hAAAKhElEQVT1uDt55dvILtlIeX0pA+OGEDg2nndyj3G0oor0lP7MPT2DgsObWPf+Voorakgf1Je5My4hbcikFtcfCDhv5+1lbfZOSiqrGZOazPzJ4xmR3C9cmyQi0q7CdqRgZtHAw8ClwCTgGjNr+pv168BRdx8H3A8sDlc9ALmlW9l8ZDUA/WNS2LyjkCdfX0NVoIJh/ZMoKCnn6dXP8qfX3yLKYGj/3hw6WsGTK1/gwOHcFte/LiePv77zIdEWxdB+Sew/VsLS9W9TUFIWzs0SEWk34Tx9NBPIdfed7l4DPAcsaDJmAfBs6PWfgEvMzMJRTL3X8VHJ2/SNSSYuKoFAALbviqZfolHpRZgZ/XtHc7S4iPhY6BUXi5kxoE88UWa8/sH6k66/pq6Otdk7GdI3qeFzkxN74+68sWN3ODZJRKTdhTMUhgN7G03nh+Y1O8bd64BiYGDTFZnZLWa2ycw2FRYWtqmYmvoqar2GmKg4AKprndp6Jz4ujqr64F/yHqgg4FDvH7+WkJgQw/4jxSddf2lVDXX1AWJjoj82v098PPuPlrSpZhGRjhbOUGjuL/6mV19bMwZ3X+rume6emZqa2qZi4qJ7ERsVT22gBoD4WCM2xqisraFXdFKwGOtNlEF0k7LKqmoZmZp80vUnJcQRGx1FTV39xz+3ukbXFESkywhnKOQDIxtNjwD2n2iMmcUA/YAj4Sgm2qI5vd85lNQdoaq+AosKMD69jtJyiGcg9YEARyrqGJicSnUdlFfVUB9wikorAeMzk2addP1xMTHMPm0sB0tKKa+uoT4Q4HBZOdFRxmfGjgrHJomItLtwdh9tBDLMbDSwD1gIXNtkzDLgq8CbwFeAVzyMvZxjEicTa3FklWyirL6YmWOGM3NQBptzj3K4rIIxqcnMnvRPFBZtYd3771BUWsmYIQO4ZPolDBk4psX1n5eRTq+4ONZl76SorIKMwQOZPSmD1KTEcG2SiEi7snD205vZ54AHCLakPuXu/2lmPwM2ufsyM0sAfgOcSfAIYaG77zzZOjMzM33Tpk1hq1lEpDsys83untnSuLC+T8HdXwRebDLvp41eVwFXhrMGERFpvR77jmYREfkkhYKIiDRQKIiISAOFgoiINFAoiIhIA4WCiIg0UCiIiEiDsL55LRzMrBD4tLcdTQEOt0M5kabt6Fy0HZ2LtuPjRrl7izeP63Kh0B7MbFNr3tnX2Wk7OhdtR+ei7WgbnT4SEZEGCgUREWnQU0NhaaQLaCfajs5F29G5aDvaoEdeUxARkeb11CMFERFpRo8LBTObb2bZZpZrZndGup5TYWa7zOx9M9tiZptC85LNbLWZbQ/9OyDSdTZlZk+ZWYGZfdBoXrN1W9AvQvvnPTObHrnKP+4E23GXme0L7ZMtoWeIHF/2L6HtyDazeZGp+uPMbKSZrTGzj8zsQzP7fmh+l9ofJ9mOrrY/EszsbTPbGtqOfw/NH21mb4X2xx/MLC40Pz40nRtant7uRbl7j/kg+LCfHcAYIA7YCkyKdF2nUP8uIKXJvCXAnaHXdwKLI11nM3WfD0wHPmipbuBzwHKCz+8+B3gr0vW3sB13Abc3M3ZS6PsrHhgd+r6L7gTbMBSYHnqdBOSEau1S++Mk29HV9ocBfUKvY4G3Qv/PzxN86BjAY8C3Qq+/DTwWer0Q+EN719TTjhRmArnuvtPda4DngAURrunTWgA8G3r9LPDFCNbSLHdfzyefvX2iuhcAv/agDUB/MxvaMZWe3Am240QWAM+5e7W75wG5BL//IsrdD7j7O6HXpcBHwHC62P44yXacSGfdH+7uZaHJ2NCHAxcDfwrNb7o/ju+nPwGXmJm1Z009LRSGA3sbTedz8m+kzsaBVWa22cxuCc0b7O4HIPiDAgyKWHWn5kR1d8V9dGvo1MpTjU7fdfrtCJ16OJPgX6dddn802Q7oYvvDzKLNbAtQAKwmeBRzzN3rQkMa19qwHaHlxcDA9qynp4VCc4naldqvznP36cClwHfM7PxIFxQGXW0fPQqMBaYBB4B7Q/M79XaYWR/gz8AP3L3kZEObmdeZt6PL7Q93r3f3acAIgkcvpzU3LPRv2Lejp4VCPjCy0fQIYH+Eajll7r4/9G8B8FeC30CHjh/Oh/4tiFyFp+REdXepfeTuh0I/1AHgV/zjlESn3Q4ziyX4i/R37v6X0Owutz+a246uuD+Oc/djwFqC1xT6m1lMaFHjWhu2I7S8H60/pdkqPS0UNgIZoSv7cQQv1CyLcE2tYmaJZpZ0/DUwF/iAYP1fDQ37KvDfkanwlJ2o7mXAP4W6Xs4Bio+f1uiMmpxf/xLBfQLB7VgY6hYZDWQAb3d0fU2Fzj8/CXzk7vc1WtSl9seJtqML7o9UM+sfet0LmE3w+sga4CuhYU33x/H99BXgFQ9ddW43kb763tEfBLspcgiet/vXSNdzCnWPIdg9sRX48HjtBM8nvgxsD/2bHOlam6n99wQP5WsJ/qXz9RPVTfDw+OHQ/nkfyIx0/S1sx29Cdb4X+oEd2mj8v4a2Ixu4NNL1h2r6LMHTDe8BW0Ifn+tq++Mk29HV9scZwLuhej8AfhqaP4ZgaOUCfwTiQ/MTQtO5oeVj2rsmvaNZREQa9LTTRyIichIKBRERaaBQEBGRBgoFERFpoFAQEZEGMS0PEen8zOx4SyXAEKAeKAxNz/Tgva46FTO7CXjR3Q9GuhaR49SSKt2Omd0FlLn7PZ2glmh3rz/BsteAW919yymsL8b/cU8ckXan00fS7ZnZV0P3rN9iZo+YWZSZxZjZMTO728zeMbOVZna2ma0zs53H78NvZjeb2V9Dy7PN7CetXO9/mNnbwEwz+3cz22hmH5jZY6F3B19N8P48fwh9fpyZ5Td6d+s5ZvZS6PV/mNnjZrYaeDr0Ne4Lfe33zOzmjv9fle5KoSDdmplNJni7g3M9eNOxGIK3N4HgfWNWefAmgzUE78V/CXAl8LNGq5kZ+pzpwLVmNq0V633H3We6+5vAg+5+FjAltGy+u/+B4Ltwr3b3aa04vXUm8AV3vwG4BShw95nAWQRvjpjWlv8fkaZ0TUG6u9kEf3FuCt12vhf/uIVypbuvDr1+n+B9ferM7H0gvdE6Vrr7UQAze4HgLRZiTrLeGoI3LDzuEjO7g+AtClKAzQQfXHMq/tvdq0Kv5wKnmVnjEMoA9pziOkU+QaEg3Z0BT7n7//nYzOAdJhv/dR4Aqhu9bvyz0fTCm7ew3ko/fuMgs97AQwSfErbPzP6DYDg0p45/HL03HVPeZJu+7e4vI9LOdPpIuruXgKvMLAWCXUptONUy18z6h37BLwBeP4X19iIYModDd7m9otGyUoKPkjxuFzAj9LrxuKZWAt8+fmtlM5sQusOmyKemIwXp1tz9fQs+DP0lM4sieIfT/8Wp3Uv/NeC/CD685TfHu4Vas153LzKzZwneAXM3/3g6GMDTwBNmVknwusVdwK/M7CAnv63z40AasCV06qqArv9YWekk1JIqchKhzp7J7v6DSNci0hF0+khERBroSEFERBroSEFERBooFEREpIFCQUREGigURESkgUJBREQaKBRERKTB/we8XMpHNLSc2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x214060c4160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1 ,5, 10, 10, 25, 50, 70, 75, 300]\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1 ]\n",
    "\n",
    "colors = np.random.rand(len(x))\n",
    "plt.plot(np.unique(x),np.poly1d(np.polyfit(x,y,1))(np.unique(x)))\n",
    "plt.ylabel(\"Fever\")\n",
    "plt.xlabel(\"Temperature \")\n",
    "plt.scatter(x,y , c=colors ,alpha =0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Linear Regression Probelm 1 ***\n",
    "\n",
    "Fever points not predicted with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression In-Depth \n",
    "\n",
    "*** Predicting Probability ***\n",
    "\n",
    "* Linear Regresion Doesn't work\n",
    "* Instead of predicting direct values : predict probability\n",
    "\n",
    "![LogisticReg](log.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Logistic Function g()***\n",
    "\n",
    "* Two class logistic regression\n",
    "* y = Ax+b\n",
    "* g(y) = $ \\frac{1}{1+e^{-y}} =\\frac{1}{1+e^{(Ax+b)}}  $\n",
    "* g(y) = Estimated probability that y=1 given x\n",
    "\n",
    "*** Softmax Function g()***\n",
    "\n",
    "* Multi-Class logistic regression\n",
    "* Generalization of logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Cross Entry Function D() ***\n",
    "\n",
    "* $ D(S,L) = LlogS-(1-L)log(1-S)$\n",
    "    * If L = 0 (label)\n",
    "        * $ D(S,0)= -log(1-S) $\n",
    "            * $ -log(1-S) $: less positive if $S$ ---> 0  \n",
    "            * $ -log(1-S) $: more positive if $S$ ---> 1 (BIGGER LOSS)\n",
    "            \n",
    "    * If L = 1 (label)\n",
    "        * $ D(S,1)= LogS $\n",
    "            * $ LogS $: less negative if $S$ ---> 0  \n",
    "            * $ LogS $: more negative if $S$ ---> 1 (BIGGER LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000050000287824e-05\n",
      "11.51292546497478\n",
      "-1.0000050000287824e-05\n",
      "-11.512925464970229\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(-math.log(1-0.00001))\n",
    "print(-math.log(1-0.99999))\n",
    "\n",
    "print(math.log(0.99999))\n",
    "print(math.log(0.00001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Cross Entropy Loss L***-\n",
    "\n",
    "* Goal : Minimizing Cross Entropy Loss\n",
    "* $L = \\frac{1}{N}\\sum_i D(g(A_{x}+b),L_{i}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Logistic Regression Model With PyTorch\n",
    "\n",
    "##### Steps\n",
    "* Step 1: Load Dataset\n",
    "* Step 2: Make Dataset iterable\n",
    "* Step 3: Make Model Class\n",
    "* Step 4: Instantiate Model Class\n",
    "* Step 5: Instantiate Loss Class\n",
    "* Step 6: Instantiate Optimizer Class\n",
    "* Step 7: Instantiate Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Loading MNIST Train Dataset\n",
    "*** Images from 1 to 9 ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as dsets \n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# create the train and tets dataset \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]), tensor(5))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input matrix \n",
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label\n",
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Displaying MNIST *** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2140f297c88>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2140f1b2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img = train_dataset[0][0].numpy().reshape(28,28)\n",
    "plt.imshow(show_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking label\n",
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21408424048>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2140f2ad278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try another\n",
    "show_img = train_dataset[1][0].numpy().reshape(28,28)\n",
    "plt.imshow(show_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label\n",
    "train_dataset[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x214084add30>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2140f2d2f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "show_img = test_dataset[0][0].numpy().reshape(28,28)\n",
    "plt.imshow(show_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking label\n",
    "test_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Make Dataset Iterable\n",
    "\n",
    "* Aim : Make the datset iterable\n",
    "* Total Data : 60,000 \n",
    "* Minibatch : 100\n",
    "    * Number of examples in 1 iteration\n",
    "* Iteration: 3000\n",
    "    * 1 Iteration : one mini batch forward & backward-pass\n",
    "   \n",
    "* Epochs\n",
    "    * 1 Epoch : Running through the whole dataset once\n",
    "    * $ \\ epochs = iterations\\div\\frac{totaldata}{minibatch}=3000 \\div \\frac{60000}{100} = 5\\ $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create iterable objects : Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check iterability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "isinstance (train_loader,collections.Iterable)\n",
    "# return true if it's iterable, return false if its not iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create iterable objects : Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# check iterability\n",
    "isinstance (test_loader,collections.Iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Aim: Iterate through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.ones((28,28))\n",
    "img_2 = np.ones((28,28))\n",
    "\n",
    "lst = [img_1,img_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Need to iterate\n",
    "# Let's think of it as an image\n",
    "\n",
    "for i in lst:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model # same as linear regression \n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 :  Instantiate Model Class\n",
    "\n",
    "* Input Dimension :\n",
    "    * Size of image\n",
    "    * 28*28 = 784\n",
    "* Output Dimension : 10\n",
    "    * 0,1,2,3,4,5,6,7,8,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of the images\n",
    "\n",
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 :  Instantiate Loss Class\n",
    "\n",
    "* Logistic Regression : Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** What happens in nn.CrossEntropyLoss() *** \n",
    "\n",
    "* Computes softmax(logistic/softmax function)\n",
    "* Computes cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "\n",
    "#### Instantiate Optimizer Class\n",
    "\n",
    "* Simplified Equation :$\\Theta = \\Theta -\\eta .*\\bigtriangledown _{\\Theta }$\n",
    "    * $\\Theta$ : parameter(Our Variables)\n",
    "    * $\\eta$ : learning rate\n",
    "    * $\\bigtriangledown _{\\Theta }$ : parameter's gradients\n",
    "    \n",
    "* Even Simpler equation:\n",
    "    * parameters  = parameters-learning_rate * parameter_gradients\n",
    "    * At every iteration , we update our model parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x00000214085312B0>\n",
      "2\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "# FC1 Parameters\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC2 Parameters\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 7: TRAIN THE MODEL\n",
    "\n",
    "* Process\n",
    "    1. Convert input/labels to variables\n",
    "    2. Clear gradient buffets\n",
    "    3. Get Output given inputs\n",
    "    4. Get loss\n",
    "    5. Get gradient w.r.t to parameters\n",
    "    6. update parameters using gradients\n",
    "        * parameters = parameters-learning_rate * parameter_gradients\n",
    "    7. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.3525338172912598 \n",
      "Iteration: 2. Loss: 2.348985433578491 \n",
      "Iteration: 3. Loss: 2.2768068313598633 \n",
      "Iteration: 4. Loss: 2.3310117721557617 \n",
      "Iteration: 5. Loss: 2.314495325088501 \n",
      "Iteration: 6. Loss: 2.3247199058532715 \n",
      "Iteration: 7. Loss: 2.3429088592529297 \n",
      "Iteration: 8. Loss: 2.3038530349731445 \n",
      "Iteration: 9. Loss: 2.3187692165374756 \n",
      "Iteration: 10. Loss: 2.341754913330078 \n",
      "Iteration: 11. Loss: 2.341477632522583 \n",
      "Iteration: 12. Loss: 2.3177871704101562 \n",
      "Iteration: 13. Loss: 2.3168540000915527 \n",
      "Iteration: 14. Loss: 2.3244564533233643 \n",
      "Iteration: 15. Loss: 2.3124876022338867 \n",
      "Iteration: 16. Loss: 2.3459393978118896 \n",
      "Iteration: 17. Loss: 2.306161880493164 \n",
      "Iteration: 18. Loss: 2.292889356613159 \n",
      "Iteration: 19. Loss: 2.293450355529785 \n",
      "Iteration: 20. Loss: 2.2930209636688232 \n",
      "Iteration: 21. Loss: 2.297396421432495 \n",
      "Iteration: 22. Loss: 2.285226345062256 \n",
      "Iteration: 23. Loss: 2.3198423385620117 \n",
      "Iteration: 24. Loss: 2.30489444732666 \n",
      "Iteration: 25. Loss: 2.298140287399292 \n",
      "Iteration: 26. Loss: 2.2982993125915527 \n",
      "Iteration: 27. Loss: 2.2797553539276123 \n",
      "Iteration: 28. Loss: 2.2949047088623047 \n",
      "Iteration: 29. Loss: 2.2877211570739746 \n",
      "Iteration: 30. Loss: 2.2911500930786133 \n",
      "Iteration: 31. Loss: 2.298492670059204 \n",
      "Iteration: 32. Loss: 2.328716993331909 \n",
      "Iteration: 33. Loss: 2.2814290523529053 \n",
      "Iteration: 34. Loss: 2.2671735286712646 \n",
      "Iteration: 35. Loss: 2.289215326309204 \n",
      "Iteration: 36. Loss: 2.2775988578796387 \n",
      "Iteration: 37. Loss: 2.2855024337768555 \n",
      "Iteration: 38. Loss: 2.285132646560669 \n",
      "Iteration: 39. Loss: 2.2774839401245117 \n",
      "Iteration: 40. Loss: 2.297255516052246 \n",
      "Iteration: 41. Loss: 2.281738042831421 \n",
      "Iteration: 42. Loss: 2.260331630706787 \n",
      "Iteration: 43. Loss: 2.2943429946899414 \n",
      "Iteration: 44. Loss: 2.248262882232666 \n",
      "Iteration: 45. Loss: 2.2522270679473877 \n",
      "Iteration: 46. Loss: 2.2780885696411133 \n",
      "Iteration: 47. Loss: 2.304758071899414 \n",
      "Iteration: 48. Loss: 2.2818005084991455 \n",
      "Iteration: 49. Loss: 2.247873544692993 \n",
      "Iteration: 50. Loss: 2.2480673789978027 \n",
      "Iteration: 51. Loss: 2.2602734565734863 \n",
      "Iteration: 52. Loss: 2.2589309215545654 \n",
      "Iteration: 53. Loss: 2.2810471057891846 \n",
      "Iteration: 54. Loss: 2.2485952377319336 \n",
      "Iteration: 55. Loss: 2.2751283645629883 \n",
      "Iteration: 56. Loss: 2.2885570526123047 \n",
      "Iteration: 57. Loss: 2.2346696853637695 \n",
      "Iteration: 58. Loss: 2.2714149951934814 \n",
      "Iteration: 59. Loss: 2.2425098419189453 \n",
      "Iteration: 60. Loss: 2.292733669281006 \n",
      "Iteration: 61. Loss: 2.248562812805176 \n",
      "Iteration: 62. Loss: 2.2612361907958984 \n",
      "Iteration: 63. Loss: 2.2357840538024902 \n",
      "Iteration: 64. Loss: 2.2623157501220703 \n",
      "Iteration: 65. Loss: 2.2501919269561768 \n",
      "Iteration: 66. Loss: 2.2396767139434814 \n",
      "Iteration: 67. Loss: 2.270418167114258 \n",
      "Iteration: 68. Loss: 2.2592570781707764 \n",
      "Iteration: 69. Loss: 2.2233731746673584 \n",
      "Iteration: 70. Loss: 2.26411509513855 \n",
      "Iteration: 71. Loss: 2.2640914916992188 \n",
      "Iteration: 72. Loss: 2.253582239151001 \n",
      "Iteration: 73. Loss: 2.2471578121185303 \n",
      "Iteration: 74. Loss: 2.2522690296173096 \n",
      "Iteration: 75. Loss: 2.2331576347351074 \n",
      "Iteration: 76. Loss: 2.2324717044830322 \n",
      "Iteration: 77. Loss: 2.25268816947937 \n",
      "Iteration: 78. Loss: 2.2433691024780273 \n",
      "Iteration: 79. Loss: 2.218325138092041 \n",
      "Iteration: 80. Loss: 2.2481131553649902 \n",
      "Iteration: 81. Loss: 2.243696689605713 \n",
      "Iteration: 82. Loss: 2.2592966556549072 \n",
      "Iteration: 83. Loss: 2.214519739151001 \n",
      "Iteration: 84. Loss: 2.2304675579071045 \n",
      "Iteration: 85. Loss: 2.240353584289551 \n",
      "Iteration: 86. Loss: 2.2663958072662354 \n",
      "Iteration: 87. Loss: 2.233978509902954 \n",
      "Iteration: 88. Loss: 2.2228736877441406 \n",
      "Iteration: 89. Loss: 2.2454640865325928 \n",
      "Iteration: 90. Loss: 2.219857931137085 \n",
      "Iteration: 91. Loss: 2.228715658187866 \n",
      "Iteration: 92. Loss: 2.234916925430298 \n",
      "Iteration: 93. Loss: 2.221738815307617 \n",
      "Iteration: 94. Loss: 2.2403557300567627 \n",
      "Iteration: 95. Loss: 2.2354185581207275 \n",
      "Iteration: 96. Loss: 2.2177908420562744 \n",
      "Iteration: 97. Loss: 2.2276272773742676 \n",
      "Iteration: 98. Loss: 2.206254005432129 \n",
      "Iteration: 99. Loss: 2.2193784713745117 \n",
      "Iteration: 100. Loss: 2.2217531204223633 \n",
      "Iteration: 101. Loss: 2.220496416091919 \n",
      "Iteration: 102. Loss: 2.2022593021392822 \n",
      "Iteration: 103. Loss: 2.182662010192871 \n",
      "Iteration: 104. Loss: 2.196833848953247 \n",
      "Iteration: 105. Loss: 2.171419143676758 \n",
      "Iteration: 106. Loss: 2.188913106918335 \n",
      "Iteration: 107. Loss: 2.2261240482330322 \n",
      "Iteration: 108. Loss: 2.2028579711914062 \n",
      "Iteration: 109. Loss: 2.2359778881073 \n",
      "Iteration: 110. Loss: 2.179532766342163 \n",
      "Iteration: 111. Loss: 2.180643081665039 \n",
      "Iteration: 112. Loss: 2.2047414779663086 \n",
      "Iteration: 113. Loss: 2.1999645233154297 \n",
      "Iteration: 114. Loss: 2.1978578567504883 \n",
      "Iteration: 115. Loss: 2.212111473083496 \n",
      "Iteration: 116. Loss: 2.197272300720215 \n",
      "Iteration: 117. Loss: 2.176746129989624 \n",
      "Iteration: 118. Loss: 2.202589511871338 \n",
      "Iteration: 119. Loss: 2.1834583282470703 \n",
      "Iteration: 120. Loss: 2.172278881072998 \n",
      "Iteration: 121. Loss: 2.189997911453247 \n",
      "Iteration: 122. Loss: 2.1889727115631104 \n",
      "Iteration: 123. Loss: 2.1759819984436035 \n",
      "Iteration: 124. Loss: 2.2059943675994873 \n",
      "Iteration: 125. Loss: 2.1989355087280273 \n",
      "Iteration: 126. Loss: 2.1683640480041504 \n",
      "Iteration: 127. Loss: 2.1954097747802734 \n",
      "Iteration: 128. Loss: 2.1869957447052 \n",
      "Iteration: 129. Loss: 2.190009832382202 \n",
      "Iteration: 130. Loss: 2.178171157836914 \n",
      "Iteration: 131. Loss: 2.182765245437622 \n",
      "Iteration: 132. Loss: 2.191244602203369 \n",
      "Iteration: 133. Loss: 2.2055835723876953 \n",
      "Iteration: 134. Loss: 2.1745097637176514 \n",
      "Iteration: 135. Loss: 2.1971099376678467 \n",
      "Iteration: 136. Loss: 2.1803274154663086 \n",
      "Iteration: 137. Loss: 2.1626408100128174 \n",
      "Iteration: 138. Loss: 2.18198561668396 \n",
      "Iteration: 139. Loss: 2.200094699859619 \n",
      "Iteration: 140. Loss: 2.166013717651367 \n",
      "Iteration: 141. Loss: 2.200401544570923 \n",
      "Iteration: 142. Loss: 2.134019136428833 \n",
      "Iteration: 143. Loss: 2.1938021183013916 \n",
      "Iteration: 144. Loss: 2.182805061340332 \n",
      "Iteration: 145. Loss: 2.1612908840179443 \n",
      "Iteration: 146. Loss: 2.173346519470215 \n",
      "Iteration: 147. Loss: 2.1942429542541504 \n",
      "Iteration: 148. Loss: 2.1559135913848877 \n",
      "Iteration: 149. Loss: 2.168379783630371 \n",
      "Iteration: 150. Loss: 2.1548988819122314 \n",
      "Iteration: 151. Loss: 2.167964220046997 \n",
      "Iteration: 152. Loss: 2.1783313751220703 \n",
      "Iteration: 153. Loss: 2.1718907356262207 \n",
      "Iteration: 154. Loss: 2.1311428546905518 \n",
      "Iteration: 155. Loss: 2.153292417526245 \n",
      "Iteration: 156. Loss: 2.1421806812286377 \n",
      "Iteration: 157. Loss: 2.160444498062134 \n",
      "Iteration: 158. Loss: 2.161123752593994 \n",
      "Iteration: 159. Loss: 2.180539608001709 \n",
      "Iteration: 160. Loss: 2.164185047149658 \n",
      "Iteration: 161. Loss: 2.149402379989624 \n",
      "Iteration: 162. Loss: 2.1748571395874023 \n",
      "Iteration: 163. Loss: 2.1359405517578125 \n",
      "Iteration: 164. Loss: 2.1266367435455322 \n",
      "Iteration: 165. Loss: 2.155118465423584 \n",
      "Iteration: 166. Loss: 2.15141224861145 \n",
      "Iteration: 167. Loss: 2.1719655990600586 \n",
      "Iteration: 168. Loss: 2.133720636367798 \n",
      "Iteration: 169. Loss: 2.165379524230957 \n",
      "Iteration: 170. Loss: 2.1334645748138428 \n",
      "Iteration: 171. Loss: 2.138093948364258 \n",
      "Iteration: 172. Loss: 2.151308536529541 \n",
      "Iteration: 173. Loss: 2.1462996006011963 \n",
      "Iteration: 174. Loss: 2.1427810192108154 \n",
      "Iteration: 175. Loss: 2.1164700984954834 \n",
      "Iteration: 176. Loss: 2.145448684692383 \n",
      "Iteration: 177. Loss: 2.1360023021698 \n",
      "Iteration: 178. Loss: 2.1557602882385254 \n",
      "Iteration: 179. Loss: 2.138399600982666 \n",
      "Iteration: 180. Loss: 2.142441749572754 \n",
      "Iteration: 181. Loss: 2.163407325744629 \n",
      "Iteration: 182. Loss: 2.1447458267211914 \n",
      "Iteration: 183. Loss: 2.13332462310791 \n",
      "Iteration: 184. Loss: 2.1573989391326904 \n",
      "Iteration: 185. Loss: 2.1483960151672363 \n",
      "Iteration: 186. Loss: 2.147233486175537 \n",
      "Iteration: 187. Loss: 2.1468589305877686 \n",
      "Iteration: 188. Loss: 2.1474573612213135 \n",
      "Iteration: 189. Loss: 2.115640640258789 \n",
      "Iteration: 190. Loss: 2.12753963470459 \n",
      "Iteration: 191. Loss: 2.114175796508789 \n",
      "Iteration: 192. Loss: 2.158923864364624 \n",
      "Iteration: 193. Loss: 2.0886189937591553 \n",
      "Iteration: 194. Loss: 2.1163666248321533 \n",
      "Iteration: 195. Loss: 2.1381452083587646 \n",
      "Iteration: 196. Loss: 2.1284029483795166 \n",
      "Iteration: 197. Loss: 2.101550340652466 \n",
      "Iteration: 198. Loss: 2.1359152793884277 \n",
      "Iteration: 199. Loss: 2.1003732681274414 \n",
      "Iteration: 200. Loss: 2.123297691345215 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 201. Loss: 2.1084706783294678 \n",
      "Iteration: 202. Loss: 2.1044504642486572 \n",
      "Iteration: 203. Loss: 2.12017822265625 \n",
      "Iteration: 204. Loss: 2.125405788421631 \n",
      "Iteration: 205. Loss: 2.098329782485962 \n",
      "Iteration: 206. Loss: 2.111501455307007 \n",
      "Iteration: 207. Loss: 2.1259233951568604 \n",
      "Iteration: 208. Loss: 2.113164186477661 \n",
      "Iteration: 209. Loss: 2.1181282997131348 \n",
      "Iteration: 210. Loss: 2.099489688873291 \n",
      "Iteration: 211. Loss: 2.119657516479492 \n",
      "Iteration: 212. Loss: 2.0899267196655273 \n",
      "Iteration: 213. Loss: 2.104435682296753 \n",
      "Iteration: 214. Loss: 2.0891735553741455 \n",
      "Iteration: 215. Loss: 2.1229751110076904 \n",
      "Iteration: 216. Loss: 2.1111249923706055 \n",
      "Iteration: 217. Loss: 2.1073427200317383 \n",
      "Iteration: 218. Loss: 2.093324661254883 \n",
      "Iteration: 219. Loss: 2.104048728942871 \n",
      "Iteration: 220. Loss: 2.1089649200439453 \n",
      "Iteration: 221. Loss: 2.079092502593994 \n",
      "Iteration: 222. Loss: 2.0961503982543945 \n",
      "Iteration: 223. Loss: 2.0825626850128174 \n",
      "Iteration: 224. Loss: 2.098996162414551 \n",
      "Iteration: 225. Loss: 2.0887651443481445 \n",
      "Iteration: 226. Loss: 2.1329736709594727 \n",
      "Iteration: 227. Loss: 2.0660977363586426 \n",
      "Iteration: 228. Loss: 2.118865489959717 \n",
      "Iteration: 229. Loss: 2.1281661987304688 \n",
      "Iteration: 230. Loss: 2.115438938140869 \n",
      "Iteration: 231. Loss: 2.067492723464966 \n",
      "Iteration: 232. Loss: 2.0728585720062256 \n",
      "Iteration: 233. Loss: 2.082858085632324 \n",
      "Iteration: 234. Loss: 2.078155040740967 \n",
      "Iteration: 235. Loss: 2.135225772857666 \n",
      "Iteration: 236. Loss: 2.102841854095459 \n",
      "Iteration: 237. Loss: 2.0851244926452637 \n",
      "Iteration: 238. Loss: 2.0910627841949463 \n",
      "Iteration: 239. Loss: 2.0634751319885254 \n",
      "Iteration: 240. Loss: 2.082078456878662 \n",
      "Iteration: 241. Loss: 2.092538833618164 \n",
      "Iteration: 242. Loss: 2.092836856842041 \n",
      "Iteration: 243. Loss: 2.0981132984161377 \n",
      "Iteration: 244. Loss: 2.066286563873291 \n",
      "Iteration: 245. Loss: 2.0778987407684326 \n",
      "Iteration: 246. Loss: 2.0482420921325684 \n",
      "Iteration: 247. Loss: 2.0977752208709717 \n",
      "Iteration: 248. Loss: 2.087437629699707 \n",
      "Iteration: 249. Loss: 2.0592451095581055 \n",
      "Iteration: 250. Loss: 2.1048614978790283 \n",
      "Iteration: 251. Loss: 2.055330753326416 \n",
      "Iteration: 252. Loss: 2.07550311088562 \n",
      "Iteration: 253. Loss: 2.0826973915100098 \n",
      "Iteration: 254. Loss: 2.0483574867248535 \n",
      "Iteration: 255. Loss: 2.0848703384399414 \n",
      "Iteration: 256. Loss: 2.0613510608673096 \n",
      "Iteration: 257. Loss: 2.078434944152832 \n",
      "Iteration: 258. Loss: 2.1123509407043457 \n",
      "Iteration: 259. Loss: 2.0742547512054443 \n",
      "Iteration: 260. Loss: 2.068941354751587 \n",
      "Iteration: 261. Loss: 2.0715456008911133 \n",
      "Iteration: 262. Loss: 2.055790662765503 \n",
      "Iteration: 263. Loss: 2.081249475479126 \n",
      "Iteration: 264. Loss: 2.049185276031494 \n",
      "Iteration: 265. Loss: 2.000985622406006 \n",
      "Iteration: 266. Loss: 2.055441379547119 \n",
      "Iteration: 267. Loss: 2.04542875289917 \n",
      "Iteration: 268. Loss: 2.0145761966705322 \n",
      "Iteration: 269. Loss: 2.0279288291931152 \n",
      "Iteration: 270. Loss: 2.073561191558838 \n",
      "Iteration: 271. Loss: 2.040424346923828 \n",
      "Iteration: 272. Loss: 2.0007107257843018 \n",
      "Iteration: 273. Loss: 2.0300192832946777 \n",
      "Iteration: 274. Loss: 2.0612640380859375 \n",
      "Iteration: 275. Loss: 2.068251609802246 \n",
      "Iteration: 276. Loss: 2.0379891395568848 \n",
      "Iteration: 277. Loss: 2.0568435192108154 \n",
      "Iteration: 278. Loss: 2.0531768798828125 \n",
      "Iteration: 279. Loss: 2.014772653579712 \n",
      "Iteration: 280. Loss: 2.0429816246032715 \n",
      "Iteration: 281. Loss: 2.024104118347168 \n",
      "Iteration: 282. Loss: 2.014029026031494 \n",
      "Iteration: 283. Loss: 2.0613760948181152 \n",
      "Iteration: 284. Loss: 2.0285985469818115 \n",
      "Iteration: 285. Loss: 2.0320000648498535 \n",
      "Iteration: 286. Loss: 2.059277296066284 \n",
      "Iteration: 287. Loss: 2.0307581424713135 \n",
      "Iteration: 288. Loss: 2.059317111968994 \n",
      "Iteration: 289. Loss: 2.01578950881958 \n",
      "Iteration: 290. Loss: 2.0888314247131348 \n",
      "Iteration: 291. Loss: 2.0159502029418945 \n",
      "Iteration: 292. Loss: 1.987349271774292 \n",
      "Iteration: 293. Loss: 2.0382914543151855 \n",
      "Iteration: 294. Loss: 2.042579174041748 \n",
      "Iteration: 295. Loss: 2.0268731117248535 \n",
      "Iteration: 296. Loss: 2.0597739219665527 \n",
      "Iteration: 297. Loss: 2.0354819297790527 \n",
      "Iteration: 298. Loss: 2.0192978382110596 \n",
      "Iteration: 299. Loss: 2.0601789951324463 \n",
      "Iteration: 300. Loss: 1.9948996305465698 \n",
      "Iteration: 301. Loss: 2.029294013977051 \n",
      "Iteration: 302. Loss: 2.008345365524292 \n",
      "Iteration: 303. Loss: 2.0360770225524902 \n",
      "Iteration: 304. Loss: 2.0247015953063965 \n",
      "Iteration: 305. Loss: 1.9897522926330566 \n",
      "Iteration: 306. Loss: 2.008751630783081 \n",
      "Iteration: 307. Loss: 2.039053440093994 \n",
      "Iteration: 308. Loss: 2.0187768936157227 \n",
      "Iteration: 309. Loss: 2.006636142730713 \n",
      "Iteration: 310. Loss: 2.0151188373565674 \n",
      "Iteration: 311. Loss: 2.010399580001831 \n",
      "Iteration: 312. Loss: 2.0111587047576904 \n",
      "Iteration: 313. Loss: 2.059668779373169 \n",
      "Iteration: 314. Loss: 2.0003888607025146 \n",
      "Iteration: 315. Loss: 2.009415864944458 \n",
      "Iteration: 316. Loss: 1.9864181280136108 \n",
      "Iteration: 317. Loss: 2.0203146934509277 \n",
      "Iteration: 318. Loss: 2.040471076965332 \n",
      "Iteration: 319. Loss: 2.0231385231018066 \n",
      "Iteration: 320. Loss: 2.0164296627044678 \n",
      "Iteration: 321. Loss: 1.9996764659881592 \n",
      "Iteration: 322. Loss: 1.997567892074585 \n",
      "Iteration: 323. Loss: 2.0102896690368652 \n",
      "Iteration: 324. Loss: 2.008456230163574 \n",
      "Iteration: 325. Loss: 2.0175788402557373 \n",
      "Iteration: 326. Loss: 2.0137462615966797 \n",
      "Iteration: 327. Loss: 2.0018482208251953 \n",
      "Iteration: 328. Loss: 2.0048601627349854 \n",
      "Iteration: 329. Loss: 1.9620976448059082 \n",
      "Iteration: 330. Loss: 1.985381007194519 \n",
      "Iteration: 331. Loss: 1.9959040880203247 \n",
      "Iteration: 332. Loss: 1.9829020500183105 \n",
      "Iteration: 333. Loss: 2.013786792755127 \n",
      "Iteration: 334. Loss: 1.9759083986282349 \n",
      "Iteration: 335. Loss: 2.009866237640381 \n",
      "Iteration: 336. Loss: 2.0056638717651367 \n",
      "Iteration: 337. Loss: 1.9992010593414307 \n",
      "Iteration: 338. Loss: 1.9986164569854736 \n",
      "Iteration: 339. Loss: 1.978583812713623 \n",
      "Iteration: 340. Loss: 1.975022315979004 \n",
      "Iteration: 341. Loss: 1.9801459312438965 \n",
      "Iteration: 342. Loss: 1.9690855741500854 \n",
      "Iteration: 343. Loss: 1.9752490520477295 \n",
      "Iteration: 344. Loss: 1.9905799627304077 \n",
      "Iteration: 345. Loss: 1.982580304145813 \n",
      "Iteration: 346. Loss: 1.9959782361984253 \n",
      "Iteration: 347. Loss: 2.006667375564575 \n",
      "Iteration: 348. Loss: 1.9660332202911377 \n",
      "Iteration: 349. Loss: 2.027432918548584 \n",
      "Iteration: 350. Loss: 1.997257947921753 \n",
      "Iteration: 351. Loss: 1.9723206758499146 \n",
      "Iteration: 352. Loss: 1.9927821159362793 \n",
      "Iteration: 353. Loss: 1.9956806898117065 \n",
      "Iteration: 354. Loss: 1.9482067823410034 \n",
      "Iteration: 355. Loss: 1.9662797451019287 \n",
      "Iteration: 356. Loss: 2.010248899459839 \n",
      "Iteration: 357. Loss: 1.9676482677459717 \n",
      "Iteration: 358. Loss: 1.9643512964248657 \n",
      "Iteration: 359. Loss: 1.9804279804229736 \n",
      "Iteration: 360. Loss: 1.982192873954773 \n",
      "Iteration: 361. Loss: 1.9661781787872314 \n",
      "Iteration: 362. Loss: 1.9856904745101929 \n",
      "Iteration: 363. Loss: 1.938918113708496 \n",
      "Iteration: 364. Loss: 2.0083084106445312 \n",
      "Iteration: 365. Loss: 1.9825468063354492 \n",
      "Iteration: 366. Loss: 1.9935460090637207 \n",
      "Iteration: 367. Loss: 1.9686076641082764 \n",
      "Iteration: 368. Loss: 1.981698751449585 \n",
      "Iteration: 369. Loss: 1.9445875883102417 \n",
      "Iteration: 370. Loss: 1.9658408164978027 \n",
      "Iteration: 371. Loss: 1.9849228858947754 \n",
      "Iteration: 372. Loss: 1.9369839429855347 \n",
      "Iteration: 373. Loss: 1.9857666492462158 \n",
      "Iteration: 374. Loss: 1.9316586256027222 \n",
      "Iteration: 375. Loss: 1.9576700925827026 \n",
      "Iteration: 376. Loss: 1.9359527826309204 \n",
      "Iteration: 377. Loss: 1.9339661598205566 \n",
      "Iteration: 378. Loss: 1.9622645378112793 \n",
      "Iteration: 379. Loss: 1.9557586908340454 \n",
      "Iteration: 380. Loss: 2.0158960819244385 \n",
      "Iteration: 381. Loss: 1.9770057201385498 \n",
      "Iteration: 382. Loss: 1.958327293395996 \n",
      "Iteration: 383. Loss: 1.9174838066101074 \n",
      "Iteration: 384. Loss: 1.9654651880264282 \n",
      "Iteration: 385. Loss: 1.9602125883102417 \n",
      "Iteration: 386. Loss: 1.943621039390564 \n",
      "Iteration: 387. Loss: 1.9738305807113647 \n",
      "Iteration: 388. Loss: 1.9356595277786255 \n",
      "Iteration: 389. Loss: 1.9512450695037842 \n",
      "Iteration: 390. Loss: 1.9752272367477417 \n",
      "Iteration: 391. Loss: 1.9017746448516846 \n",
      "Iteration: 392. Loss: 1.9210869073867798 \n",
      "Iteration: 393. Loss: 1.909848928451538 \n",
      "Iteration: 394. Loss: 1.9711418151855469 \n",
      "Iteration: 395. Loss: 1.9185774326324463 \n",
      "Iteration: 396. Loss: 1.9247955083847046 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 397. Loss: 1.950493335723877 \n",
      "Iteration: 398. Loss: 1.9251339435577393 \n",
      "Iteration: 399. Loss: 1.9680157899856567 \n",
      "Iteration: 400. Loss: 1.9683815240859985 \n",
      "Iteration: 401. Loss: 1.9380422830581665 \n",
      "Iteration: 402. Loss: 1.930470585823059 \n",
      "Iteration: 403. Loss: 1.9594213962554932 \n",
      "Iteration: 404. Loss: 1.9457390308380127 \n",
      "Iteration: 405. Loss: 1.9504719972610474 \n",
      "Iteration: 406. Loss: 1.9792784452438354 \n",
      "Iteration: 407. Loss: 1.9489399194717407 \n",
      "Iteration: 408. Loss: 1.9632524251937866 \n",
      "Iteration: 409. Loss: 1.9199154376983643 \n",
      "Iteration: 410. Loss: 1.9382704496383667 \n",
      "Iteration: 411. Loss: 1.9651957750320435 \n",
      "Iteration: 412. Loss: 1.9260274171829224 \n",
      "Iteration: 413. Loss: 1.9568748474121094 \n",
      "Iteration: 414. Loss: 1.9575752019882202 \n",
      "Iteration: 415. Loss: 1.9564698934555054 \n",
      "Iteration: 416. Loss: 1.9218450784683228 \n",
      "Iteration: 417. Loss: 1.941413402557373 \n",
      "Iteration: 418. Loss: 1.9182183742523193 \n",
      "Iteration: 419. Loss: 1.9628018140792847 \n",
      "Iteration: 420. Loss: 1.9350236654281616 \n",
      "Iteration: 421. Loss: 1.937172293663025 \n",
      "Iteration: 422. Loss: 1.9351511001586914 \n",
      "Iteration: 423. Loss: 1.939172387123108 \n",
      "Iteration: 424. Loss: 1.9067460298538208 \n",
      "Iteration: 425. Loss: 1.9053747653961182 \n",
      "Iteration: 426. Loss: 1.8920291662216187 \n",
      "Iteration: 427. Loss: 1.938655138015747 \n",
      "Iteration: 428. Loss: 1.9408318996429443 \n",
      "Iteration: 429. Loss: 1.9718252420425415 \n",
      "Iteration: 430. Loss: 1.9318767786026 \n",
      "Iteration: 431. Loss: 1.9426910877227783 \n",
      "Iteration: 432. Loss: 1.8964364528656006 \n",
      "Iteration: 433. Loss: 1.9486638307571411 \n",
      "Iteration: 434. Loss: 1.9144790172576904 \n",
      "Iteration: 435. Loss: 1.916650414466858 \n",
      "Iteration: 436. Loss: 1.9184705018997192 \n",
      "Iteration: 437. Loss: 1.8784109354019165 \n",
      "Iteration: 438. Loss: 1.919381856918335 \n",
      "Iteration: 439. Loss: 1.9460710287094116 \n",
      "Iteration: 440. Loss: 1.9228183031082153 \n",
      "Iteration: 441. Loss: 1.890059471130371 \n",
      "Iteration: 442. Loss: 1.9299633502960205 \n",
      "Iteration: 443. Loss: 1.9036247730255127 \n",
      "Iteration: 444. Loss: 1.843379020690918 \n",
      "Iteration: 445. Loss: 1.888444423675537 \n",
      "Iteration: 446. Loss: 1.9048763513565063 \n",
      "Iteration: 447. Loss: 1.9416927099227905 \n",
      "Iteration: 448. Loss: 1.934638261795044 \n",
      "Iteration: 449. Loss: 1.9196656942367554 \n",
      "Iteration: 450. Loss: 1.8587837219238281 \n",
      "Iteration: 451. Loss: 1.8536977767944336 \n",
      "Iteration: 452. Loss: 1.8589385747909546 \n",
      "Iteration: 453. Loss: 1.907762050628662 \n",
      "Iteration: 454. Loss: 1.9237174987792969 \n",
      "Iteration: 455. Loss: 1.8834439516067505 \n",
      "Iteration: 456. Loss: 1.8788976669311523 \n",
      "Iteration: 457. Loss: 1.8946839570999146 \n",
      "Iteration: 458. Loss: 1.8780694007873535 \n",
      "Iteration: 459. Loss: 1.9019992351531982 \n",
      "Iteration: 460. Loss: 1.9051311016082764 \n",
      "Iteration: 461. Loss: 1.8878188133239746 \n",
      "Iteration: 462. Loss: 1.9435572624206543 \n",
      "Iteration: 463. Loss: 1.9385409355163574 \n",
      "Iteration: 464. Loss: 1.873978614807129 \n",
      "Iteration: 465. Loss: 1.8845469951629639 \n",
      "Iteration: 466. Loss: 1.9120988845825195 \n",
      "Iteration: 467. Loss: 1.9233028888702393 \n",
      "Iteration: 468. Loss: 1.9146907329559326 \n",
      "Iteration: 469. Loss: 1.872753620147705 \n",
      "Iteration: 470. Loss: 1.8528132438659668 \n",
      "Iteration: 471. Loss: 1.8528339862823486 \n",
      "Iteration: 472. Loss: 1.8770010471343994 \n",
      "Iteration: 473. Loss: 1.8947771787643433 \n",
      "Iteration: 474. Loss: 1.971321702003479 \n",
      "Iteration: 475. Loss: 1.8663804531097412 \n",
      "Iteration: 476. Loss: 1.8671073913574219 \n",
      "Iteration: 477. Loss: 1.8941810131072998 \n",
      "Iteration: 478. Loss: 1.9129481315612793 \n",
      "Iteration: 479. Loss: 1.8473172187805176 \n",
      "Iteration: 480. Loss: 1.856452226638794 \n",
      "Iteration: 481. Loss: 1.9124069213867188 \n",
      "Iteration: 482. Loss: 1.8801050186157227 \n",
      "Iteration: 483. Loss: 1.8656021356582642 \n",
      "Iteration: 484. Loss: 1.905144214630127 \n",
      "Iteration: 485. Loss: 1.8629854917526245 \n",
      "Iteration: 486. Loss: 1.9050740003585815 \n",
      "Iteration: 487. Loss: 1.899930715560913 \n",
      "Iteration: 488. Loss: 1.877425193786621 \n",
      "Iteration: 489. Loss: 1.8713440895080566 \n",
      "Iteration: 490. Loss: 1.911189317703247 \n",
      "Iteration: 491. Loss: 1.844218134880066 \n",
      "Iteration: 492. Loss: 1.9291207790374756 \n",
      "Iteration: 493. Loss: 1.8817286491394043 \n",
      "Iteration: 494. Loss: 1.855629324913025 \n",
      "Iteration: 495. Loss: 1.8807700872421265 \n",
      "Iteration: 496. Loss: 1.8606048822402954 \n",
      "Iteration: 497. Loss: 1.8900221586227417 \n",
      "Iteration: 498. Loss: 1.853607177734375 \n",
      "Iteration: 499. Loss: 1.8463984727859497 \n",
      "Iteration: 500. Loss: 1.8386856317520142 \n",
      "Iteration: 501. Loss: 1.917833685874939 \n",
      "Iteration: 502. Loss: 1.8909319639205933 \n",
      "Iteration: 503. Loss: 1.8920310735702515 \n",
      "Iteration: 504. Loss: 1.8981504440307617 \n",
      "Iteration: 505. Loss: 1.8532207012176514 \n",
      "Iteration: 506. Loss: 1.9001450538635254 \n",
      "Iteration: 507. Loss: 1.829857349395752 \n",
      "Iteration: 508. Loss: 1.8427414894104004 \n",
      "Iteration: 509. Loss: 1.8409173488616943 \n",
      "Iteration: 510. Loss: 1.8460898399353027 \n",
      "Iteration: 511. Loss: 1.8139578104019165 \n",
      "Iteration: 512. Loss: 1.8171709775924683 \n",
      "Iteration: 513. Loss: 1.8117436170578003 \n",
      "Iteration: 514. Loss: 1.8720862865447998 \n",
      "Iteration: 515. Loss: 1.8422812223434448 \n",
      "Iteration: 516. Loss: 1.8637243509292603 \n",
      "Iteration: 517. Loss: 1.8532322645187378 \n",
      "Iteration: 518. Loss: 1.8516807556152344 \n",
      "Iteration: 519. Loss: 1.8498053550720215 \n",
      "Iteration: 520. Loss: 1.8846882581710815 \n",
      "Iteration: 521. Loss: 1.8880302906036377 \n",
      "Iteration: 522. Loss: 1.8083146810531616 \n",
      "Iteration: 523. Loss: 1.8286231756210327 \n",
      "Iteration: 524. Loss: 1.8727467060089111 \n",
      "Iteration: 525. Loss: 1.8958505392074585 \n",
      "Iteration: 526. Loss: 1.839878797531128 \n",
      "Iteration: 527. Loss: 1.8676904439926147 \n",
      "Iteration: 528. Loss: 1.7986587285995483 \n",
      "Iteration: 529. Loss: 1.7525031566619873 \n",
      "Iteration: 530. Loss: 1.7992175817489624 \n",
      "Iteration: 531. Loss: 1.8452041149139404 \n",
      "Iteration: 532. Loss: 1.8554996252059937 \n",
      "Iteration: 533. Loss: 1.8084003925323486 \n",
      "Iteration: 534. Loss: 1.8486906290054321 \n",
      "Iteration: 535. Loss: 1.8746833801269531 \n",
      "Iteration: 536. Loss: 1.8505178689956665 \n",
      "Iteration: 537. Loss: 1.8574529886245728 \n",
      "Iteration: 538. Loss: 1.8574247360229492 \n",
      "Iteration: 539. Loss: 1.8485450744628906 \n",
      "Iteration: 540. Loss: 1.853943943977356 \n",
      "Iteration: 541. Loss: 1.8477847576141357 \n",
      "Iteration: 542. Loss: 1.808786153793335 \n",
      "Iteration: 543. Loss: 1.8721930980682373 \n",
      "Iteration: 544. Loss: 1.8435883522033691 \n",
      "Iteration: 545. Loss: 1.8786805868148804 \n",
      "Iteration: 546. Loss: 1.7866343259811401 \n",
      "Iteration: 547. Loss: 1.8303054571151733 \n",
      "Iteration: 548. Loss: 1.838530421257019 \n",
      "Iteration: 549. Loss: 1.838067650794983 \n",
      "Iteration: 550. Loss: 1.785829782485962 \n",
      "Iteration: 551. Loss: 1.864306926727295 \n",
      "Iteration: 552. Loss: 1.87343430519104 \n",
      "Iteration: 553. Loss: 1.7834789752960205 \n",
      "Iteration: 554. Loss: 1.8389396667480469 \n",
      "Iteration: 555. Loss: 1.8572808504104614 \n",
      "Iteration: 556. Loss: 1.8140156269073486 \n",
      "Iteration: 557. Loss: 1.8389379978179932 \n",
      "Iteration: 558. Loss: 1.8244699239730835 \n",
      "Iteration: 559. Loss: 1.8261651992797852 \n",
      "Iteration: 560. Loss: 1.8035011291503906 \n",
      "Iteration: 561. Loss: 1.8251919746398926 \n",
      "Iteration: 562. Loss: 1.8041495084762573 \n",
      "Iteration: 563. Loss: 1.8443799018859863 \n",
      "Iteration: 564. Loss: 1.7901430130004883 \n",
      "Iteration: 565. Loss: 1.843595027923584 \n",
      "Iteration: 566. Loss: 1.8238348960876465 \n",
      "Iteration: 567. Loss: 1.7750520706176758 \n",
      "Iteration: 568. Loss: 1.8347843885421753 \n",
      "Iteration: 569. Loss: 1.8036835193634033 \n",
      "Iteration: 570. Loss: 1.8265992403030396 \n",
      "Iteration: 571. Loss: 1.785738229751587 \n",
      "Iteration: 572. Loss: 1.7836511135101318 \n",
      "Iteration: 573. Loss: 1.8632577657699585 \n",
      "Iteration: 574. Loss: 1.8002630472183228 \n",
      "Iteration: 575. Loss: 1.8331321477890015 \n",
      "Iteration: 576. Loss: 1.8006675243377686 \n",
      "Iteration: 577. Loss: 1.8104056119918823 \n",
      "Iteration: 578. Loss: 1.8151649236679077 \n",
      "Iteration: 579. Loss: 1.8219587802886963 \n",
      "Iteration: 580. Loss: 1.7642756700515747 \n",
      "Iteration: 581. Loss: 1.7869577407836914 \n",
      "Iteration: 582. Loss: 1.8480697870254517 \n",
      "Iteration: 583. Loss: 1.8406492471694946 \n",
      "Iteration: 584. Loss: 1.8164801597595215 \n",
      "Iteration: 585. Loss: 1.8411833047866821 \n",
      "Iteration: 586. Loss: 1.8038532733917236 \n",
      "Iteration: 587. Loss: 1.8018971681594849 \n",
      "Iteration: 588. Loss: 1.8157631158828735 \n",
      "Iteration: 589. Loss: 1.8008536100387573 \n",
      "Iteration: 590. Loss: 1.7305744886398315 \n",
      "Iteration: 591. Loss: 1.808779001235962 \n",
      "Iteration: 592. Loss: 1.8195745944976807 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 593. Loss: 1.7730008363723755 \n",
      "Iteration: 594. Loss: 1.8095098733901978 \n",
      "Iteration: 595. Loss: 1.7886381149291992 \n",
      "Iteration: 596. Loss: 1.8452180624008179 \n",
      "Iteration: 597. Loss: 1.8093093633651733 \n",
      "Iteration: 598. Loss: 1.8076916933059692 \n",
      "Iteration: 599. Loss: 1.8489094972610474 \n",
      "Iteration: 600. Loss: 1.7659666538238525 \n",
      "Iteration: 601. Loss: 1.8282470703125 \n",
      "Iteration: 602. Loss: 1.7988462448120117 \n",
      "Iteration: 603. Loss: 1.871422290802002 \n",
      "Iteration: 604. Loss: 1.7504616975784302 \n",
      "Iteration: 605. Loss: 1.7723450660705566 \n",
      "Iteration: 606. Loss: 1.8199061155319214 \n",
      "Iteration: 607. Loss: 1.7961925268173218 \n",
      "Iteration: 608. Loss: 1.7323939800262451 \n",
      "Iteration: 609. Loss: 1.8093081712722778 \n",
      "Iteration: 610. Loss: 1.8296630382537842 \n",
      "Iteration: 611. Loss: 1.7784936428070068 \n",
      "Iteration: 612. Loss: 1.7185752391815186 \n",
      "Iteration: 613. Loss: 1.773566484451294 \n",
      "Iteration: 614. Loss: 1.804080605506897 \n",
      "Iteration: 615. Loss: 1.7680031061172485 \n",
      "Iteration: 616. Loss: 1.7671741247177124 \n",
      "Iteration: 617. Loss: 1.7419955730438232 \n",
      "Iteration: 618. Loss: 1.8100837469100952 \n",
      "Iteration: 619. Loss: 1.8009002208709717 \n",
      "Iteration: 620. Loss: 1.8167556524276733 \n",
      "Iteration: 621. Loss: 1.803368330001831 \n",
      "Iteration: 622. Loss: 1.7683104276657104 \n",
      "Iteration: 623. Loss: 1.8091953992843628 \n",
      "Iteration: 624. Loss: 1.8232712745666504 \n",
      "Iteration: 625. Loss: 1.7022022008895874 \n",
      "Iteration: 626. Loss: 1.7683219909667969 \n",
      "Iteration: 627. Loss: 1.7435240745544434 \n",
      "Iteration: 628. Loss: 1.762783169746399 \n",
      "Iteration: 629. Loss: 1.7491052150726318 \n",
      "Iteration: 630. Loss: 1.749362826347351 \n",
      "Iteration: 631. Loss: 1.8408424854278564 \n",
      "Iteration: 632. Loss: 1.781917691230774 \n",
      "Iteration: 633. Loss: 1.8028345108032227 \n",
      "Iteration: 634. Loss: 1.7502363920211792 \n",
      "Iteration: 635. Loss: 1.748343825340271 \n",
      "Iteration: 636. Loss: 1.7638005018234253 \n",
      "Iteration: 637. Loss: 1.7597585916519165 \n",
      "Iteration: 638. Loss: 1.7886639833450317 \n",
      "Iteration: 639. Loss: 1.834134817123413 \n",
      "Iteration: 640. Loss: 1.790225625038147 \n",
      "Iteration: 641. Loss: 1.7945747375488281 \n",
      "Iteration: 642. Loss: 1.7617003917694092 \n",
      "Iteration: 643. Loss: 1.7376290559768677 \n",
      "Iteration: 644. Loss: 1.8064374923706055 \n",
      "Iteration: 645. Loss: 1.7621771097183228 \n",
      "Iteration: 646. Loss: 1.7870982885360718 \n",
      "Iteration: 647. Loss: 1.7407009601593018 \n",
      "Iteration: 648. Loss: 1.7210876941680908 \n",
      "Iteration: 649. Loss: 1.7636866569519043 \n",
      "Iteration: 650. Loss: 1.814406394958496 \n",
      "Iteration: 651. Loss: 1.8028244972229004 \n",
      "Iteration: 652. Loss: 1.824434757232666 \n",
      "Iteration: 653. Loss: 1.7504644393920898 \n",
      "Iteration: 654. Loss: 1.7651172876358032 \n",
      "Iteration: 655. Loss: 1.7439396381378174 \n",
      "Iteration: 656. Loss: 1.7480541467666626 \n",
      "Iteration: 657. Loss: 1.729648470878601 \n",
      "Iteration: 658. Loss: 1.6716949939727783 \n",
      "Iteration: 659. Loss: 1.7306612730026245 \n",
      "Iteration: 660. Loss: 1.824103593826294 \n",
      "Iteration: 661. Loss: 1.7379072904586792 \n",
      "Iteration: 662. Loss: 1.7796708345413208 \n",
      "Iteration: 663. Loss: 1.7013616561889648 \n",
      "Iteration: 664. Loss: 1.735459566116333 \n",
      "Iteration: 665. Loss: 1.7228896617889404 \n",
      "Iteration: 666. Loss: 1.7497378587722778 \n",
      "Iteration: 667. Loss: 1.7382595539093018 \n",
      "Iteration: 668. Loss: 1.7245734930038452 \n",
      "Iteration: 669. Loss: 1.7416807413101196 \n",
      "Iteration: 670. Loss: 1.7575774192810059 \n",
      "Iteration: 671. Loss: 1.7255473136901855 \n",
      "Iteration: 672. Loss: 1.776139497756958 \n",
      "Iteration: 673. Loss: 1.7452778816223145 \n",
      "Iteration: 674. Loss: 1.771378993988037 \n",
      "Iteration: 675. Loss: 1.7015554904937744 \n",
      "Iteration: 676. Loss: 1.7695024013519287 \n",
      "Iteration: 677. Loss: 1.7908952236175537 \n",
      "Iteration: 678. Loss: 1.7457865476608276 \n",
      "Iteration: 679. Loss: 1.7559622526168823 \n",
      "Iteration: 680. Loss: 1.7404866218566895 \n",
      "Iteration: 681. Loss: 1.8160653114318848 \n",
      "Iteration: 682. Loss: 1.7547780275344849 \n",
      "Iteration: 683. Loss: 1.733709454536438 \n",
      "Iteration: 684. Loss: 1.7502247095108032 \n",
      "Iteration: 685. Loss: 1.6639004945755005 \n",
      "Iteration: 686. Loss: 1.7299803495407104 \n",
      "Iteration: 687. Loss: 1.685193657875061 \n",
      "Iteration: 688. Loss: 1.755804181098938 \n",
      "Iteration: 689. Loss: 1.7724711894989014 \n",
      "Iteration: 690. Loss: 1.7439677715301514 \n",
      "Iteration: 691. Loss: 1.7240593433380127 \n",
      "Iteration: 692. Loss: 1.713602900505066 \n",
      "Iteration: 693. Loss: 1.7851866483688354 \n",
      "Iteration: 694. Loss: 1.7305266857147217 \n",
      "Iteration: 695. Loss: 1.6959055662155151 \n",
      "Iteration: 696. Loss: 1.6622986793518066 \n",
      "Iteration: 697. Loss: 1.7128958702087402 \n",
      "Iteration: 698. Loss: 1.7509068250656128 \n",
      "Iteration: 699. Loss: 1.7006950378417969 \n",
      "Iteration: 700. Loss: 1.6732324361801147 \n",
      "Iteration: 701. Loss: 1.7460330724716187 \n",
      "Iteration: 702. Loss: 1.7521954774856567 \n",
      "Iteration: 703. Loss: 1.698713779449463 \n",
      "Iteration: 704. Loss: 1.7050979137420654 \n",
      "Iteration: 705. Loss: 1.7659798860549927 \n",
      "Iteration: 706. Loss: 1.7267886400222778 \n",
      "Iteration: 707. Loss: 1.7426186800003052 \n",
      "Iteration: 708. Loss: 1.7519606351852417 \n",
      "Iteration: 709. Loss: 1.6633965969085693 \n",
      "Iteration: 710. Loss: 1.6915326118469238 \n",
      "Iteration: 711. Loss: 1.7179043292999268 \n",
      "Iteration: 712. Loss: 1.7295118570327759 \n",
      "Iteration: 713. Loss: 1.7397961616516113 \n",
      "Iteration: 714. Loss: 1.6905889511108398 \n",
      "Iteration: 715. Loss: 1.7190338373184204 \n",
      "Iteration: 716. Loss: 1.7173857688903809 \n",
      "Iteration: 717. Loss: 1.6883330345153809 \n",
      "Iteration: 718. Loss: 1.6344349384307861 \n",
      "Iteration: 719. Loss: 1.6477845907211304 \n",
      "Iteration: 720. Loss: 1.7295868396759033 \n",
      "Iteration: 721. Loss: 1.6852203607559204 \n",
      "Iteration: 722. Loss: 1.7049708366394043 \n",
      "Iteration: 723. Loss: 1.6678141355514526 \n",
      "Iteration: 724. Loss: 1.6405818462371826 \n",
      "Iteration: 725. Loss: 1.7502262592315674 \n",
      "Iteration: 726. Loss: 1.7343173027038574 \n",
      "Iteration: 727. Loss: 1.7039450407028198 \n",
      "Iteration: 728. Loss: 1.7408069372177124 \n",
      "Iteration: 729. Loss: 1.7019531726837158 \n",
      "Iteration: 730. Loss: 1.747023582458496 \n",
      "Iteration: 731. Loss: 1.689031958580017 \n",
      "Iteration: 732. Loss: 1.6902079582214355 \n",
      "Iteration: 733. Loss: 1.744600534439087 \n",
      "Iteration: 734. Loss: 1.7089251279830933 \n",
      "Iteration: 735. Loss: 1.6670259237289429 \n",
      "Iteration: 736. Loss: 1.7720825672149658 \n",
      "Iteration: 737. Loss: 1.7519968748092651 \n",
      "Iteration: 738. Loss: 1.7017594575881958 \n",
      "Iteration: 739. Loss: 1.7301820516586304 \n",
      "Iteration: 740. Loss: 1.7465431690216064 \n",
      "Iteration: 741. Loss: 1.6554166078567505 \n",
      "Iteration: 742. Loss: 1.719754934310913 \n",
      "Iteration: 743. Loss: 1.6881883144378662 \n",
      "Iteration: 744. Loss: 1.6571612358093262 \n",
      "Iteration: 745. Loss: 1.6502020359039307 \n",
      "Iteration: 746. Loss: 1.7268222570419312 \n",
      "Iteration: 747. Loss: 1.759230613708496 \n",
      "Iteration: 748. Loss: 1.659722924232483 \n",
      "Iteration: 749. Loss: 1.6581841707229614 \n",
      "Iteration: 750. Loss: 1.7201766967773438 \n",
      "Iteration: 751. Loss: 1.698136806488037 \n",
      "Iteration: 752. Loss: 1.6748042106628418 \n",
      "Iteration: 753. Loss: 1.6699609756469727 \n",
      "Iteration: 754. Loss: 1.679215431213379 \n",
      "Iteration: 755. Loss: 1.657230019569397 \n",
      "Iteration: 756. Loss: 1.706913709640503 \n",
      "Iteration: 757. Loss: 1.7693942785263062 \n",
      "Iteration: 758. Loss: 1.7089219093322754 \n",
      "Iteration: 759. Loss: 1.698176622390747 \n",
      "Iteration: 760. Loss: 1.6605539321899414 \n",
      "Iteration: 761. Loss: 1.619317650794983 \n",
      "Iteration: 762. Loss: 1.628272533416748 \n",
      "Iteration: 763. Loss: 1.681465744972229 \n",
      "Iteration: 764. Loss: 1.6817108392715454 \n",
      "Iteration: 765. Loss: 1.702515721321106 \n",
      "Iteration: 766. Loss: 1.6837599277496338 \n",
      "Iteration: 767. Loss: 1.6224582195281982 \n",
      "Iteration: 768. Loss: 1.6402232646942139 \n",
      "Iteration: 769. Loss: 1.717618465423584 \n",
      "Iteration: 770. Loss: 1.7045482397079468 \n",
      "Iteration: 771. Loss: 1.6884536743164062 \n",
      "Iteration: 772. Loss: 1.6377099752426147 \n",
      "Iteration: 773. Loss: 1.7573845386505127 \n",
      "Iteration: 774. Loss: 1.7051641941070557 \n",
      "Iteration: 775. Loss: 1.6762737035751343 \n",
      "Iteration: 776. Loss: 1.637272834777832 \n",
      "Iteration: 777. Loss: 1.6713768243789673 \n",
      "Iteration: 778. Loss: 1.63840913772583 \n",
      "Iteration: 779. Loss: 1.6539568901062012 \n",
      "Iteration: 780. Loss: 1.650510311126709 \n",
      "Iteration: 781. Loss: 1.663509488105774 \n",
      "Iteration: 782. Loss: 1.6974433660507202 \n",
      "Iteration: 783. Loss: 1.6095516681671143 \n",
      "Iteration: 784. Loss: 1.6587620973587036 \n",
      "Iteration: 785. Loss: 1.6506327390670776 \n",
      "Iteration: 786. Loss: 1.7119245529174805 \n",
      "Iteration: 787. Loss: 1.6744108200073242 \n",
      "Iteration: 788. Loss: 1.7013499736785889 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 789. Loss: 1.5974308252334595 \n",
      "Iteration: 790. Loss: 1.723865032196045 \n",
      "Iteration: 791. Loss: 1.6741849184036255 \n",
      "Iteration: 792. Loss: 1.7250269651412964 \n",
      "Iteration: 793. Loss: 1.6607288122177124 \n",
      "Iteration: 794. Loss: 1.6639024019241333 \n",
      "Iteration: 795. Loss: 1.6293072700500488 \n",
      "Iteration: 796. Loss: 1.6821627616882324 \n",
      "Iteration: 797. Loss: 1.6723045110702515 \n",
      "Iteration: 798. Loss: 1.7023082971572876 \n",
      "Iteration: 799. Loss: 1.625656008720398 \n",
      "Iteration: 800. Loss: 1.6372276544570923 \n",
      "Iteration: 801. Loss: 1.6740310192108154 \n",
      "Iteration: 802. Loss: 1.6823914051055908 \n",
      "Iteration: 803. Loss: 1.6960493326187134 \n",
      "Iteration: 804. Loss: 1.684924602508545 \n",
      "Iteration: 805. Loss: 1.6761441230773926 \n",
      "Iteration: 806. Loss: 1.6576277017593384 \n",
      "Iteration: 807. Loss: 1.6545939445495605 \n",
      "Iteration: 808. Loss: 1.6469833850860596 \n",
      "Iteration: 809. Loss: 1.6613688468933105 \n",
      "Iteration: 810. Loss: 1.630854606628418 \n",
      "Iteration: 811. Loss: 1.6146131753921509 \n",
      "Iteration: 812. Loss: 1.6320126056671143 \n",
      "Iteration: 813. Loss: 1.697421908378601 \n",
      "Iteration: 814. Loss: 1.6926791667938232 \n",
      "Iteration: 815. Loss: 1.6574876308441162 \n",
      "Iteration: 816. Loss: 1.7056128978729248 \n",
      "Iteration: 817. Loss: 1.573581337928772 \n",
      "Iteration: 818. Loss: 1.6488441228866577 \n",
      "Iteration: 819. Loss: 1.6412570476531982 \n",
      "Iteration: 820. Loss: 1.6552189588546753 \n",
      "Iteration: 821. Loss: 1.6685928106307983 \n",
      "Iteration: 822. Loss: 1.639707088470459 \n",
      "Iteration: 823. Loss: 1.6450196504592896 \n",
      "Iteration: 824. Loss: 1.73164963722229 \n",
      "Iteration: 825. Loss: 1.6272854804992676 \n",
      "Iteration: 826. Loss: 1.7142627239227295 \n",
      "Iteration: 827. Loss: 1.585607647895813 \n",
      "Iteration: 828. Loss: 1.6471136808395386 \n",
      "Iteration: 829. Loss: 1.6278538703918457 \n",
      "Iteration: 830. Loss: 1.6467368602752686 \n",
      "Iteration: 831. Loss: 1.752286434173584 \n",
      "Iteration: 832. Loss: 1.6265071630477905 \n",
      "Iteration: 833. Loss: 1.595033884048462 \n",
      "Iteration: 834. Loss: 1.6899486780166626 \n",
      "Iteration: 835. Loss: 1.6500974893569946 \n",
      "Iteration: 836. Loss: 1.6898622512817383 \n",
      "Iteration: 837. Loss: 1.5802083015441895 \n",
      "Iteration: 838. Loss: 1.6335129737854004 \n",
      "Iteration: 839. Loss: 1.6128720045089722 \n",
      "Iteration: 840. Loss: 1.6383795738220215 \n",
      "Iteration: 841. Loss: 1.6369848251342773 \n",
      "Iteration: 842. Loss: 1.6427977085113525 \n",
      "Iteration: 843. Loss: 1.6678400039672852 \n",
      "Iteration: 844. Loss: 1.6906977891921997 \n",
      "Iteration: 845. Loss: 1.630478858947754 \n",
      "Iteration: 846. Loss: 1.6436156034469604 \n",
      "Iteration: 847. Loss: 1.627829909324646 \n",
      "Iteration: 848. Loss: 1.6111429929733276 \n",
      "Iteration: 849. Loss: 1.5960687398910522 \n",
      "Iteration: 850. Loss: 1.6513956785202026 \n",
      "Iteration: 851. Loss: 1.5932366847991943 \n",
      "Iteration: 852. Loss: 1.6609063148498535 \n",
      "Iteration: 853. Loss: 1.6212633848190308 \n",
      "Iteration: 854. Loss: 1.6340969800949097 \n",
      "Iteration: 855. Loss: 1.6147825717926025 \n",
      "Iteration: 856. Loss: 1.5948251485824585 \n",
      "Iteration: 857. Loss: 1.6819676160812378 \n",
      "Iteration: 858. Loss: 1.6126741170883179 \n",
      "Iteration: 859. Loss: 1.6309328079223633 \n",
      "Iteration: 860. Loss: 1.6038451194763184 \n",
      "Iteration: 861. Loss: 1.6722217798233032 \n",
      "Iteration: 862. Loss: 1.5881274938583374 \n",
      "Iteration: 863. Loss: 1.6418406963348389 \n",
      "Iteration: 864. Loss: 1.6419824361801147 \n",
      "Iteration: 865. Loss: 1.6927707195281982 \n",
      "Iteration: 866. Loss: 1.6306391954421997 \n",
      "Iteration: 867. Loss: 1.6710797548294067 \n",
      "Iteration: 868. Loss: 1.5876638889312744 \n",
      "Iteration: 869. Loss: 1.642615556716919 \n",
      "Iteration: 870. Loss: 1.612326979637146 \n",
      "Iteration: 871. Loss: 1.6432979106903076 \n",
      "Iteration: 872. Loss: 1.6728806495666504 \n",
      "Iteration: 873. Loss: 1.6868162155151367 \n",
      "Iteration: 874. Loss: 1.687703251838684 \n",
      "Iteration: 875. Loss: 1.6644359827041626 \n",
      "Iteration: 876. Loss: 1.6033461093902588 \n",
      "Iteration: 877. Loss: 1.6393406391143799 \n",
      "Iteration: 878. Loss: 1.610409140586853 \n",
      "Iteration: 879. Loss: 1.6454005241394043 \n",
      "Iteration: 880. Loss: 1.6007447242736816 \n",
      "Iteration: 881. Loss: 1.6109086275100708 \n",
      "Iteration: 882. Loss: 1.5329077243804932 \n",
      "Iteration: 883. Loss: 1.6723743677139282 \n",
      "Iteration: 884. Loss: 1.6386473178863525 \n",
      "Iteration: 885. Loss: 1.5922250747680664 \n",
      "Iteration: 886. Loss: 1.5431427955627441 \n",
      "Iteration: 887. Loss: 1.6236094236373901 \n",
      "Iteration: 888. Loss: 1.5991952419281006 \n",
      "Iteration: 889. Loss: 1.6483912467956543 \n",
      "Iteration: 890. Loss: 1.584113359451294 \n",
      "Iteration: 891. Loss: 1.586754560470581 \n",
      "Iteration: 892. Loss: 1.577256441116333 \n",
      "Iteration: 893. Loss: 1.6577792167663574 \n",
      "Iteration: 894. Loss: 1.5429500341415405 \n",
      "Iteration: 895. Loss: 1.6100579500198364 \n",
      "Iteration: 896. Loss: 1.6212021112442017 \n",
      "Iteration: 897. Loss: 1.6746490001678467 \n",
      "Iteration: 898. Loss: 1.600821852684021 \n",
      "Iteration: 899. Loss: 1.6129800081253052 \n",
      "Iteration: 900. Loss: 1.558258056640625 \n",
      "Iteration: 901. Loss: 1.6083732843399048 \n",
      "Iteration: 902. Loss: 1.6459550857543945 \n",
      "Iteration: 903. Loss: 1.5206749439239502 \n",
      "Iteration: 904. Loss: 1.6798241138458252 \n",
      "Iteration: 905. Loss: 1.633395791053772 \n",
      "Iteration: 906. Loss: 1.5806686878204346 \n",
      "Iteration: 907. Loss: 1.651050090789795 \n",
      "Iteration: 908. Loss: 1.5428155660629272 \n",
      "Iteration: 909. Loss: 1.616981863975525 \n",
      "Iteration: 910. Loss: 1.5919491052627563 \n",
      "Iteration: 911. Loss: 1.6500587463378906 \n",
      "Iteration: 912. Loss: 1.5695695877075195 \n",
      "Iteration: 913. Loss: 1.531432032585144 \n",
      "Iteration: 914. Loss: 1.586696982383728 \n",
      "Iteration: 915. Loss: 1.6135401725769043 \n",
      "Iteration: 916. Loss: 1.6245836019515991 \n",
      "Iteration: 917. Loss: 1.5835556983947754 \n",
      "Iteration: 918. Loss: 1.5580253601074219 \n",
      "Iteration: 919. Loss: 1.5779772996902466 \n",
      "Iteration: 920. Loss: 1.553683876991272 \n",
      "Iteration: 921. Loss: 1.5412832498550415 \n",
      "Iteration: 922. Loss: 1.5746219158172607 \n",
      "Iteration: 923. Loss: 1.653200626373291 \n",
      "Iteration: 924. Loss: 1.6271138191223145 \n",
      "Iteration: 925. Loss: 1.6096841096878052 \n",
      "Iteration: 926. Loss: 1.5183515548706055 \n",
      "Iteration: 927. Loss: 1.6499056816101074 \n",
      "Iteration: 928. Loss: 1.5754411220550537 \n",
      "Iteration: 929. Loss: 1.5774650573730469 \n",
      "Iteration: 930. Loss: 1.5729248523712158 \n",
      "Iteration: 931. Loss: 1.5694680213928223 \n",
      "Iteration: 932. Loss: 1.60062837600708 \n",
      "Iteration: 933. Loss: 1.5798639059066772 \n",
      "Iteration: 934. Loss: 1.6361464262008667 \n",
      "Iteration: 935. Loss: 1.603631615638733 \n",
      "Iteration: 936. Loss: 1.552909255027771 \n",
      "Iteration: 937. Loss: 1.5881603956222534 \n",
      "Iteration: 938. Loss: 1.5805838108062744 \n",
      "Iteration: 939. Loss: 1.597854495048523 \n",
      "Iteration: 940. Loss: 1.603317141532898 \n",
      "Iteration: 941. Loss: 1.6358115673065186 \n",
      "Iteration: 942. Loss: 1.510230541229248 \n",
      "Iteration: 943. Loss: 1.5585670471191406 \n",
      "Iteration: 944. Loss: 1.5570777654647827 \n",
      "Iteration: 945. Loss: 1.6236093044281006 \n",
      "Iteration: 946. Loss: 1.6163551807403564 \n",
      "Iteration: 947. Loss: 1.573684811592102 \n",
      "Iteration: 948. Loss: 1.5934945344924927 \n",
      "Iteration: 949. Loss: 1.5879943370819092 \n",
      "Iteration: 950. Loss: 1.5162765979766846 \n",
      "Iteration: 951. Loss: 1.634084939956665 \n",
      "Iteration: 952. Loss: 1.5630627870559692 \n",
      "Iteration: 953. Loss: 1.5768253803253174 \n",
      "Iteration: 954. Loss: 1.6419705152511597 \n",
      "Iteration: 955. Loss: 1.526400089263916 \n",
      "Iteration: 956. Loss: 1.6064397096633911 \n",
      "Iteration: 957. Loss: 1.7152806520462036 \n",
      "Iteration: 958. Loss: 1.6520757675170898 \n",
      "Iteration: 959. Loss: 1.5739349126815796 \n",
      "Iteration: 960. Loss: 1.5510749816894531 \n",
      "Iteration: 961. Loss: 1.5799050331115723 \n",
      "Iteration: 962. Loss: 1.5740666389465332 \n",
      "Iteration: 963. Loss: 1.6297489404678345 \n",
      "Iteration: 964. Loss: 1.549883246421814 \n",
      "Iteration: 965. Loss: 1.639095664024353 \n",
      "Iteration: 966. Loss: 1.6439529657363892 \n",
      "Iteration: 967. Loss: 1.51851224899292 \n",
      "Iteration: 968. Loss: 1.5650813579559326 \n",
      "Iteration: 969. Loss: 1.5667407512664795 \n",
      "Iteration: 970. Loss: 1.6432158946990967 \n",
      "Iteration: 971. Loss: 1.6246757507324219 \n",
      "Iteration: 972. Loss: 1.555080533027649 \n",
      "Iteration: 973. Loss: 1.5715446472167969 \n",
      "Iteration: 974. Loss: 1.615088701248169 \n",
      "Iteration: 975. Loss: 1.5980318784713745 \n",
      "Iteration: 976. Loss: 1.5554744005203247 \n",
      "Iteration: 977. Loss: 1.610080599784851 \n",
      "Iteration: 978. Loss: 1.6144697666168213 \n",
      "Iteration: 979. Loss: 1.5615284442901611 \n",
      "Iteration: 980. Loss: 1.610127568244934 \n",
      "Iteration: 981. Loss: 1.5382424592971802 \n",
      "Iteration: 982. Loss: 1.5881975889205933 \n",
      "Iteration: 983. Loss: 1.5904977321624756 \n",
      "Iteration: 984. Loss: 1.585079550743103 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 985. Loss: 1.557947039604187 \n",
      "Iteration: 986. Loss: 1.5489091873168945 \n",
      "Iteration: 987. Loss: 1.5116055011749268 \n",
      "Iteration: 988. Loss: 1.5588123798370361 \n",
      "Iteration: 989. Loss: 1.5546764135360718 \n",
      "Iteration: 990. Loss: 1.5522894859313965 \n",
      "Iteration: 991. Loss: 1.607147216796875 \n",
      "Iteration: 992. Loss: 1.6087623834609985 \n",
      "Iteration: 993. Loss: 1.5396357774734497 \n",
      "Iteration: 994. Loss: 1.5928272008895874 \n",
      "Iteration: 995. Loss: 1.5552982091903687 \n",
      "Iteration: 996. Loss: 1.5901129245758057 \n",
      "Iteration: 997. Loss: 1.5962977409362793 \n",
      "Iteration: 998. Loss: 1.576180100440979 \n",
      "Iteration: 999. Loss: 1.5526649951934814 \n",
      "Iteration: 1000. Loss: 1.5265874862670898 \n",
      "Iteration: 1001. Loss: 1.5821928977966309 \n",
      "Iteration: 1002. Loss: 1.5591440200805664 \n",
      "Iteration: 1003. Loss: 1.5226805210113525 \n",
      "Iteration: 1004. Loss: 1.6445045471191406 \n",
      "Iteration: 1005. Loss: 1.5722312927246094 \n",
      "Iteration: 1006. Loss: 1.5793713331222534 \n",
      "Iteration: 1007. Loss: 1.5292729139328003 \n",
      "Iteration: 1008. Loss: 1.5803107023239136 \n",
      "Iteration: 1009. Loss: 1.6025522947311401 \n",
      "Iteration: 1010. Loss: 1.5843003988265991 \n",
      "Iteration: 1011. Loss: 1.6284490823745728 \n",
      "Iteration: 1012. Loss: 1.5343977212905884 \n",
      "Iteration: 1013. Loss: 1.568739414215088 \n",
      "Iteration: 1014. Loss: 1.6062730550765991 \n",
      "Iteration: 1015. Loss: 1.5187875032424927 \n",
      "Iteration: 1016. Loss: 1.5547326803207397 \n",
      "Iteration: 1017. Loss: 1.6035586595535278 \n",
      "Iteration: 1018. Loss: 1.5962631702423096 \n",
      "Iteration: 1019. Loss: 1.6234512329101562 \n",
      "Iteration: 1020. Loss: 1.4969725608825684 \n",
      "Iteration: 1021. Loss: 1.5425176620483398 \n",
      "Iteration: 1022. Loss: 1.561378002166748 \n",
      "Iteration: 1023. Loss: 1.5330934524536133 \n",
      "Iteration: 1024. Loss: 1.5891478061676025 \n",
      "Iteration: 1025. Loss: 1.5536184310913086 \n",
      "Iteration: 1026. Loss: 1.5459970235824585 \n",
      "Iteration: 1027. Loss: 1.4832885265350342 \n",
      "Iteration: 1028. Loss: 1.5872163772583008 \n",
      "Iteration: 1029. Loss: 1.5571843385696411 \n",
      "Iteration: 1030. Loss: 1.544429063796997 \n",
      "Iteration: 1031. Loss: 1.465869426727295 \n",
      "Iteration: 1032. Loss: 1.5392603874206543 \n",
      "Iteration: 1033. Loss: 1.5374637842178345 \n",
      "Iteration: 1034. Loss: 1.5352058410644531 \n",
      "Iteration: 1035. Loss: 1.526036262512207 \n",
      "Iteration: 1036. Loss: 1.427945852279663 \n",
      "Iteration: 1037. Loss: 1.5216310024261475 \n",
      "Iteration: 1038. Loss: 1.5976775884628296 \n",
      "Iteration: 1039. Loss: 1.5827428102493286 \n",
      "Iteration: 1040. Loss: 1.5682252645492554 \n",
      "Iteration: 1041. Loss: 1.6113985776901245 \n",
      "Iteration: 1042. Loss: 1.4754992723464966 \n",
      "Iteration: 1043. Loss: 1.4967063665390015 \n",
      "Iteration: 1044. Loss: 1.5801787376403809 \n",
      "Iteration: 1045. Loss: 1.5584450960159302 \n",
      "Iteration: 1046. Loss: 1.5747212171554565 \n",
      "Iteration: 1047. Loss: 1.5099459886550903 \n",
      "Iteration: 1048. Loss: 1.4898369312286377 \n",
      "Iteration: 1049. Loss: 1.5740760564804077 \n",
      "Iteration: 1050. Loss: 1.5905708074569702 \n",
      "Iteration: 1051. Loss: 1.558444619178772 \n",
      "Iteration: 1052. Loss: 1.5867384672164917 \n",
      "Iteration: 1053. Loss: 1.565211296081543 \n",
      "Iteration: 1054. Loss: 1.5634377002716064 \n",
      "Iteration: 1055. Loss: 1.5545833110809326 \n",
      "Iteration: 1056. Loss: 1.6128828525543213 \n",
      "Iteration: 1057. Loss: 1.5188233852386475 \n",
      "Iteration: 1058. Loss: 1.5215219259262085 \n",
      "Iteration: 1059. Loss: 1.5720406770706177 \n",
      "Iteration: 1060. Loss: 1.5655444860458374 \n",
      "Iteration: 1061. Loss: 1.4946327209472656 \n",
      "Iteration: 1062. Loss: 1.5064117908477783 \n",
      "Iteration: 1063. Loss: 1.4986984729766846 \n",
      "Iteration: 1064. Loss: 1.5122805833816528 \n",
      "Iteration: 1065. Loss: 1.5429142713546753 \n",
      "Iteration: 1066. Loss: 1.4918535947799683 \n",
      "Iteration: 1067. Loss: 1.5787371397018433 \n",
      "Iteration: 1068. Loss: 1.4951882362365723 \n",
      "Iteration: 1069. Loss: 1.5477250814437866 \n",
      "Iteration: 1070. Loss: 1.598481297492981 \n",
      "Iteration: 1071. Loss: 1.5496059656143188 \n",
      "Iteration: 1072. Loss: 1.48667573928833 \n",
      "Iteration: 1073. Loss: 1.5314767360687256 \n",
      "Iteration: 1074. Loss: 1.5093379020690918 \n",
      "Iteration: 1075. Loss: 1.4887642860412598 \n",
      "Iteration: 1076. Loss: 1.5123742818832397 \n",
      "Iteration: 1077. Loss: 1.507262110710144 \n",
      "Iteration: 1078. Loss: 1.5197068452835083 \n",
      "Iteration: 1079. Loss: 1.5348457098007202 \n",
      "Iteration: 1080. Loss: 1.505568265914917 \n",
      "Iteration: 1081. Loss: 1.5643028020858765 \n",
      "Iteration: 1082. Loss: 1.6043837070465088 \n",
      "Iteration: 1083. Loss: 1.5719608068466187 \n",
      "Iteration: 1084. Loss: 1.552121877670288 \n",
      "Iteration: 1085. Loss: 1.4731999635696411 \n",
      "Iteration: 1086. Loss: 1.5426292419433594 \n",
      "Iteration: 1087. Loss: 1.538792371749878 \n",
      "Iteration: 1088. Loss: 1.4490387439727783 \n",
      "Iteration: 1089. Loss: 1.5867605209350586 \n",
      "Iteration: 1090. Loss: 1.4956023693084717 \n",
      "Iteration: 1091. Loss: 1.4526311159133911 \n",
      "Iteration: 1092. Loss: 1.4782389402389526 \n",
      "Iteration: 1093. Loss: 1.49826979637146 \n",
      "Iteration: 1094. Loss: 1.4860247373580933 \n",
      "Iteration: 1095. Loss: 1.4518083333969116 \n",
      "Iteration: 1096. Loss: 1.450361967086792 \n",
      "Iteration: 1097. Loss: 1.5082526206970215 \n",
      "Iteration: 1098. Loss: 1.4325557947158813 \n",
      "Iteration: 1099. Loss: 1.5164448022842407 \n",
      "Iteration: 1100. Loss: 1.5670760869979858 \n",
      "Iteration: 1101. Loss: 1.5282365083694458 \n",
      "Iteration: 1102. Loss: 1.5121726989746094 \n",
      "Iteration: 1103. Loss: 1.4960006475448608 \n",
      "Iteration: 1104. Loss: 1.4963383674621582 \n",
      "Iteration: 1105. Loss: 1.5651756525039673 \n",
      "Iteration: 1106. Loss: 1.465359091758728 \n",
      "Iteration: 1107. Loss: 1.4799444675445557 \n",
      "Iteration: 1108. Loss: 1.523098111152649 \n",
      "Iteration: 1109. Loss: 1.615565299987793 \n",
      "Iteration: 1110. Loss: 1.4607200622558594 \n",
      "Iteration: 1111. Loss: 1.5918846130371094 \n",
      "Iteration: 1112. Loss: 1.4768240451812744 \n",
      "Iteration: 1113. Loss: 1.4820224046707153 \n",
      "Iteration: 1114. Loss: 1.4920380115509033 \n",
      "Iteration: 1115. Loss: 1.5347110033035278 \n",
      "Iteration: 1116. Loss: 1.6313639879226685 \n",
      "Iteration: 1117. Loss: 1.5461721420288086 \n",
      "Iteration: 1118. Loss: 1.5426348447799683 \n",
      "Iteration: 1119. Loss: 1.5412002801895142 \n",
      "Iteration: 1120. Loss: 1.4650053977966309 \n",
      "Iteration: 1121. Loss: 1.5162626504898071 \n",
      "Iteration: 1122. Loss: 1.46720552444458 \n",
      "Iteration: 1123. Loss: 1.5119001865386963 \n",
      "Iteration: 1124. Loss: 1.495948314666748 \n",
      "Iteration: 1125. Loss: 1.4917311668395996 \n",
      "Iteration: 1126. Loss: 1.5204346179962158 \n",
      "Iteration: 1127. Loss: 1.3849725723266602 \n",
      "Iteration: 1128. Loss: 1.4973649978637695 \n",
      "Iteration: 1129. Loss: 1.4426952600479126 \n",
      "Iteration: 1130. Loss: 1.5053027868270874 \n",
      "Iteration: 1131. Loss: 1.5382325649261475 \n",
      "Iteration: 1132. Loss: 1.4424177408218384 \n",
      "Iteration: 1133. Loss: 1.5203839540481567 \n",
      "Iteration: 1134. Loss: 1.4711297750473022 \n",
      "Iteration: 1135. Loss: 1.4810500144958496 \n",
      "Iteration: 1136. Loss: 1.5031431913375854 \n",
      "Iteration: 1137. Loss: 1.475373387336731 \n",
      "Iteration: 1138. Loss: 1.4851263761520386 \n",
      "Iteration: 1139. Loss: 1.5075160264968872 \n",
      "Iteration: 1140. Loss: 1.4809985160827637 \n",
      "Iteration: 1141. Loss: 1.4546314477920532 \n",
      "Iteration: 1142. Loss: 1.513105869293213 \n",
      "Iteration: 1143. Loss: 1.4199060201644897 \n",
      "Iteration: 1144. Loss: 1.361201524734497 \n",
      "Iteration: 1145. Loss: 1.4370965957641602 \n",
      "Iteration: 1146. Loss: 1.4584983587265015 \n",
      "Iteration: 1147. Loss: 1.454349398612976 \n",
      "Iteration: 1148. Loss: 1.4953962564468384 \n",
      "Iteration: 1149. Loss: 1.4997869729995728 \n",
      "Iteration: 1150. Loss: 1.4641292095184326 \n",
      "Iteration: 1151. Loss: 1.5031646490097046 \n",
      "Iteration: 1152. Loss: 1.5045474767684937 \n",
      "Iteration: 1153. Loss: 1.5204353332519531 \n",
      "Iteration: 1154. Loss: 1.5008271932601929 \n",
      "Iteration: 1155. Loss: 1.5441839694976807 \n",
      "Iteration: 1156. Loss: 1.5015342235565186 \n",
      "Iteration: 1157. Loss: 1.529090404510498 \n",
      "Iteration: 1158. Loss: 1.502798318862915 \n",
      "Iteration: 1159. Loss: 1.524213433265686 \n",
      "Iteration: 1160. Loss: 1.4749929904937744 \n",
      "Iteration: 1161. Loss: 1.4477332830429077 \n",
      "Iteration: 1162. Loss: 1.478474736213684 \n",
      "Iteration: 1163. Loss: 1.493503451347351 \n",
      "Iteration: 1164. Loss: 1.4657166004180908 \n",
      "Iteration: 1165. Loss: 1.4964975118637085 \n",
      "Iteration: 1166. Loss: 1.500127911567688 \n",
      "Iteration: 1167. Loss: 1.4826312065124512 \n",
      "Iteration: 1168. Loss: 1.4521582126617432 \n",
      "Iteration: 1169. Loss: 1.4838905334472656 \n",
      "Iteration: 1170. Loss: 1.4605008363723755 \n",
      "Iteration: 1171. Loss: 1.4965606927871704 \n",
      "Iteration: 1172. Loss: 1.4791419506072998 \n",
      "Iteration: 1173. Loss: 1.4567936658859253 \n",
      "Iteration: 1174. Loss: 1.449510931968689 \n",
      "Iteration: 1175. Loss: 1.5428723096847534 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1176. Loss: 1.4693888425827026 \n",
      "Iteration: 1177. Loss: 1.495317816734314 \n",
      "Iteration: 1178. Loss: 1.4661619663238525 \n",
      "Iteration: 1179. Loss: 1.5266985893249512 \n",
      "Iteration: 1180. Loss: 1.508236289024353 \n",
      "Iteration: 1181. Loss: 1.5418422222137451 \n",
      "Iteration: 1182. Loss: 1.4782496690750122 \n",
      "Iteration: 1183. Loss: 1.4664665460586548 \n",
      "Iteration: 1184. Loss: 1.4230051040649414 \n",
      "Iteration: 1185. Loss: 1.4671907424926758 \n",
      "Iteration: 1186. Loss: 1.4813103675842285 \n",
      "Iteration: 1187. Loss: 1.4496971368789673 \n",
      "Iteration: 1188. Loss: 1.4928609132766724 \n",
      "Iteration: 1189. Loss: 1.3665189743041992 \n",
      "Iteration: 1190. Loss: 1.4422569274902344 \n",
      "Iteration: 1191. Loss: 1.4624457359313965 \n",
      "Iteration: 1192. Loss: 1.3880314826965332 \n",
      "Iteration: 1193. Loss: 1.457821249961853 \n",
      "Iteration: 1194. Loss: 1.4219489097595215 \n",
      "Iteration: 1195. Loss: 1.4254831075668335 \n",
      "Iteration: 1196. Loss: 1.5063984394073486 \n",
      "Iteration: 1197. Loss: 1.4703642129898071 \n",
      "Iteration: 1198. Loss: 1.4693100452423096 \n",
      "Iteration: 1199. Loss: 1.4424024820327759 \n",
      "Iteration: 1200. Loss: 1.4986392259597778 \n",
      "Iteration: 1201. Loss: 1.4983115196228027 \n",
      "Iteration: 1202. Loss: 1.475226879119873 \n",
      "Iteration: 1203. Loss: 1.3854368925094604 \n",
      "Iteration: 1204. Loss: 1.470314621925354 \n",
      "Iteration: 1205. Loss: 1.47365140914917 \n",
      "Iteration: 1206. Loss: 1.4797852039337158 \n",
      "Iteration: 1207. Loss: 1.4539445638656616 \n",
      "Iteration: 1208. Loss: 1.424583077430725 \n",
      "Iteration: 1209. Loss: 1.399601697921753 \n",
      "Iteration: 1210. Loss: 1.5252196788787842 \n",
      "Iteration: 1211. Loss: 1.5201202630996704 \n",
      "Iteration: 1212. Loss: 1.4755103588104248 \n",
      "Iteration: 1213. Loss: 1.4178862571716309 \n",
      "Iteration: 1214. Loss: 1.420128345489502 \n",
      "Iteration: 1215. Loss: 1.46562922000885 \n",
      "Iteration: 1216. Loss: 1.4361166954040527 \n",
      "Iteration: 1217. Loss: 1.4390733242034912 \n",
      "Iteration: 1218. Loss: 1.4586615562438965 \n",
      "Iteration: 1219. Loss: 1.464082956314087 \n",
      "Iteration: 1220. Loss: 1.4005656242370605 \n",
      "Iteration: 1221. Loss: 1.4359676837921143 \n",
      "Iteration: 1222. Loss: 1.459751009941101 \n",
      "Iteration: 1223. Loss: 1.3670802116394043 \n",
      "Iteration: 1224. Loss: 1.467725396156311 \n",
      "Iteration: 1225. Loss: 1.4193283319473267 \n",
      "Iteration: 1226. Loss: 1.3367902040481567 \n",
      "Iteration: 1227. Loss: 1.4591689109802246 \n",
      "Iteration: 1228. Loss: 1.4151901006698608 \n",
      "Iteration: 1229. Loss: 1.3952833414077759 \n",
      "Iteration: 1230. Loss: 1.545285940170288 \n",
      "Iteration: 1231. Loss: 1.5061759948730469 \n",
      "Iteration: 1232. Loss: 1.4237987995147705 \n",
      "Iteration: 1233. Loss: 1.4437118768692017 \n",
      "Iteration: 1234. Loss: 1.5562400817871094 \n",
      "Iteration: 1235. Loss: 1.442622423171997 \n",
      "Iteration: 1236. Loss: 1.4068320989608765 \n",
      "Iteration: 1237. Loss: 1.468520164489746 \n",
      "Iteration: 1238. Loss: 1.4891184568405151 \n",
      "Iteration: 1239. Loss: 1.4918715953826904 \n",
      "Iteration: 1240. Loss: 1.449861764907837 \n",
      "Iteration: 1241. Loss: 1.4786790609359741 \n",
      "Iteration: 1242. Loss: 1.4325628280639648 \n",
      "Iteration: 1243. Loss: 1.4858505725860596 \n",
      "Iteration: 1244. Loss: 1.4166966676712036 \n",
      "Iteration: 1245. Loss: 1.4497779607772827 \n",
      "Iteration: 1246. Loss: 1.413378119468689 \n",
      "Iteration: 1247. Loss: 1.5106256008148193 \n",
      "Iteration: 1248. Loss: 1.502265214920044 \n",
      "Iteration: 1249. Loss: 1.5455697774887085 \n",
      "Iteration: 1250. Loss: 1.3898496627807617 \n",
      "Iteration: 1251. Loss: 1.5106769800186157 \n",
      "Iteration: 1252. Loss: 1.42531156539917 \n",
      "Iteration: 1253. Loss: 1.4862974882125854 \n",
      "Iteration: 1254. Loss: 1.4611128568649292 \n",
      "Iteration: 1255. Loss: 1.3982703685760498 \n",
      "Iteration: 1256. Loss: 1.4110842943191528 \n",
      "Iteration: 1257. Loss: 1.4077779054641724 \n",
      "Iteration: 1258. Loss: 1.4201682806015015 \n",
      "Iteration: 1259. Loss: 1.5142462253570557 \n",
      "Iteration: 1260. Loss: 1.460120439529419 \n",
      "Iteration: 1261. Loss: 1.400160551071167 \n",
      "Iteration: 1262. Loss: 1.463446855545044 \n",
      "Iteration: 1263. Loss: 1.4883887767791748 \n",
      "Iteration: 1264. Loss: 1.4500274658203125 \n",
      "Iteration: 1265. Loss: 1.4761948585510254 \n",
      "Iteration: 1266. Loss: 1.4891093969345093 \n",
      "Iteration: 1267. Loss: 1.3646681308746338 \n",
      "Iteration: 1268. Loss: 1.4257702827453613 \n",
      "Iteration: 1269. Loss: 1.4646148681640625 \n",
      "Iteration: 1270. Loss: 1.3979496955871582 \n",
      "Iteration: 1271. Loss: 1.4621446132659912 \n",
      "Iteration: 1272. Loss: 1.4454896450042725 \n",
      "Iteration: 1273. Loss: 1.389553189277649 \n",
      "Iteration: 1274. Loss: 1.4694997072219849 \n",
      "Iteration: 1275. Loss: 1.4415796995162964 \n",
      "Iteration: 1276. Loss: 1.4110369682312012 \n",
      "Iteration: 1277. Loss: 1.3824360370635986 \n",
      "Iteration: 1278. Loss: 1.3945364952087402 \n",
      "Iteration: 1279. Loss: 1.4051891565322876 \n",
      "Iteration: 1280. Loss: 1.3738913536071777 \n",
      "Iteration: 1281. Loss: 1.429490089416504 \n",
      "Iteration: 1282. Loss: 1.4022842645645142 \n",
      "Iteration: 1283. Loss: 1.364894986152649 \n",
      "Iteration: 1284. Loss: 1.4748353958129883 \n",
      "Iteration: 1285. Loss: 1.4857960939407349 \n",
      "Iteration: 1286. Loss: 1.3837400674819946 \n",
      "Iteration: 1287. Loss: 1.3925498723983765 \n",
      "Iteration: 1288. Loss: 1.4131057262420654 \n",
      "Iteration: 1289. Loss: 1.4236563444137573 \n",
      "Iteration: 1290. Loss: 1.4326492547988892 \n",
      "Iteration: 1291. Loss: 1.4062117338180542 \n",
      "Iteration: 1292. Loss: 1.3840653896331787 \n",
      "Iteration: 1293. Loss: 1.4180697202682495 \n",
      "Iteration: 1294. Loss: 1.4067552089691162 \n",
      "Iteration: 1295. Loss: 1.435317039489746 \n",
      "Iteration: 1296. Loss: 1.4874883890151978 \n",
      "Iteration: 1297. Loss: 1.4245775938034058 \n",
      "Iteration: 1298. Loss: 1.4848437309265137 \n",
      "Iteration: 1299. Loss: 1.411280870437622 \n",
      "Iteration: 1300. Loss: 1.4529719352722168 \n",
      "Iteration: 1301. Loss: 1.3897035121917725 \n",
      "Iteration: 1302. Loss: 1.3649157285690308 \n",
      "Iteration: 1303. Loss: 1.4729913473129272 \n",
      "Iteration: 1304. Loss: 1.4142588376998901 \n",
      "Iteration: 1305. Loss: 1.4306678771972656 \n",
      "Iteration: 1306. Loss: 1.456180214881897 \n",
      "Iteration: 1307. Loss: 1.4047726392745972 \n",
      "Iteration: 1308. Loss: 1.4353704452514648 \n",
      "Iteration: 1309. Loss: 1.4164713621139526 \n",
      "Iteration: 1310. Loss: 1.4177993535995483 \n",
      "Iteration: 1311. Loss: 1.4101508855819702 \n",
      "Iteration: 1312. Loss: 1.3556103706359863 \n",
      "Iteration: 1313. Loss: 1.4606525897979736 \n",
      "Iteration: 1314. Loss: 1.4055976867675781 \n",
      "Iteration: 1315. Loss: 1.3636833429336548 \n",
      "Iteration: 1316. Loss: 1.4739564657211304 \n",
      "Iteration: 1317. Loss: 1.4740900993347168 \n",
      "Iteration: 1318. Loss: 1.4155001640319824 \n",
      "Iteration: 1319. Loss: 1.3512890338897705 \n",
      "Iteration: 1320. Loss: 1.4226897954940796 \n",
      "Iteration: 1321. Loss: 1.36640202999115 \n",
      "Iteration: 1322. Loss: 1.4549022912979126 \n",
      "Iteration: 1323. Loss: 1.4164057970046997 \n",
      "Iteration: 1324. Loss: 1.4446758031845093 \n",
      "Iteration: 1325. Loss: 1.3456236124038696 \n",
      "Iteration: 1326. Loss: 1.3989043235778809 \n",
      "Iteration: 1327. Loss: 1.3694672584533691 \n",
      "Iteration: 1328. Loss: 1.3626877069473267 \n",
      "Iteration: 1329. Loss: 1.350419044494629 \n",
      "Iteration: 1330. Loss: 1.4638185501098633 \n",
      "Iteration: 1331. Loss: 1.382887601852417 \n",
      "Iteration: 1332. Loss: 1.4209237098693848 \n",
      "Iteration: 1333. Loss: 1.4564154148101807 \n",
      "Iteration: 1334. Loss: 1.4152358770370483 \n",
      "Iteration: 1335. Loss: 1.4059181213378906 \n",
      "Iteration: 1336. Loss: 1.5177311897277832 \n",
      "Iteration: 1337. Loss: 1.3696781396865845 \n",
      "Iteration: 1338. Loss: 1.3942161798477173 \n",
      "Iteration: 1339. Loss: 1.4762792587280273 \n",
      "Iteration: 1340. Loss: 1.4417539834976196 \n",
      "Iteration: 1341. Loss: 1.427487850189209 \n",
      "Iteration: 1342. Loss: 1.4550093412399292 \n",
      "Iteration: 1343. Loss: 1.426937460899353 \n",
      "Iteration: 1344. Loss: 1.3581730127334595 \n",
      "Iteration: 1345. Loss: 1.4390918016433716 \n",
      "Iteration: 1346. Loss: 1.3680243492126465 \n",
      "Iteration: 1347. Loss: 1.3817436695098877 \n",
      "Iteration: 1348. Loss: 1.4091488122940063 \n",
      "Iteration: 1349. Loss: 1.3866604566574097 \n",
      "Iteration: 1350. Loss: 1.372043251991272 \n",
      "Iteration: 1351. Loss: 1.436207890510559 \n",
      "Iteration: 1352. Loss: 1.4631669521331787 \n",
      "Iteration: 1353. Loss: 1.3988248109817505 \n",
      "Iteration: 1354. Loss: 1.4314342737197876 \n",
      "Iteration: 1355. Loss: 1.4401417970657349 \n",
      "Iteration: 1356. Loss: 1.2550891637802124 \n",
      "Iteration: 1357. Loss: 1.279548168182373 \n",
      "Iteration: 1358. Loss: 1.4797852039337158 \n",
      "Iteration: 1359. Loss: 1.3905847072601318 \n",
      "Iteration: 1360. Loss: 1.3865232467651367 \n",
      "Iteration: 1361. Loss: 1.3192589282989502 \n",
      "Iteration: 1362. Loss: 1.3731675148010254 \n",
      "Iteration: 1363. Loss: 1.4324268102645874 \n",
      "Iteration: 1364. Loss: 1.4418652057647705 \n",
      "Iteration: 1365. Loss: 1.3304848670959473 \n",
      "Iteration: 1366. Loss: 1.440016508102417 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1367. Loss: 1.3401989936828613 \n",
      "Iteration: 1368. Loss: 1.4251551628112793 \n",
      "Iteration: 1369. Loss: 1.4651951789855957 \n",
      "Iteration: 1370. Loss: 1.377311110496521 \n",
      "Iteration: 1371. Loss: 1.382981300354004 \n",
      "Iteration: 1372. Loss: 1.4423795938491821 \n",
      "Iteration: 1373. Loss: 1.3887022733688354 \n",
      "Iteration: 1374. Loss: 1.3711949586868286 \n",
      "Iteration: 1375. Loss: 1.361975073814392 \n",
      "Iteration: 1376. Loss: 1.426649808883667 \n",
      "Iteration: 1377. Loss: 1.3731622695922852 \n",
      "Iteration: 1378. Loss: 1.4118481874465942 \n",
      "Iteration: 1379. Loss: 1.4064892530441284 \n",
      "Iteration: 1380. Loss: 1.4367725849151611 \n",
      "Iteration: 1381. Loss: 1.3194184303283691 \n",
      "Iteration: 1382. Loss: 1.4622400999069214 \n",
      "Iteration: 1383. Loss: 1.4290410280227661 \n",
      "Iteration: 1384. Loss: 1.3854436874389648 \n",
      "Iteration: 1385. Loss: 1.3615096807479858 \n",
      "Iteration: 1386. Loss: 1.4226348400115967 \n",
      "Iteration: 1387. Loss: 1.3995441198349 \n",
      "Iteration: 1388. Loss: 1.3755123615264893 \n",
      "Iteration: 1389. Loss: 1.451098918914795 \n",
      "Iteration: 1390. Loss: 1.3347134590148926 \n",
      "Iteration: 1391. Loss: 1.35504949092865 \n",
      "Iteration: 1392. Loss: 1.394964337348938 \n",
      "Iteration: 1393. Loss: 1.4039350748062134 \n",
      "Iteration: 1394. Loss: 1.4474356174468994 \n",
      "Iteration: 1395. Loss: 1.4282668828964233 \n",
      "Iteration: 1396. Loss: 1.3714015483856201 \n",
      "Iteration: 1397. Loss: 1.420937180519104 \n",
      "Iteration: 1398. Loss: 1.447279453277588 \n",
      "Iteration: 1399. Loss: 1.3969403505325317 \n",
      "Iteration: 1400. Loss: 1.409837245941162 \n",
      "Iteration: 1401. Loss: 1.373780369758606 \n",
      "Iteration: 1402. Loss: 1.3330819606781006 \n",
      "Iteration: 1403. Loss: 1.3021987676620483 \n",
      "Iteration: 1404. Loss: 1.386854887008667 \n",
      "Iteration: 1405. Loss: 1.3872196674346924 \n",
      "Iteration: 1406. Loss: 1.303565502166748 \n",
      "Iteration: 1407. Loss: 1.4353904724121094 \n",
      "Iteration: 1408. Loss: 1.3719052076339722 \n",
      "Iteration: 1409. Loss: 1.4298057556152344 \n",
      "Iteration: 1410. Loss: 1.3620506525039673 \n",
      "Iteration: 1411. Loss: 1.4017400741577148 \n",
      "Iteration: 1412. Loss: 1.3014400005340576 \n",
      "Iteration: 1413. Loss: 1.4181427955627441 \n",
      "Iteration: 1414. Loss: 1.3335262537002563 \n",
      "Iteration: 1415. Loss: 1.4740171432495117 \n",
      "Iteration: 1416. Loss: 1.2781481742858887 \n",
      "Iteration: 1417. Loss: 1.3081984519958496 \n",
      "Iteration: 1418. Loss: 1.3715336322784424 \n",
      "Iteration: 1419. Loss: 1.3134254217147827 \n",
      "Iteration: 1420. Loss: 1.426463007926941 \n",
      "Iteration: 1421. Loss: 1.2946316003799438 \n",
      "Iteration: 1422. Loss: 1.3499699831008911 \n",
      "Iteration: 1423. Loss: 1.3680715560913086 \n",
      "Iteration: 1424. Loss: 1.4022122621536255 \n",
      "Iteration: 1425. Loss: 1.266232967376709 \n",
      "Iteration: 1426. Loss: 1.470244288444519 \n",
      "Iteration: 1427. Loss: 1.290733814239502 \n",
      "Iteration: 1428. Loss: 1.4745516777038574 \n",
      "Iteration: 1429. Loss: 1.3387080430984497 \n",
      "Iteration: 1430. Loss: 1.4243611097335815 \n",
      "Iteration: 1431. Loss: 1.3704724311828613 \n",
      "Iteration: 1432. Loss: 1.3190443515777588 \n",
      "Iteration: 1433. Loss: 1.4290653467178345 \n",
      "Iteration: 1434. Loss: 1.3799200057983398 \n",
      "Iteration: 1435. Loss: 1.341119647026062 \n",
      "Iteration: 1436. Loss: 1.374191403388977 \n",
      "Iteration: 1437. Loss: 1.3876479864120483 \n",
      "Iteration: 1438. Loss: 1.3871861696243286 \n",
      "Iteration: 1439. Loss: 1.3648775815963745 \n",
      "Iteration: 1440. Loss: 1.3832882642745972 \n",
      "Iteration: 1441. Loss: 1.3078062534332275 \n",
      "Iteration: 1442. Loss: 1.460141897201538 \n",
      "Iteration: 1443. Loss: 1.388770580291748 \n",
      "Iteration: 1444. Loss: 1.344527006149292 \n",
      "Iteration: 1445. Loss: 1.3465276956558228 \n",
      "Iteration: 1446. Loss: 1.3523615598678589 \n",
      "Iteration: 1447. Loss: 1.4111922979354858 \n",
      "Iteration: 1448. Loss: 1.3867541551589966 \n",
      "Iteration: 1449. Loss: 1.3257603645324707 \n",
      "Iteration: 1450. Loss: 1.3966952562332153 \n",
      "Iteration: 1451. Loss: 1.3189806938171387 \n",
      "Iteration: 1452. Loss: 1.456488847732544 \n",
      "Iteration: 1453. Loss: 1.3498634099960327 \n",
      "Iteration: 1454. Loss: 1.323307991027832 \n",
      "Iteration: 1455. Loss: 1.3853764533996582 \n",
      "Iteration: 1456. Loss: 1.3973774909973145 \n",
      "Iteration: 1457. Loss: 1.3918174505233765 \n",
      "Iteration: 1458. Loss: 1.347591519355774 \n",
      "Iteration: 1459. Loss: 1.2833735942840576 \n",
      "Iteration: 1460. Loss: 1.3669437170028687 \n",
      "Iteration: 1461. Loss: 1.4148354530334473 \n",
      "Iteration: 1462. Loss: 1.2979693412780762 \n",
      "Iteration: 1463. Loss: 1.4115451574325562 \n",
      "Iteration: 1464. Loss: 1.4248242378234863 \n",
      "Iteration: 1465. Loss: 1.337541103363037 \n",
      "Iteration: 1466. Loss: 1.4710144996643066 \n",
      "Iteration: 1467. Loss: 1.421156883239746 \n",
      "Iteration: 1468. Loss: 1.3579025268554688 \n",
      "Iteration: 1469. Loss: 1.3582130670547485 \n",
      "Iteration: 1470. Loss: 1.373646855354309 \n",
      "Iteration: 1471. Loss: 1.412567138671875 \n",
      "Iteration: 1472. Loss: 1.3237602710723877 \n",
      "Iteration: 1473. Loss: 1.414299726486206 \n",
      "Iteration: 1474. Loss: 1.3812124729156494 \n",
      "Iteration: 1475. Loss: 1.3259811401367188 \n",
      "Iteration: 1476. Loss: 1.3362268209457397 \n",
      "Iteration: 1477. Loss: 1.326361894607544 \n",
      "Iteration: 1478. Loss: 1.3481427431106567 \n",
      "Iteration: 1479. Loss: 1.3840000629425049 \n",
      "Iteration: 1480. Loss: 1.316045880317688 \n",
      "Iteration: 1481. Loss: 1.3786919116973877 \n",
      "Iteration: 1482. Loss: 1.4321060180664062 \n",
      "Iteration: 1483. Loss: 1.3996667861938477 \n",
      "Iteration: 1484. Loss: 1.3209710121154785 \n",
      "Iteration: 1485. Loss: 1.393884539604187 \n",
      "Iteration: 1486. Loss: 1.3693175315856934 \n",
      "Iteration: 1487. Loss: 1.3782713413238525 \n",
      "Iteration: 1488. Loss: 1.3305768966674805 \n",
      "Iteration: 1489. Loss: 1.342563271522522 \n",
      "Iteration: 1490. Loss: 1.3793352842330933 \n",
      "Iteration: 1491. Loss: 1.3559809923171997 \n",
      "Iteration: 1492. Loss: 1.3515468835830688 \n",
      "Iteration: 1493. Loss: 1.3875764608383179 \n",
      "Iteration: 1494. Loss: 1.3459385633468628 \n",
      "Iteration: 1495. Loss: 1.2228854894638062 \n",
      "Iteration: 1496. Loss: 1.3412481546401978 \n",
      "Iteration: 1497. Loss: 1.3802070617675781 \n",
      "Iteration: 1498. Loss: 1.448083519935608 \n",
      "Iteration: 1499. Loss: 1.3392657041549683 \n",
      "Iteration: 1500. Loss: 1.384063720703125 \n",
      "Iteration: 1501. Loss: 1.3502386808395386 \n",
      "Iteration: 1502. Loss: 1.3415641784667969 \n",
      "Iteration: 1503. Loss: 1.2199561595916748 \n",
      "Iteration: 1504. Loss: 1.294209599494934 \n",
      "Iteration: 1505. Loss: 1.3205064535140991 \n",
      "Iteration: 1506. Loss: 1.2666661739349365 \n",
      "Iteration: 1507. Loss: 1.3144019842147827 \n",
      "Iteration: 1508. Loss: 1.2608389854431152 \n",
      "Iteration: 1509. Loss: 1.4107880592346191 \n",
      "Iteration: 1510. Loss: 1.3014042377471924 \n",
      "Iteration: 1511. Loss: 1.2645901441574097 \n",
      "Iteration: 1512. Loss: 1.4168177843093872 \n",
      "Iteration: 1513. Loss: 1.4278160333633423 \n",
      "Iteration: 1514. Loss: 1.3586366176605225 \n",
      "Iteration: 1515. Loss: 1.4010686874389648 \n",
      "Iteration: 1516. Loss: 1.274114966392517 \n",
      "Iteration: 1517. Loss: 1.3515619039535522 \n",
      "Iteration: 1518. Loss: 1.259482979774475 \n",
      "Iteration: 1519. Loss: 1.3721036911010742 \n",
      "Iteration: 1520. Loss: 1.3477166891098022 \n",
      "Iteration: 1521. Loss: 1.3827004432678223 \n",
      "Iteration: 1522. Loss: 1.3668005466461182 \n",
      "Iteration: 1523. Loss: 1.3451532125473022 \n",
      "Iteration: 1524. Loss: 1.3601187467575073 \n",
      "Iteration: 1525. Loss: 1.3654108047485352 \n",
      "Iteration: 1526. Loss: 1.316408634185791 \n",
      "Iteration: 1527. Loss: 1.3560634851455688 \n",
      "Iteration: 1528. Loss: 1.3096716403961182 \n",
      "Iteration: 1529. Loss: 1.321882724761963 \n",
      "Iteration: 1530. Loss: 1.2827495336532593 \n",
      "Iteration: 1531. Loss: 1.3569928407669067 \n",
      "Iteration: 1532. Loss: 1.41554856300354 \n",
      "Iteration: 1533. Loss: 1.344738483428955 \n",
      "Iteration: 1534. Loss: 1.3463648557662964 \n",
      "Iteration: 1535. Loss: 1.3280149698257446 \n",
      "Iteration: 1536. Loss: 1.3314379453659058 \n",
      "Iteration: 1537. Loss: 1.3200416564941406 \n",
      "Iteration: 1538. Loss: 1.381839394569397 \n",
      "Iteration: 1539. Loss: 1.3659846782684326 \n",
      "Iteration: 1540. Loss: 1.2968864440917969 \n",
      "Iteration: 1541. Loss: 1.2656248807907104 \n",
      "Iteration: 1542. Loss: 1.3416240215301514 \n",
      "Iteration: 1543. Loss: 1.3069151639938354 \n",
      "Iteration: 1544. Loss: 1.3517321348190308 \n",
      "Iteration: 1545. Loss: 1.2892662286758423 \n",
      "Iteration: 1546. Loss: 1.2945258617401123 \n",
      "Iteration: 1547. Loss: 1.274435043334961 \n",
      "Iteration: 1548. Loss: 1.3126966953277588 \n",
      "Iteration: 1549. Loss: 1.3719940185546875 \n",
      "Iteration: 1550. Loss: 1.3147529363632202 \n",
      "Iteration: 1551. Loss: 1.2986974716186523 \n",
      "Iteration: 1552. Loss: 1.3556245565414429 \n",
      "Iteration: 1553. Loss: 1.305391550064087 \n",
      "Iteration: 1554. Loss: 1.366676688194275 \n",
      "Iteration: 1555. Loss: 1.3382594585418701 \n",
      "Iteration: 1556. Loss: 1.2604557275772095 \n",
      "Iteration: 1557. Loss: 1.3275909423828125 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1558. Loss: 1.324836015701294 \n",
      "Iteration: 1559. Loss: 1.2727514505386353 \n",
      "Iteration: 1560. Loss: 1.2767077684402466 \n",
      "Iteration: 1561. Loss: 1.4010391235351562 \n",
      "Iteration: 1562. Loss: 1.364791989326477 \n",
      "Iteration: 1563. Loss: 1.368595838546753 \n",
      "Iteration: 1564. Loss: 1.3010191917419434 \n",
      "Iteration: 1565. Loss: 1.3338720798492432 \n",
      "Iteration: 1566. Loss: 1.3367886543273926 \n",
      "Iteration: 1567. Loss: 1.30855393409729 \n",
      "Iteration: 1568. Loss: 1.2978260517120361 \n",
      "Iteration: 1569. Loss: 1.2701377868652344 \n",
      "Iteration: 1570. Loss: 1.3605152368545532 \n",
      "Iteration: 1571. Loss: 1.241385579109192 \n",
      "Iteration: 1572. Loss: 1.3674225807189941 \n",
      "Iteration: 1573. Loss: 1.3065265417099 \n",
      "Iteration: 1574. Loss: 1.329389214515686 \n",
      "Iteration: 1575. Loss: 1.2835055589675903 \n",
      "Iteration: 1576. Loss: 1.2785749435424805 \n",
      "Iteration: 1577. Loss: 1.3439421653747559 \n",
      "Iteration: 1578. Loss: 1.3558653593063354 \n",
      "Iteration: 1579. Loss: 1.3360633850097656 \n",
      "Iteration: 1580. Loss: 1.3509331941604614 \n",
      "Iteration: 1581. Loss: 1.2790199518203735 \n",
      "Iteration: 1582. Loss: 1.3073136806488037 \n",
      "Iteration: 1583. Loss: 1.3375649452209473 \n",
      "Iteration: 1584. Loss: 1.3274613618850708 \n",
      "Iteration: 1585. Loss: 1.3798747062683105 \n",
      "Iteration: 1586. Loss: 1.323988676071167 \n",
      "Iteration: 1587. Loss: 1.2502408027648926 \n",
      "Iteration: 1588. Loss: 1.2837741374969482 \n",
      "Iteration: 1589. Loss: 1.3863584995269775 \n",
      "Iteration: 1590. Loss: 1.3761465549468994 \n",
      "Iteration: 1591. Loss: 1.3795464038848877 \n",
      "Iteration: 1592. Loss: 1.2616548538208008 \n",
      "Iteration: 1593. Loss: 1.3584309816360474 \n",
      "Iteration: 1594. Loss: 1.3696037530899048 \n",
      "Iteration: 1595. Loss: 1.4540196657180786 \n",
      "Iteration: 1596. Loss: 1.3540531396865845 \n",
      "Iteration: 1597. Loss: 1.3086557388305664 \n",
      "Iteration: 1598. Loss: 1.2351497411727905 \n",
      "Iteration: 1599. Loss: 1.3024096488952637 \n",
      "Iteration: 1600. Loss: 1.4000705480575562 \n",
      "Iteration: 1601. Loss: 1.3109420537948608 \n",
      "Iteration: 1602. Loss: 1.342450737953186 \n",
      "Iteration: 1603. Loss: 1.305872917175293 \n",
      "Iteration: 1604. Loss: 1.3698186874389648 \n",
      "Iteration: 1605. Loss: 1.308759331703186 \n",
      "Iteration: 1606. Loss: 1.2523953914642334 \n",
      "Iteration: 1607. Loss: 1.2384835481643677 \n",
      "Iteration: 1608. Loss: 1.3614085912704468 \n",
      "Iteration: 1609. Loss: 1.3430671691894531 \n",
      "Iteration: 1610. Loss: 1.2666597366333008 \n",
      "Iteration: 1611. Loss: 1.3804794549942017 \n",
      "Iteration: 1612. Loss: 1.4072726964950562 \n",
      "Iteration: 1613. Loss: 1.2604789733886719 \n",
      "Iteration: 1614. Loss: 1.288591742515564 \n",
      "Iteration: 1615. Loss: 1.369160771369934 \n",
      "Iteration: 1616. Loss: 1.318292260169983 \n",
      "Iteration: 1617. Loss: 1.277147889137268 \n",
      "Iteration: 1618. Loss: 1.3024191856384277 \n",
      "Iteration: 1619. Loss: 1.2878228425979614 \n",
      "Iteration: 1620. Loss: 1.28398859500885 \n",
      "Iteration: 1621. Loss: 1.2561548948287964 \n",
      "Iteration: 1622. Loss: 1.3479630947113037 \n",
      "Iteration: 1623. Loss: 1.2970753908157349 \n",
      "Iteration: 1624. Loss: 1.2853102684020996 \n",
      "Iteration: 1625. Loss: 1.2957113981246948 \n",
      "Iteration: 1626. Loss: 1.4143805503845215 \n",
      "Iteration: 1627. Loss: 1.353458046913147 \n",
      "Iteration: 1628. Loss: 1.2692348957061768 \n",
      "Iteration: 1629. Loss: 1.2265734672546387 \n",
      "Iteration: 1630. Loss: 1.2847598791122437 \n",
      "Iteration: 1631. Loss: 1.3961073160171509 \n",
      "Iteration: 1632. Loss: 1.2741923332214355 \n",
      "Iteration: 1633. Loss: 1.3222441673278809 \n",
      "Iteration: 1634. Loss: 1.2452207803726196 \n",
      "Iteration: 1635. Loss: 1.2737635374069214 \n",
      "Iteration: 1636. Loss: 1.305126667022705 \n",
      "Iteration: 1637. Loss: 1.3465509414672852 \n",
      "Iteration: 1638. Loss: 1.3430981636047363 \n",
      "Iteration: 1639. Loss: 1.2685296535491943 \n",
      "Iteration: 1640. Loss: 1.2430022954940796 \n",
      "Iteration: 1641. Loss: 1.3613097667694092 \n",
      "Iteration: 1642. Loss: 1.2353752851486206 \n",
      "Iteration: 1643. Loss: 1.2821561098098755 \n",
      "Iteration: 1644. Loss: 1.3197687864303589 \n",
      "Iteration: 1645. Loss: 1.3038760423660278 \n",
      "Iteration: 1646. Loss: 1.293125033378601 \n",
      "Iteration: 1647. Loss: 1.2595276832580566 \n",
      "Iteration: 1648. Loss: 1.3312433958053589 \n",
      "Iteration: 1649. Loss: 1.3139175176620483 \n",
      "Iteration: 1650. Loss: 1.324295163154602 \n",
      "Iteration: 1651. Loss: 1.331815481185913 \n",
      "Iteration: 1652. Loss: 1.202763319015503 \n",
      "Iteration: 1653. Loss: 1.2144156694412231 \n",
      "Iteration: 1654. Loss: 1.3752015829086304 \n",
      "Iteration: 1655. Loss: 1.2572014331817627 \n",
      "Iteration: 1656. Loss: 1.2280330657958984 \n",
      "Iteration: 1657. Loss: 1.3983185291290283 \n",
      "Iteration: 1658. Loss: 1.2449195384979248 \n",
      "Iteration: 1659. Loss: 1.2856844663619995 \n",
      "Iteration: 1660. Loss: 1.2875831127166748 \n",
      "Iteration: 1661. Loss: 1.1648975610733032 \n",
      "Iteration: 1662. Loss: 1.3165106773376465 \n",
      "Iteration: 1663. Loss: 1.2895004749298096 \n",
      "Iteration: 1664. Loss: 1.239183783531189 \n",
      "Iteration: 1665. Loss: 1.2442806959152222 \n",
      "Iteration: 1666. Loss: 1.2015864849090576 \n",
      "Iteration: 1667. Loss: 1.286118507385254 \n",
      "Iteration: 1668. Loss: 1.2578579187393188 \n",
      "Iteration: 1669. Loss: 1.3030699491500854 \n",
      "Iteration: 1670. Loss: 1.3068413734436035 \n",
      "Iteration: 1671. Loss: 1.3147461414337158 \n",
      "Iteration: 1672. Loss: 1.3636064529418945 \n",
      "Iteration: 1673. Loss: 1.3600831031799316 \n",
      "Iteration: 1674. Loss: 1.3377245664596558 \n",
      "Iteration: 1675. Loss: 1.3277380466461182 \n",
      "Iteration: 1676. Loss: 1.2520933151245117 \n",
      "Iteration: 1677. Loss: 1.249335765838623 \n",
      "Iteration: 1678. Loss: 1.3932157754898071 \n",
      "Iteration: 1679. Loss: 1.2683674097061157 \n",
      "Iteration: 1680. Loss: 1.258215308189392 \n",
      "Iteration: 1681. Loss: 1.3302263021469116 \n",
      "Iteration: 1682. Loss: 1.3404823541641235 \n",
      "Iteration: 1683. Loss: 1.347409725189209 \n",
      "Iteration: 1684. Loss: 1.2504867315292358 \n",
      "Iteration: 1685. Loss: 1.281874656677246 \n",
      "Iteration: 1686. Loss: 1.3254776000976562 \n",
      "Iteration: 1687. Loss: 1.2188029289245605 \n",
      "Iteration: 1688. Loss: 1.239309549331665 \n",
      "Iteration: 1689. Loss: 1.3143000602722168 \n",
      "Iteration: 1690. Loss: 1.3028862476348877 \n",
      "Iteration: 1691. Loss: 1.3149505853652954 \n",
      "Iteration: 1692. Loss: 1.2777588367462158 \n",
      "Iteration: 1693. Loss: 1.2326254844665527 \n",
      "Iteration: 1694. Loss: 1.3224101066589355 \n",
      "Iteration: 1695. Loss: 1.232010006904602 \n",
      "Iteration: 1696. Loss: 1.2970541715621948 \n",
      "Iteration: 1697. Loss: 1.2307333946228027 \n",
      "Iteration: 1698. Loss: 1.2155635356903076 \n",
      "Iteration: 1699. Loss: 1.2285172939300537 \n",
      "Iteration: 1700. Loss: 1.2718403339385986 \n",
      "Iteration: 1701. Loss: 1.2057679891586304 \n",
      "Iteration: 1702. Loss: 1.3025327920913696 \n",
      "Iteration: 1703. Loss: 1.2438050508499146 \n",
      "Iteration: 1704. Loss: 1.2778902053833008 \n",
      "Iteration: 1705. Loss: 1.3602060079574585 \n",
      "Iteration: 1706. Loss: 1.3151419162750244 \n",
      "Iteration: 1707. Loss: 1.410762906074524 \n",
      "Iteration: 1708. Loss: 1.229324221611023 \n",
      "Iteration: 1709. Loss: 1.2800164222717285 \n",
      "Iteration: 1710. Loss: 1.2921476364135742 \n",
      "Iteration: 1711. Loss: 1.2849535942077637 \n",
      "Iteration: 1712. Loss: 1.2997654676437378 \n",
      "Iteration: 1713. Loss: 1.321423053741455 \n",
      "Iteration: 1714. Loss: 1.2804640531539917 \n",
      "Iteration: 1715. Loss: 1.2206737995147705 \n",
      "Iteration: 1716. Loss: 1.3194019794464111 \n",
      "Iteration: 1717. Loss: 1.200209617614746 \n",
      "Iteration: 1718. Loss: 1.2839852571487427 \n",
      "Iteration: 1719. Loss: 1.2854244709014893 \n",
      "Iteration: 1720. Loss: 1.2074049711227417 \n",
      "Iteration: 1721. Loss: 1.2885217666625977 \n",
      "Iteration: 1722. Loss: 1.3068026304244995 \n",
      "Iteration: 1723. Loss: 1.2392067909240723 \n",
      "Iteration: 1724. Loss: 1.282906174659729 \n",
      "Iteration: 1725. Loss: 1.2351256608963013 \n",
      "Iteration: 1726. Loss: 1.2537412643432617 \n",
      "Iteration: 1727. Loss: 1.2465972900390625 \n",
      "Iteration: 1728. Loss: 1.1999671459197998 \n",
      "Iteration: 1729. Loss: 1.2822325229644775 \n",
      "Iteration: 1730. Loss: 1.2522082328796387 \n",
      "Iteration: 1731. Loss: 1.276026725769043 \n",
      "Iteration: 1732. Loss: 1.2552841901779175 \n",
      "Iteration: 1733. Loss: 1.3199453353881836 \n",
      "Iteration: 1734. Loss: 1.2186002731323242 \n",
      "Iteration: 1735. Loss: 1.3156960010528564 \n",
      "Iteration: 1736. Loss: 1.190500020980835 \n",
      "Iteration: 1737. Loss: 1.2208943367004395 \n",
      "Iteration: 1738. Loss: 1.2282474040985107 \n",
      "Iteration: 1739. Loss: 1.267643690109253 \n",
      "Iteration: 1740. Loss: 1.2169694900512695 \n",
      "Iteration: 1741. Loss: 1.2099506855010986 \n",
      "Iteration: 1742. Loss: 1.1693682670593262 \n",
      "Iteration: 1743. Loss: 1.280479073524475 \n",
      "Iteration: 1744. Loss: 1.301122784614563 \n",
      "Iteration: 1745. Loss: 1.2589588165283203 \n",
      "Iteration: 1746. Loss: 1.245619297027588 \n",
      "Iteration: 1747. Loss: 1.2324331998825073 \n",
      "Iteration: 1748. Loss: 1.2656630277633667 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1749. Loss: 1.3164271116256714 \n",
      "Iteration: 1750. Loss: 1.290082573890686 \n",
      "Iteration: 1751. Loss: 1.3052189350128174 \n",
      "Iteration: 1752. Loss: 1.2326164245605469 \n",
      "Iteration: 1753. Loss: 1.192158579826355 \n",
      "Iteration: 1754. Loss: 1.247243881225586 \n",
      "Iteration: 1755. Loss: 1.314354658126831 \n",
      "Iteration: 1756. Loss: 1.2034063339233398 \n",
      "Iteration: 1757. Loss: 1.2530200481414795 \n",
      "Iteration: 1758. Loss: 1.1934260129928589 \n",
      "Iteration: 1759. Loss: 1.3445465564727783 \n",
      "Iteration: 1760. Loss: 1.1907590627670288 \n",
      "Iteration: 1761. Loss: 1.205503225326538 \n",
      "Iteration: 1762. Loss: 1.1970562934875488 \n",
      "Iteration: 1763. Loss: 1.2777321338653564 \n",
      "Iteration: 1764. Loss: 1.3988308906555176 \n",
      "Iteration: 1765. Loss: 1.1664766073226929 \n",
      "Iteration: 1766. Loss: 1.297231912612915 \n",
      "Iteration: 1767. Loss: 1.2635005712509155 \n",
      "Iteration: 1768. Loss: 1.2332340478897095 \n",
      "Iteration: 1769. Loss: 1.278333067893982 \n",
      "Iteration: 1770. Loss: 1.2223544120788574 \n",
      "Iteration: 1771. Loss: 1.2938833236694336 \n",
      "Iteration: 1772. Loss: 1.217500925064087 \n",
      "Iteration: 1773. Loss: 1.2114492654800415 \n",
      "Iteration: 1774. Loss: 1.2471425533294678 \n",
      "Iteration: 1775. Loss: 1.3213202953338623 \n",
      "Iteration: 1776. Loss: 1.1835628747940063 \n",
      "Iteration: 1777. Loss: 1.2809667587280273 \n",
      "Iteration: 1778. Loss: 1.256999135017395 \n",
      "Iteration: 1779. Loss: 1.2089247703552246 \n",
      "Iteration: 1780. Loss: 1.3198143243789673 \n",
      "Iteration: 1781. Loss: 1.3425400257110596 \n",
      "Iteration: 1782. Loss: 1.225610375404358 \n",
      "Iteration: 1783. Loss: 1.2253292798995972 \n",
      "Iteration: 1784. Loss: 1.2940542697906494 \n",
      "Iteration: 1785. Loss: 1.2412328720092773 \n",
      "Iteration: 1786. Loss: 1.2617154121398926 \n",
      "Iteration: 1787. Loss: 1.252748966217041 \n",
      "Iteration: 1788. Loss: 1.2561752796173096 \n",
      "Iteration: 1789. Loss: 1.2370631694793701 \n",
      "Iteration: 1790. Loss: 1.193353533744812 \n",
      "Iteration: 1791. Loss: 1.2434420585632324 \n",
      "Iteration: 1792. Loss: 1.2486110925674438 \n",
      "Iteration: 1793. Loss: 1.1554865837097168 \n",
      "Iteration: 1794. Loss: 1.192918062210083 \n",
      "Iteration: 1795. Loss: 1.2087585926055908 \n",
      "Iteration: 1796. Loss: 1.2922108173370361 \n",
      "Iteration: 1797. Loss: 1.208224892616272 \n",
      "Iteration: 1798. Loss: 1.2612074613571167 \n",
      "Iteration: 1799. Loss: 1.310653805732727 \n",
      "Iteration: 1800. Loss: 1.1898431777954102 \n",
      "Iteration: 1801. Loss: 1.1865648031234741 \n",
      "Iteration: 1802. Loss: 1.1467678546905518 \n",
      "Iteration: 1803. Loss: 1.2562536001205444 \n",
      "Iteration: 1804. Loss: 1.2591965198516846 \n",
      "Iteration: 1805. Loss: 1.2040987014770508 \n",
      "Iteration: 1806. Loss: 1.1933598518371582 \n",
      "Iteration: 1807. Loss: 1.3559902906417847 \n",
      "Iteration: 1808. Loss: 1.1616883277893066 \n",
      "Iteration: 1809. Loss: 1.1610788106918335 \n",
      "Iteration: 1810. Loss: 1.2758017778396606 \n",
      "Iteration: 1811. Loss: 1.224256157875061 \n",
      "Iteration: 1812. Loss: 1.226040005683899 \n",
      "Iteration: 1813. Loss: 1.2526606321334839 \n",
      "Iteration: 1814. Loss: 1.2792836427688599 \n",
      "Iteration: 1815. Loss: 1.2890642881393433 \n",
      "Iteration: 1816. Loss: 1.3212790489196777 \n",
      "Iteration: 1817. Loss: 1.2282588481903076 \n",
      "Iteration: 1818. Loss: 1.284122109413147 \n",
      "Iteration: 1819. Loss: 1.3060026168823242 \n",
      "Iteration: 1820. Loss: 1.1804850101470947 \n",
      "Iteration: 1821. Loss: 1.2730635404586792 \n",
      "Iteration: 1822. Loss: 1.2213431596755981 \n",
      "Iteration: 1823. Loss: 1.2717688083648682 \n",
      "Iteration: 1824. Loss: 1.1575164794921875 \n",
      "Iteration: 1825. Loss: 1.3569221496582031 \n",
      "Iteration: 1826. Loss: 1.2103952169418335 \n",
      "Iteration: 1827. Loss: 1.2043547630310059 \n",
      "Iteration: 1828. Loss: 1.1529834270477295 \n",
      "Iteration: 1829. Loss: 1.2795425653457642 \n",
      "Iteration: 1830. Loss: 1.2105482816696167 \n",
      "Iteration: 1831. Loss: 1.2411141395568848 \n",
      "Iteration: 1832. Loss: 1.1560395956039429 \n",
      "Iteration: 1833. Loss: 1.2198185920715332 \n",
      "Iteration: 1834. Loss: 1.1546480655670166 \n",
      "Iteration: 1835. Loss: 1.3252180814743042 \n",
      "Iteration: 1836. Loss: 1.2435011863708496 \n",
      "Iteration: 1837. Loss: 1.2459344863891602 \n",
      "Iteration: 1838. Loss: 1.305680274963379 \n",
      "Iteration: 1839. Loss: 1.2568128108978271 \n",
      "Iteration: 1840. Loss: 1.2824296951293945 \n",
      "Iteration: 1841. Loss: 1.2131272554397583 \n",
      "Iteration: 1842. Loss: 1.2236560583114624 \n",
      "Iteration: 1843. Loss: 1.270412564277649 \n",
      "Iteration: 1844. Loss: 1.2987018823623657 \n",
      "Iteration: 1845. Loss: 1.1998997926712036 \n",
      "Iteration: 1846. Loss: 1.2356281280517578 \n",
      "Iteration: 1847. Loss: 1.2696218490600586 \n",
      "Iteration: 1848. Loss: 1.2161365747451782 \n",
      "Iteration: 1849. Loss: 1.311293363571167 \n",
      "Iteration: 1850. Loss: 1.208465814590454 \n",
      "Iteration: 1851. Loss: 1.2819833755493164 \n",
      "Iteration: 1852. Loss: 1.2672150135040283 \n",
      "Iteration: 1853. Loss: 1.311245322227478 \n",
      "Iteration: 1854. Loss: 1.1994144916534424 \n",
      "Iteration: 1855. Loss: 1.2376518249511719 \n",
      "Iteration: 1856. Loss: 1.2143875360488892 \n",
      "Iteration: 1857. Loss: 1.1584488153457642 \n",
      "Iteration: 1858. Loss: 1.2235716581344604 \n",
      "Iteration: 1859. Loss: 1.344927191734314 \n",
      "Iteration: 1860. Loss: 1.1827067136764526 \n",
      "Iteration: 1861. Loss: 1.2415803670883179 \n",
      "Iteration: 1862. Loss: 1.2329676151275635 \n",
      "Iteration: 1863. Loss: 1.2475112676620483 \n",
      "Iteration: 1864. Loss: 1.2370870113372803 \n",
      "Iteration: 1865. Loss: 1.2476224899291992 \n",
      "Iteration: 1866. Loss: 1.242464542388916 \n",
      "Iteration: 1867. Loss: 1.2415037155151367 \n",
      "Iteration: 1868. Loss: 1.2343696355819702 \n",
      "Iteration: 1869. Loss: 1.1926352977752686 \n",
      "Iteration: 1870. Loss: 1.2375104427337646 \n",
      "Iteration: 1871. Loss: 1.268803596496582 \n",
      "Iteration: 1872. Loss: 1.2685956954956055 \n",
      "Iteration: 1873. Loss: 1.2025409936904907 \n",
      "Iteration: 1874. Loss: 1.2394663095474243 \n",
      "Iteration: 1875. Loss: 1.2740678787231445 \n",
      "Iteration: 1876. Loss: 1.2743785381317139 \n",
      "Iteration: 1877. Loss: 1.1942535638809204 \n",
      "Iteration: 1878. Loss: 1.2592248916625977 \n",
      "Iteration: 1879. Loss: 1.1363611221313477 \n",
      "Iteration: 1880. Loss: 1.2527334690093994 \n",
      "Iteration: 1881. Loss: 1.224570870399475 \n",
      "Iteration: 1882. Loss: 1.2003406286239624 \n",
      "Iteration: 1883. Loss: 1.2885677814483643 \n",
      "Iteration: 1884. Loss: 1.3692511320114136 \n",
      "Iteration: 1885. Loss: 1.228040099143982 \n",
      "Iteration: 1886. Loss: 1.2405890226364136 \n",
      "Iteration: 1887. Loss: 1.2624911069869995 \n",
      "Iteration: 1888. Loss: 1.2672762870788574 \n",
      "Iteration: 1889. Loss: 1.2088618278503418 \n",
      "Iteration: 1890. Loss: 1.1916389465332031 \n",
      "Iteration: 1891. Loss: 1.2033802270889282 \n",
      "Iteration: 1892. Loss: 1.2600830793380737 \n",
      "Iteration: 1893. Loss: 1.3713642358779907 \n",
      "Iteration: 1894. Loss: 1.330116868019104 \n",
      "Iteration: 1895. Loss: 1.141664743423462 \n",
      "Iteration: 1896. Loss: 1.1545852422714233 \n",
      "Iteration: 1897. Loss: 1.26021409034729 \n",
      "Iteration: 1898. Loss: 1.1540716886520386 \n",
      "Iteration: 1899. Loss: 1.3155819177627563 \n",
      "Iteration: 1900. Loss: 1.1064252853393555 \n",
      "Iteration: 1901. Loss: 1.2125880718231201 \n",
      "Iteration: 1902. Loss: 1.223371148109436 \n",
      "Iteration: 1903. Loss: 1.259751796722412 \n",
      "Iteration: 1904. Loss: 1.2413908243179321 \n",
      "Iteration: 1905. Loss: 1.1171042919158936 \n",
      "Iteration: 1906. Loss: 1.1973391771316528 \n",
      "Iteration: 1907. Loss: 1.1357226371765137 \n",
      "Iteration: 1908. Loss: 1.316909670829773 \n",
      "Iteration: 1909. Loss: 1.2234071493148804 \n",
      "Iteration: 1910. Loss: 1.222447395324707 \n",
      "Iteration: 1911. Loss: 1.2771979570388794 \n",
      "Iteration: 1912. Loss: 1.1597379446029663 \n",
      "Iteration: 1913. Loss: 1.2232846021652222 \n",
      "Iteration: 1914. Loss: 1.1654247045516968 \n",
      "Iteration: 1915. Loss: 1.2558401823043823 \n",
      "Iteration: 1916. Loss: 1.231753945350647 \n",
      "Iteration: 1917. Loss: 1.229981780052185 \n",
      "Iteration: 1918. Loss: 1.2515758275985718 \n",
      "Iteration: 1919. Loss: 1.2260338068008423 \n",
      "Iteration: 1920. Loss: 1.1456879377365112 \n",
      "Iteration: 1921. Loss: 1.1496626138687134 \n",
      "Iteration: 1922. Loss: 1.296289324760437 \n",
      "Iteration: 1923. Loss: 1.1484439373016357 \n",
      "Iteration: 1924. Loss: 1.1358991861343384 \n",
      "Iteration: 1925. Loss: 1.2245585918426514 \n",
      "Iteration: 1926. Loss: 1.2122831344604492 \n",
      "Iteration: 1927. Loss: 1.19920814037323 \n",
      "Iteration: 1928. Loss: 1.2568169832229614 \n",
      "Iteration: 1929. Loss: 1.2632166147232056 \n",
      "Iteration: 1930. Loss: 1.2107270956039429 \n",
      "Iteration: 1931. Loss: 1.233149528503418 \n",
      "Iteration: 1932. Loss: 1.1581493616104126 \n",
      "Iteration: 1933. Loss: 1.265590786933899 \n",
      "Iteration: 1934. Loss: 1.2435789108276367 \n",
      "Iteration: 1935. Loss: 1.279274344444275 \n",
      "Iteration: 1936. Loss: 1.2951985597610474 \n",
      "Iteration: 1937. Loss: 1.1869237422943115 \n",
      "Iteration: 1938. Loss: 1.1967926025390625 \n",
      "Iteration: 1939. Loss: 1.196548581123352 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1940. Loss: 1.276241660118103 \n",
      "Iteration: 1941. Loss: 1.1807582378387451 \n",
      "Iteration: 1942. Loss: 1.1446616649627686 \n",
      "Iteration: 1943. Loss: 1.1423885822296143 \n",
      "Iteration: 1944. Loss: 1.2849851846694946 \n",
      "Iteration: 1945. Loss: 1.2190426588058472 \n",
      "Iteration: 1946. Loss: 1.2435325384140015 \n",
      "Iteration: 1947. Loss: 1.2119314670562744 \n",
      "Iteration: 1948. Loss: 1.2118563652038574 \n",
      "Iteration: 1949. Loss: 1.2122935056686401 \n",
      "Iteration: 1950. Loss: 1.381056547164917 \n",
      "Iteration: 1951. Loss: 1.2392823696136475 \n",
      "Iteration: 1952. Loss: 1.237289547920227 \n",
      "Iteration: 1953. Loss: 1.287832498550415 \n",
      "Iteration: 1954. Loss: 1.2613714933395386 \n",
      "Iteration: 1955. Loss: 1.2458324432373047 \n",
      "Iteration: 1956. Loss: 1.1430259943008423 \n",
      "Iteration: 1957. Loss: 1.2354379892349243 \n",
      "Iteration: 1958. Loss: 1.231711745262146 \n",
      "Iteration: 1959. Loss: 1.146255612373352 \n",
      "Iteration: 1960. Loss: 1.2042787075042725 \n",
      "Iteration: 1961. Loss: 1.233798861503601 \n",
      "Iteration: 1962. Loss: 1.1421564817428589 \n",
      "Iteration: 1963. Loss: 1.1974352598190308 \n",
      "Iteration: 1964. Loss: 1.2704157829284668 \n",
      "Iteration: 1965. Loss: 1.2451893091201782 \n",
      "Iteration: 1966. Loss: 1.2112499475479126 \n",
      "Iteration: 1967. Loss: 1.1355090141296387 \n",
      "Iteration: 1968. Loss: 1.1876277923583984 \n",
      "Iteration: 1969. Loss: 1.1212939023971558 \n",
      "Iteration: 1970. Loss: 1.2451072931289673 \n",
      "Iteration: 1971. Loss: 1.174351453781128 \n",
      "Iteration: 1972. Loss: 1.2131266593933105 \n",
      "Iteration: 1973. Loss: 1.1131328344345093 \n",
      "Iteration: 1974. Loss: 1.2576698064804077 \n",
      "Iteration: 1975. Loss: 1.1630197763442993 \n",
      "Iteration: 1976. Loss: 1.2348015308380127 \n",
      "Iteration: 1977. Loss: 1.2375211715698242 \n",
      "Iteration: 1978. Loss: 1.251907467842102 \n",
      "Iteration: 1979. Loss: 1.1533762216567993 \n",
      "Iteration: 1980. Loss: 1.219925045967102 \n",
      "Iteration: 1981. Loss: 1.2233424186706543 \n",
      "Iteration: 1982. Loss: 1.263439416885376 \n",
      "Iteration: 1983. Loss: 1.227304220199585 \n",
      "Iteration: 1984. Loss: 1.2520554065704346 \n",
      "Iteration: 1985. Loss: 1.2051093578338623 \n",
      "Iteration: 1986. Loss: 1.1753101348876953 \n",
      "Iteration: 1987. Loss: 1.244072437286377 \n",
      "Iteration: 1988. Loss: 1.1109458208084106 \n",
      "Iteration: 1989. Loss: 1.217737078666687 \n",
      "Iteration: 1990. Loss: 1.1293854713439941 \n",
      "Iteration: 1991. Loss: 1.1353298425674438 \n",
      "Iteration: 1992. Loss: 1.1917154788970947 \n",
      "Iteration: 1993. Loss: 1.26686692237854 \n",
      "Iteration: 1994. Loss: 1.2254524230957031 \n",
      "Iteration: 1995. Loss: 1.1877753734588623 \n",
      "Iteration: 1996. Loss: 1.1953843832015991 \n",
      "Iteration: 1997. Loss: 1.120630145072937 \n",
      "Iteration: 1998. Loss: 1.2122950553894043 \n",
      "Iteration: 1999. Loss: 1.1793166399002075 \n",
      "Iteration: 2000. Loss: 1.237713098526001 \n",
      "Iteration: 2001. Loss: 1.257680892944336 \n",
      "Iteration: 2002. Loss: 1.167380928993225 \n",
      "Iteration: 2003. Loss: 1.1329549551010132 \n",
      "Iteration: 2004. Loss: 1.1670002937316895 \n",
      "Iteration: 2005. Loss: 1.2212213277816772 \n",
      "Iteration: 2006. Loss: 1.1597577333450317 \n",
      "Iteration: 2007. Loss: 1.147189736366272 \n",
      "Iteration: 2008. Loss: 1.191583514213562 \n",
      "Iteration: 2009. Loss: 1.1955331563949585 \n",
      "Iteration: 2010. Loss: 1.3201664686203003 \n",
      "Iteration: 2011. Loss: 1.111711025238037 \n",
      "Iteration: 2012. Loss: 1.172682285308838 \n",
      "Iteration: 2013. Loss: 1.1957616806030273 \n",
      "Iteration: 2014. Loss: 1.2857944965362549 \n",
      "Iteration: 2015. Loss: 1.0860475301742554 \n",
      "Iteration: 2016. Loss: 1.1544820070266724 \n",
      "Iteration: 2017. Loss: 1.2304054498672485 \n",
      "Iteration: 2018. Loss: 1.2711564302444458 \n",
      "Iteration: 2019. Loss: 1.0818169116973877 \n",
      "Iteration: 2020. Loss: 1.2051470279693604 \n",
      "Iteration: 2021. Loss: 1.1897822618484497 \n",
      "Iteration: 2022. Loss: 1.2232651710510254 \n",
      "Iteration: 2023. Loss: 1.2598810195922852 \n",
      "Iteration: 2024. Loss: 1.092668890953064 \n",
      "Iteration: 2025. Loss: 1.1792068481445312 \n",
      "Iteration: 2026. Loss: 1.179930329322815 \n",
      "Iteration: 2027. Loss: 1.1840903759002686 \n",
      "Iteration: 2028. Loss: 1.1629557609558105 \n",
      "Iteration: 2029. Loss: 1.1741348505020142 \n",
      "Iteration: 2030. Loss: 1.1812433004379272 \n",
      "Iteration: 2031. Loss: 1.2555210590362549 \n",
      "Iteration: 2032. Loss: 1.1015678644180298 \n",
      "Iteration: 2033. Loss: 1.1444090604782104 \n",
      "Iteration: 2034. Loss: 1.2377994060516357 \n",
      "Iteration: 2035. Loss: 1.2265303134918213 \n",
      "Iteration: 2036. Loss: 1.2171217203140259 \n",
      "Iteration: 2037. Loss: 1.2442328929901123 \n",
      "Iteration: 2038. Loss: 1.0734597444534302 \n",
      "Iteration: 2039. Loss: 1.120558261871338 \n",
      "Iteration: 2040. Loss: 1.229339599609375 \n",
      "Iteration: 2041. Loss: 1.232223391532898 \n",
      "Iteration: 2042. Loss: 1.2049686908721924 \n",
      "Iteration: 2043. Loss: 1.1399426460266113 \n",
      "Iteration: 2044. Loss: 1.144993782043457 \n",
      "Iteration: 2045. Loss: 1.1145421266555786 \n",
      "Iteration: 2046. Loss: 1.2421153783798218 \n",
      "Iteration: 2047. Loss: 1.1963986158370972 \n",
      "Iteration: 2048. Loss: 1.188195824623108 \n",
      "Iteration: 2049. Loss: 1.0271166563034058 \n",
      "Iteration: 2050. Loss: 1.121919870376587 \n",
      "Iteration: 2051. Loss: 1.2170705795288086 \n",
      "Iteration: 2052. Loss: 1.1404361724853516 \n",
      "Iteration: 2053. Loss: 1.0822336673736572 \n",
      "Iteration: 2054. Loss: 1.1926953792572021 \n",
      "Iteration: 2055. Loss: 1.274874210357666 \n",
      "Iteration: 2056. Loss: 1.2631970643997192 \n",
      "Iteration: 2057. Loss: 1.144809365272522 \n",
      "Iteration: 2058. Loss: 1.148648738861084 \n",
      "Iteration: 2059. Loss: 1.1511751413345337 \n",
      "Iteration: 2060. Loss: 1.3197044134140015 \n",
      "Iteration: 2061. Loss: 1.156428575515747 \n",
      "Iteration: 2062. Loss: 1.2025693655014038 \n",
      "Iteration: 2063. Loss: 1.1890652179718018 \n",
      "Iteration: 2064. Loss: 1.1115082502365112 \n",
      "Iteration: 2065. Loss: 1.1753331422805786 \n",
      "Iteration: 2066. Loss: 1.2115166187286377 \n",
      "Iteration: 2067. Loss: 1.2553843259811401 \n",
      "Iteration: 2068. Loss: 1.170468807220459 \n",
      "Iteration: 2069. Loss: 1.1777695417404175 \n",
      "Iteration: 2070. Loss: 1.183884620666504 \n",
      "Iteration: 2071. Loss: 1.1744794845581055 \n",
      "Iteration: 2072. Loss: 1.1368449926376343 \n",
      "Iteration: 2073. Loss: 1.1248137950897217 \n",
      "Iteration: 2074. Loss: 1.2057007551193237 \n",
      "Iteration: 2075. Loss: 1.170250415802002 \n",
      "Iteration: 2076. Loss: 1.151842474937439 \n",
      "Iteration: 2077. Loss: 1.182198405265808 \n",
      "Iteration: 2078. Loss: 1.099084496498108 \n",
      "Iteration: 2079. Loss: 1.2283244132995605 \n",
      "Iteration: 2080. Loss: 1.2509459257125854 \n",
      "Iteration: 2081. Loss: 1.1878927946090698 \n",
      "Iteration: 2082. Loss: 1.19805109500885 \n",
      "Iteration: 2083. Loss: 1.2330565452575684 \n",
      "Iteration: 2084. Loss: 1.1679211854934692 \n",
      "Iteration: 2085. Loss: 1.1519768238067627 \n",
      "Iteration: 2086. Loss: 1.1189279556274414 \n",
      "Iteration: 2087. Loss: 1.1357518434524536 \n",
      "Iteration: 2088. Loss: 1.2240002155303955 \n",
      "Iteration: 2089. Loss: 1.2189686298370361 \n",
      "Iteration: 2090. Loss: 1.1555736064910889 \n",
      "Iteration: 2091. Loss: 1.1596394777297974 \n",
      "Iteration: 2092. Loss: 1.0920195579528809 \n",
      "Iteration: 2093. Loss: 1.2022037506103516 \n",
      "Iteration: 2094. Loss: 1.1526668071746826 \n",
      "Iteration: 2095. Loss: 1.1658357381820679 \n",
      "Iteration: 2096. Loss: 1.1616202592849731 \n",
      "Iteration: 2097. Loss: 1.241411805152893 \n",
      "Iteration: 2098. Loss: 1.161860704421997 \n",
      "Iteration: 2099. Loss: 1.2241765260696411 \n",
      "Iteration: 2100. Loss: 1.1468656063079834 \n",
      "Iteration: 2101. Loss: 1.1639094352722168 \n",
      "Iteration: 2102. Loss: 1.2556350231170654 \n",
      "Iteration: 2103. Loss: 1.1321409940719604 \n",
      "Iteration: 2104. Loss: 1.2645517587661743 \n",
      "Iteration: 2105. Loss: 1.1877617835998535 \n",
      "Iteration: 2106. Loss: 1.0789942741394043 \n",
      "Iteration: 2107. Loss: 1.171513319015503 \n",
      "Iteration: 2108. Loss: 1.1660592555999756 \n",
      "Iteration: 2109. Loss: 1.2035232782363892 \n",
      "Iteration: 2110. Loss: 1.2215503454208374 \n",
      "Iteration: 2111. Loss: 1.1687569618225098 \n",
      "Iteration: 2112. Loss: 1.1414198875427246 \n",
      "Iteration: 2113. Loss: 1.154050350189209 \n",
      "Iteration: 2114. Loss: 1.1400865316390991 \n",
      "Iteration: 2115. Loss: 1.1248416900634766 \n",
      "Iteration: 2116. Loss: 1.121364712715149 \n",
      "Iteration: 2117. Loss: 1.1662487983703613 \n",
      "Iteration: 2118. Loss: 1.1762129068374634 \n",
      "Iteration: 2119. Loss: 1.2294944524765015 \n",
      "Iteration: 2120. Loss: 1.1703541278839111 \n",
      "Iteration: 2121. Loss: 1.0784000158309937 \n",
      "Iteration: 2122. Loss: 1.2339258193969727 \n",
      "Iteration: 2123. Loss: 1.1201447248458862 \n",
      "Iteration: 2124. Loss: 1.1420024633407593 \n",
      "Iteration: 2125. Loss: 1.237345814704895 \n",
      "Iteration: 2126. Loss: 1.1768875122070312 \n",
      "Iteration: 2127. Loss: 1.1412200927734375 \n",
      "Iteration: 2128. Loss: 1.1616743803024292 \n",
      "Iteration: 2129. Loss: 1.2272214889526367 \n",
      "Iteration: 2130. Loss: 1.1435517072677612 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2131. Loss: 1.192460298538208 \n",
      "Iteration: 2132. Loss: 1.1492747068405151 \n",
      "Iteration: 2133. Loss: 1.2901486158370972 \n",
      "Iteration: 2134. Loss: 1.1000193357467651 \n",
      "Iteration: 2135. Loss: 1.0717849731445312 \n",
      "Iteration: 2136. Loss: 1.1799756288528442 \n",
      "Iteration: 2137. Loss: 1.0694336891174316 \n",
      "Iteration: 2138. Loss: 1.1694318056106567 \n",
      "Iteration: 2139. Loss: 1.032594084739685 \n",
      "Iteration: 2140. Loss: 1.0754331350326538 \n",
      "Iteration: 2141. Loss: 1.1565285921096802 \n",
      "Iteration: 2142. Loss: 1.1351041793823242 \n",
      "Iteration: 2143. Loss: 1.1602307558059692 \n",
      "Iteration: 2144. Loss: 1.1573116779327393 \n",
      "Iteration: 2145. Loss: 1.158410668373108 \n",
      "Iteration: 2146. Loss: 1.202126383781433 \n",
      "Iteration: 2147. Loss: 1.0786638259887695 \n",
      "Iteration: 2148. Loss: 1.2681684494018555 \n",
      "Iteration: 2149. Loss: 1.1727255582809448 \n",
      "Iteration: 2150. Loss: 1.121290683746338 \n",
      "Iteration: 2151. Loss: 1.2491728067398071 \n",
      "Iteration: 2152. Loss: 1.1686286926269531 \n",
      "Iteration: 2153. Loss: 1.090714454650879 \n",
      "Iteration: 2154. Loss: 1.1410070657730103 \n",
      "Iteration: 2155. Loss: 1.1085100173950195 \n",
      "Iteration: 2156. Loss: 1.1784636974334717 \n",
      "Iteration: 2157. Loss: 1.1374646425247192 \n",
      "Iteration: 2158. Loss: 1.0984963178634644 \n",
      "Iteration: 2159. Loss: 1.1316213607788086 \n",
      "Iteration: 2160. Loss: 1.132573127746582 \n",
      "Iteration: 2161. Loss: 1.1309279203414917 \n",
      "Iteration: 2162. Loss: 1.0823308229446411 \n",
      "Iteration: 2163. Loss: 1.1359745264053345 \n",
      "Iteration: 2164. Loss: 1.1546300649642944 \n",
      "Iteration: 2165. Loss: 1.111757516860962 \n",
      "Iteration: 2166. Loss: 1.1288596391677856 \n",
      "Iteration: 2167. Loss: 1.0280389785766602 \n",
      "Iteration: 2168. Loss: 1.1515456438064575 \n",
      "Iteration: 2169. Loss: 1.136082410812378 \n",
      "Iteration: 2170. Loss: 1.1648051738739014 \n",
      "Iteration: 2171. Loss: 1.1717002391815186 \n",
      "Iteration: 2172. Loss: 1.2051291465759277 \n",
      "Iteration: 2173. Loss: 1.1484899520874023 \n",
      "Iteration: 2174. Loss: 1.2535632848739624 \n",
      "Iteration: 2175. Loss: 1.1590241193771362 \n",
      "Iteration: 2176. Loss: 1.0823909044265747 \n",
      "Iteration: 2177. Loss: 1.2394262552261353 \n",
      "Iteration: 2178. Loss: 1.1160385608673096 \n",
      "Iteration: 2179. Loss: 1.1440153121948242 \n",
      "Iteration: 2180. Loss: 1.2521018981933594 \n",
      "Iteration: 2181. Loss: 1.126905083656311 \n",
      "Iteration: 2182. Loss: 1.0810202360153198 \n",
      "Iteration: 2183. Loss: 1.189215064048767 \n",
      "Iteration: 2184. Loss: 1.1048036813735962 \n",
      "Iteration: 2185. Loss: 1.3046138286590576 \n",
      "Iteration: 2186. Loss: 1.1591986417770386 \n",
      "Iteration: 2187. Loss: 1.038266658782959 \n",
      "Iteration: 2188. Loss: 1.114137053489685 \n",
      "Iteration: 2189. Loss: 1.1187310218811035 \n",
      "Iteration: 2190. Loss: 1.0662838220596313 \n",
      "Iteration: 2191. Loss: 1.1228570938110352 \n",
      "Iteration: 2192. Loss: 1.1538525819778442 \n",
      "Iteration: 2193. Loss: 1.1679002046585083 \n",
      "Iteration: 2194. Loss: 1.22432279586792 \n",
      "Iteration: 2195. Loss: 1.160353183746338 \n",
      "Iteration: 2196. Loss: 1.133009672164917 \n",
      "Iteration: 2197. Loss: 1.1501048803329468 \n",
      "Iteration: 2198. Loss: 1.1776776313781738 \n",
      "Iteration: 2199. Loss: 1.1520533561706543 \n",
      "Iteration: 2200. Loss: 1.0789257287979126 \n",
      "Iteration: 2201. Loss: 1.167238712310791 \n",
      "Iteration: 2202. Loss: 0.9964069128036499 \n",
      "Iteration: 2203. Loss: 1.1297804117202759 \n",
      "Iteration: 2204. Loss: 1.1145879030227661 \n",
      "Iteration: 2205. Loss: 1.2272852659225464 \n",
      "Iteration: 2206. Loss: 1.159995675086975 \n",
      "Iteration: 2207. Loss: 1.092322587966919 \n",
      "Iteration: 2208. Loss: 1.1076555252075195 \n",
      "Iteration: 2209. Loss: 1.091663122177124 \n",
      "Iteration: 2210. Loss: 1.1579008102416992 \n",
      "Iteration: 2211. Loss: 1.1717602014541626 \n",
      "Iteration: 2212. Loss: 1.1360377073287964 \n",
      "Iteration: 2213. Loss: 1.038036823272705 \n",
      "Iteration: 2214. Loss: 1.187506079673767 \n",
      "Iteration: 2215. Loss: 1.118833065032959 \n",
      "Iteration: 2216. Loss: 1.182957410812378 \n",
      "Iteration: 2217. Loss: 1.2365394830703735 \n",
      "Iteration: 2218. Loss: 1.1570185422897339 \n",
      "Iteration: 2219. Loss: 1.0563576221466064 \n",
      "Iteration: 2220. Loss: 1.191211462020874 \n",
      "Iteration: 2221. Loss: 1.0880651473999023 \n",
      "Iteration: 2222. Loss: 1.1977880001068115 \n",
      "Iteration: 2223. Loss: 1.1899758577346802 \n",
      "Iteration: 2224. Loss: 1.2092777490615845 \n",
      "Iteration: 2225. Loss: 1.1741018295288086 \n",
      "Iteration: 2226. Loss: 1.0828044414520264 \n",
      "Iteration: 2227. Loss: 1.1496834754943848 \n",
      "Iteration: 2228. Loss: 1.2312884330749512 \n",
      "Iteration: 2229. Loss: 1.1486777067184448 \n",
      "Iteration: 2230. Loss: 1.2315579652786255 \n",
      "Iteration: 2231. Loss: 1.0969492197036743 \n",
      "Iteration: 2232. Loss: 1.1620434522628784 \n",
      "Iteration: 2233. Loss: 1.087303876876831 \n",
      "Iteration: 2234. Loss: 1.15566885471344 \n",
      "Iteration: 2235. Loss: 1.1282241344451904 \n",
      "Iteration: 2236. Loss: 1.121519923210144 \n",
      "Iteration: 2237. Loss: 1.1264715194702148 \n",
      "Iteration: 2238. Loss: 1.1923320293426514 \n",
      "Iteration: 2239. Loss: 1.1162854433059692 \n",
      "Iteration: 2240. Loss: 1.207777500152588 \n",
      "Iteration: 2241. Loss: 1.070396065711975 \n",
      "Iteration: 2242. Loss: 1.081943154335022 \n",
      "Iteration: 2243. Loss: 1.1844589710235596 \n",
      "Iteration: 2244. Loss: 1.141558051109314 \n",
      "Iteration: 2245. Loss: 1.0777665376663208 \n",
      "Iteration: 2246. Loss: 1.1324273347854614 \n",
      "Iteration: 2247. Loss: 1.1147091388702393 \n",
      "Iteration: 2248. Loss: 1.1171083450317383 \n",
      "Iteration: 2249. Loss: 1.1037665605545044 \n",
      "Iteration: 2250. Loss: 1.0466939210891724 \n",
      "Iteration: 2251. Loss: 1.186400055885315 \n",
      "Iteration: 2252. Loss: 1.1414021253585815 \n",
      "Iteration: 2253. Loss: 1.1305474042892456 \n",
      "Iteration: 2254. Loss: 1.1844898462295532 \n",
      "Iteration: 2255. Loss: 1.1885870695114136 \n",
      "Iteration: 2256. Loss: 1.0922644138336182 \n",
      "Iteration: 2257. Loss: 1.129124402999878 \n",
      "Iteration: 2258. Loss: 1.143829345703125 \n",
      "Iteration: 2259. Loss: 1.2279248237609863 \n",
      "Iteration: 2260. Loss: 1.0654411315917969 \n",
      "Iteration: 2261. Loss: 1.1551880836486816 \n",
      "Iteration: 2262. Loss: 1.1573028564453125 \n",
      "Iteration: 2263. Loss: 1.1630830764770508 \n",
      "Iteration: 2264. Loss: 1.066715121269226 \n",
      "Iteration: 2265. Loss: 1.095678448677063 \n",
      "Iteration: 2266. Loss: 1.1217190027236938 \n",
      "Iteration: 2267. Loss: 1.0974318981170654 \n",
      "Iteration: 2268. Loss: 1.213326334953308 \n",
      "Iteration: 2269. Loss: 1.0687222480773926 \n",
      "Iteration: 2270. Loss: 1.1668968200683594 \n",
      "Iteration: 2271. Loss: 1.2151215076446533 \n",
      "Iteration: 2272. Loss: 1.0800589323043823 \n",
      "Iteration: 2273. Loss: 1.1464675664901733 \n",
      "Iteration: 2274. Loss: 1.1671929359436035 \n",
      "Iteration: 2275. Loss: 1.096606731414795 \n",
      "Iteration: 2276. Loss: 1.1362940073013306 \n",
      "Iteration: 2277. Loss: 1.1867949962615967 \n",
      "Iteration: 2278. Loss: 1.1633509397506714 \n",
      "Iteration: 2279. Loss: 1.1675059795379639 \n",
      "Iteration: 2280. Loss: 1.0396476984024048 \n",
      "Iteration: 2281. Loss: 1.1196496486663818 \n",
      "Iteration: 2282. Loss: 1.1051015853881836 \n",
      "Iteration: 2283. Loss: 1.0819796323776245 \n",
      "Iteration: 2284. Loss: 1.173111915588379 \n",
      "Iteration: 2285. Loss: 1.225751280784607 \n",
      "Iteration: 2286. Loss: 1.1152821779251099 \n",
      "Iteration: 2287. Loss: 1.089866280555725 \n",
      "Iteration: 2288. Loss: 1.0469995737075806 \n",
      "Iteration: 2289. Loss: 1.0538593530654907 \n",
      "Iteration: 2290. Loss: 1.0740553140640259 \n",
      "Iteration: 2291. Loss: 1.1501126289367676 \n",
      "Iteration: 2292. Loss: 1.248572587966919 \n",
      "Iteration: 2293. Loss: 1.1085667610168457 \n",
      "Iteration: 2294. Loss: 1.1405295133590698 \n",
      "Iteration: 2295. Loss: 1.2022559642791748 \n",
      "Iteration: 2296. Loss: 1.0652282238006592 \n",
      "Iteration: 2297. Loss: 1.1595940589904785 \n",
      "Iteration: 2298. Loss: 1.0821750164031982 \n",
      "Iteration: 2299. Loss: 1.133465051651001 \n",
      "Iteration: 2300. Loss: 1.1218053102493286 \n",
      "Iteration: 2301. Loss: 1.0699858665466309 \n",
      "Iteration: 2302. Loss: 1.0914515256881714 \n",
      "Iteration: 2303. Loss: 1.116044521331787 \n",
      "Iteration: 2304. Loss: 1.1146034002304077 \n",
      "Iteration: 2305. Loss: 1.1322088241577148 \n",
      "Iteration: 2306. Loss: 1.0868501663208008 \n",
      "Iteration: 2307. Loss: 1.1387667655944824 \n",
      "Iteration: 2308. Loss: 1.163469672203064 \n",
      "Iteration: 2309. Loss: 1.1526659727096558 \n",
      "Iteration: 2310. Loss: 1.111226201057434 \n",
      "Iteration: 2311. Loss: 1.140689730644226 \n",
      "Iteration: 2312. Loss: 1.139927625656128 \n",
      "Iteration: 2313. Loss: 1.1368919610977173 \n",
      "Iteration: 2314. Loss: 1.1473268270492554 \n",
      "Iteration: 2315. Loss: 1.111870527267456 \n",
      "Iteration: 2316. Loss: 1.1380668878555298 \n",
      "Iteration: 2317. Loss: 1.105535626411438 \n",
      "Iteration: 2318. Loss: 1.1351820230484009 \n",
      "Iteration: 2319. Loss: 1.1085237264633179 \n",
      "Iteration: 2320. Loss: 1.1393768787384033 \n",
      "Iteration: 2321. Loss: 1.1741374731063843 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2322. Loss: 1.2114264965057373 \n",
      "Iteration: 2323. Loss: 1.1439869403839111 \n",
      "Iteration: 2324. Loss: 1.229088306427002 \n",
      "Iteration: 2325. Loss: 1.1288553476333618 \n",
      "Iteration: 2326. Loss: 1.1330615282058716 \n",
      "Iteration: 2327. Loss: 1.1153030395507812 \n",
      "Iteration: 2328. Loss: 1.1145669221878052 \n",
      "Iteration: 2329. Loss: 1.0877841711044312 \n",
      "Iteration: 2330. Loss: 1.1423752307891846 \n",
      "Iteration: 2331. Loss: 1.0883105993270874 \n",
      "Iteration: 2332. Loss: 1.2041473388671875 \n",
      "Iteration: 2333. Loss: 1.0997157096862793 \n",
      "Iteration: 2334. Loss: 1.083531379699707 \n",
      "Iteration: 2335. Loss: 1.067569613456726 \n",
      "Iteration: 2336. Loss: 1.0793709754943848 \n",
      "Iteration: 2337. Loss: 1.1191949844360352 \n",
      "Iteration: 2338. Loss: 1.0862025022506714 \n",
      "Iteration: 2339. Loss: 1.1129778623580933 \n",
      "Iteration: 2340. Loss: 1.1119085550308228 \n",
      "Iteration: 2341. Loss: 1.113915205001831 \n",
      "Iteration: 2342. Loss: 1.1889166831970215 \n",
      "Iteration: 2343. Loss: 1.1004395484924316 \n",
      "Iteration: 2344. Loss: 1.1280930042266846 \n",
      "Iteration: 2345. Loss: 1.0588231086730957 \n",
      "Iteration: 2346. Loss: 1.0988408327102661 \n",
      "Iteration: 2347. Loss: 1.0609239339828491 \n",
      "Iteration: 2348. Loss: 1.1482733488082886 \n",
      "Iteration: 2349. Loss: 1.1382508277893066 \n",
      "Iteration: 2350. Loss: 1.0681352615356445 \n",
      "Iteration: 2351. Loss: 1.2662665843963623 \n",
      "Iteration: 2352. Loss: 1.0834046602249146 \n",
      "Iteration: 2353. Loss: 1.146213412284851 \n",
      "Iteration: 2354. Loss: 1.1243047714233398 \n",
      "Iteration: 2355. Loss: 1.074535846710205 \n",
      "Iteration: 2356. Loss: 1.2211008071899414 \n",
      "Iteration: 2357. Loss: 1.1994069814682007 \n",
      "Iteration: 2358. Loss: 1.067157506942749 \n",
      "Iteration: 2359. Loss: 1.078824520111084 \n",
      "Iteration: 2360. Loss: 1.0834463834762573 \n",
      "Iteration: 2361. Loss: 1.1056995391845703 \n",
      "Iteration: 2362. Loss: 1.117477297782898 \n",
      "Iteration: 2363. Loss: 1.0946154594421387 \n",
      "Iteration: 2364. Loss: 1.068692684173584 \n",
      "Iteration: 2365. Loss: 1.04608952999115 \n",
      "Iteration: 2366. Loss: 1.0241645574569702 \n",
      "Iteration: 2367. Loss: 1.0442302227020264 \n",
      "Iteration: 2368. Loss: 1.2038352489471436 \n",
      "Iteration: 2369. Loss: 1.0965100526809692 \n",
      "Iteration: 2370. Loss: 1.1480354070663452 \n",
      "Iteration: 2371. Loss: 1.0847854614257812 \n",
      "Iteration: 2372. Loss: 1.1229606866836548 \n",
      "Iteration: 2373. Loss: 1.0721105337142944 \n",
      "Iteration: 2374. Loss: 1.065457820892334 \n",
      "Iteration: 2375. Loss: 1.1315126419067383 \n",
      "Iteration: 2376. Loss: 1.196016788482666 \n",
      "Iteration: 2377. Loss: 1.1877562999725342 \n",
      "Iteration: 2378. Loss: 0.9792079329490662 \n",
      "Iteration: 2379. Loss: 1.1706629991531372 \n",
      "Iteration: 2380. Loss: 1.0500634908676147 \n",
      "Iteration: 2381. Loss: 1.1666512489318848 \n",
      "Iteration: 2382. Loss: 1.165051817893982 \n",
      "Iteration: 2383. Loss: 1.1330580711364746 \n",
      "Iteration: 2384. Loss: 1.172450304031372 \n",
      "Iteration: 2385. Loss: 1.131190538406372 \n",
      "Iteration: 2386. Loss: 1.130518913269043 \n",
      "Iteration: 2387. Loss: 1.0832427740097046 \n",
      "Iteration: 2388. Loss: 1.1417698860168457 \n",
      "Iteration: 2389. Loss: 1.1078572273254395 \n",
      "Iteration: 2390. Loss: 1.0615088939666748 \n",
      "Iteration: 2391. Loss: 0.9927542209625244 \n",
      "Iteration: 2392. Loss: 0.9879639148712158 \n",
      "Iteration: 2393. Loss: 1.008618712425232 \n",
      "Iteration: 2394. Loss: 1.1255459785461426 \n",
      "Iteration: 2395. Loss: 1.0990874767303467 \n",
      "Iteration: 2396. Loss: 1.1119537353515625 \n",
      "Iteration: 2397. Loss: 1.0802935361862183 \n",
      "Iteration: 2398. Loss: 1.150549292564392 \n",
      "Iteration: 2399. Loss: 1.0389165878295898 \n",
      "Iteration: 2400. Loss: 1.0367417335510254 \n",
      "Iteration: 2401. Loss: 1.0681195259094238 \n",
      "Iteration: 2402. Loss: 1.076572299003601 \n",
      "Iteration: 2403. Loss: 1.1061338186264038 \n",
      "Iteration: 2404. Loss: 1.1711151599884033 \n",
      "Iteration: 2405. Loss: 1.0729496479034424 \n",
      "Iteration: 2406. Loss: 1.0708147287368774 \n",
      "Iteration: 2407. Loss: 1.1081711053848267 \n",
      "Iteration: 2408. Loss: 1.0798872709274292 \n",
      "Iteration: 2409. Loss: 1.1221809387207031 \n",
      "Iteration: 2410. Loss: 1.0206831693649292 \n",
      "Iteration: 2411. Loss: 1.1059194803237915 \n",
      "Iteration: 2412. Loss: 1.122600793838501 \n",
      "Iteration: 2413. Loss: 1.0968406200408936 \n",
      "Iteration: 2414. Loss: 1.0208179950714111 \n",
      "Iteration: 2415. Loss: 1.1498291492462158 \n",
      "Iteration: 2416. Loss: 1.0510873794555664 \n",
      "Iteration: 2417. Loss: 1.181878924369812 \n",
      "Iteration: 2418. Loss: 1.165356993675232 \n",
      "Iteration: 2419. Loss: 1.1555922031402588 \n",
      "Iteration: 2420. Loss: 1.2061465978622437 \n",
      "Iteration: 2421. Loss: 1.1573978662490845 \n",
      "Iteration: 2422. Loss: 1.0671993494033813 \n",
      "Iteration: 2423. Loss: 1.1318050622940063 \n",
      "Iteration: 2424. Loss: 1.1365728378295898 \n",
      "Iteration: 2425. Loss: 1.0548715591430664 \n",
      "Iteration: 2426. Loss: 1.0935389995574951 \n",
      "Iteration: 2427. Loss: 1.1516398191452026 \n",
      "Iteration: 2428. Loss: 1.1764745712280273 \n",
      "Iteration: 2429. Loss: 1.0543018579483032 \n",
      "Iteration: 2430. Loss: 1.0678945779800415 \n",
      "Iteration: 2431. Loss: 1.0846168994903564 \n",
      "Iteration: 2432. Loss: 1.128023624420166 \n",
      "Iteration: 2433. Loss: 1.1490274667739868 \n",
      "Iteration: 2434. Loss: 1.1502578258514404 \n",
      "Iteration: 2435. Loss: 1.1219388246536255 \n",
      "Iteration: 2436. Loss: 1.090247631072998 \n",
      "Iteration: 2437. Loss: 1.1204657554626465 \n",
      "Iteration: 2438. Loss: 1.1322537660598755 \n",
      "Iteration: 2439. Loss: 1.089412808418274 \n",
      "Iteration: 2440. Loss: 1.0902533531188965 \n",
      "Iteration: 2441. Loss: 1.0357002019882202 \n",
      "Iteration: 2442. Loss: 1.0175905227661133 \n",
      "Iteration: 2443. Loss: 1.0448886156082153 \n",
      "Iteration: 2444. Loss: 1.0609395503997803 \n",
      "Iteration: 2445. Loss: 0.9965036511421204 \n",
      "Iteration: 2446. Loss: 1.2708978652954102 \n",
      "Iteration: 2447. Loss: 1.2269598245620728 \n",
      "Iteration: 2448. Loss: 1.0459585189819336 \n",
      "Iteration: 2449. Loss: 0.9765340685844421 \n",
      "Iteration: 2450. Loss: 1.086653232574463 \n",
      "Iteration: 2451. Loss: 1.0939468145370483 \n",
      "Iteration: 2452. Loss: 1.086140513420105 \n",
      "Iteration: 2453. Loss: 1.1287291049957275 \n",
      "Iteration: 2454. Loss: 1.0230987071990967 \n",
      "Iteration: 2455. Loss: 1.1217427253723145 \n",
      "Iteration: 2456. Loss: 1.0835814476013184 \n",
      "Iteration: 2457. Loss: 0.974417507648468 \n",
      "Iteration: 2458. Loss: 1.0940040349960327 \n",
      "Iteration: 2459. Loss: 1.0846962928771973 \n",
      "Iteration: 2460. Loss: 1.1275856494903564 \n",
      "Iteration: 2461. Loss: 1.0131900310516357 \n",
      "Iteration: 2462. Loss: 1.1495006084442139 \n",
      "Iteration: 2463. Loss: 1.0117003917694092 \n",
      "Iteration: 2464. Loss: 1.131883144378662 \n",
      "Iteration: 2465. Loss: 1.0738012790679932 \n",
      "Iteration: 2466. Loss: 1.0938315391540527 \n",
      "Iteration: 2467. Loss: 1.1056891679763794 \n",
      "Iteration: 2468. Loss: 1.1461626291275024 \n",
      "Iteration: 2469. Loss: 1.1255879402160645 \n",
      "Iteration: 2470. Loss: 1.165518045425415 \n",
      "Iteration: 2471. Loss: 1.0683292150497437 \n",
      "Iteration: 2472. Loss: 1.1369514465332031 \n",
      "Iteration: 2473. Loss: 1.1116297245025635 \n",
      "Iteration: 2474. Loss: 1.1007012128829956 \n",
      "Iteration: 2475. Loss: 1.1282968521118164 \n",
      "Iteration: 2476. Loss: 1.010596752166748 \n",
      "Iteration: 2477. Loss: 1.181503415107727 \n",
      "Iteration: 2478. Loss: 1.0353633165359497 \n",
      "Iteration: 2479. Loss: 1.1589109897613525 \n",
      "Iteration: 2480. Loss: 1.0267863273620605 \n",
      "Iteration: 2481. Loss: 1.074363350868225 \n",
      "Iteration: 2482. Loss: 1.0634093284606934 \n",
      "Iteration: 2483. Loss: 1.1134928464889526 \n",
      "Iteration: 2484. Loss: 1.1010609865188599 \n",
      "Iteration: 2485. Loss: 1.0419803857803345 \n",
      "Iteration: 2486. Loss: 1.0487968921661377 \n",
      "Iteration: 2487. Loss: 1.108535647392273 \n",
      "Iteration: 2488. Loss: 1.0914897918701172 \n",
      "Iteration: 2489. Loss: 1.1131476163864136 \n",
      "Iteration: 2490. Loss: 1.0204977989196777 \n",
      "Iteration: 2491. Loss: 1.1319047212600708 \n",
      "Iteration: 2492. Loss: 1.144627571105957 \n",
      "Iteration: 2493. Loss: 1.0151318311691284 \n",
      "Iteration: 2494. Loss: 1.134911298751831 \n",
      "Iteration: 2495. Loss: 1.1064800024032593 \n",
      "Iteration: 2496. Loss: 1.0781867504119873 \n",
      "Iteration: 2497. Loss: 1.0982853174209595 \n",
      "Iteration: 2498. Loss: 0.9982299208641052 \n",
      "Iteration: 2499. Loss: 1.1124438047409058 \n",
      "Iteration: 2500. Loss: 1.0919615030288696 \n",
      "Iteration: 2501. Loss: 1.1595667600631714 \n",
      "Iteration: 2502. Loss: 1.0681979656219482 \n",
      "Iteration: 2503. Loss: 1.1077872514724731 \n",
      "Iteration: 2504. Loss: 1.152204155921936 \n",
      "Iteration: 2505. Loss: 1.0319585800170898 \n",
      "Iteration: 2506. Loss: 1.1390169858932495 \n",
      "Iteration: 2507. Loss: 1.0311833620071411 \n",
      "Iteration: 2508. Loss: 1.1015117168426514 \n",
      "Iteration: 2509. Loss: 1.1422477960586548 \n",
      "Iteration: 2510. Loss: 1.1240485906600952 \n",
      "Iteration: 2511. Loss: 1.0086586475372314 \n",
      "Iteration: 2512. Loss: 1.0207388401031494 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2513. Loss: 1.1025207042694092 \n",
      "Iteration: 2514. Loss: 1.0662050247192383 \n",
      "Iteration: 2515. Loss: 1.1507525444030762 \n",
      "Iteration: 2516. Loss: 1.0878729820251465 \n",
      "Iteration: 2517. Loss: 1.1282633543014526 \n",
      "Iteration: 2518. Loss: 1.0486183166503906 \n",
      "Iteration: 2519. Loss: 1.0596057176589966 \n",
      "Iteration: 2520. Loss: 1.0390737056732178 \n",
      "Iteration: 2521. Loss: 0.9999217391014099 \n",
      "Iteration: 2522. Loss: 1.0629401206970215 \n",
      "Iteration: 2523. Loss: 1.1239063739776611 \n",
      "Iteration: 2524. Loss: 1.076723337173462 \n",
      "Iteration: 2525. Loss: 1.1044718027114868 \n",
      "Iteration: 2526. Loss: 0.9895103573799133 \n",
      "Iteration: 2527. Loss: 0.9632211327552795 \n",
      "Iteration: 2528. Loss: 0.969340980052948 \n",
      "Iteration: 2529. Loss: 1.1101746559143066 \n",
      "Iteration: 2530. Loss: 0.9778450727462769 \n",
      "Iteration: 2531. Loss: 1.0329678058624268 \n",
      "Iteration: 2532. Loss: 1.0682209730148315 \n",
      "Iteration: 2533. Loss: 1.0285046100616455 \n",
      "Iteration: 2534. Loss: 1.1328295469284058 \n",
      "Iteration: 2535. Loss: 1.1403037309646606 \n",
      "Iteration: 2536. Loss: 1.1125699281692505 \n",
      "Iteration: 2537. Loss: 1.0671062469482422 \n",
      "Iteration: 2538. Loss: 1.0192729234695435 \n",
      "Iteration: 2539. Loss: 1.0375205278396606 \n",
      "Iteration: 2540. Loss: 1.0759918689727783 \n",
      "Iteration: 2541. Loss: 1.1235911846160889 \n",
      "Iteration: 2542. Loss: 1.1104612350463867 \n",
      "Iteration: 2543. Loss: 1.0290759801864624 \n",
      "Iteration: 2544. Loss: 1.083545207977295 \n",
      "Iteration: 2545. Loss: 1.150356411933899 \n",
      "Iteration: 2546. Loss: 1.076884388923645 \n",
      "Iteration: 2547. Loss: 1.028138279914856 \n",
      "Iteration: 2548. Loss: 1.109243392944336 \n",
      "Iteration: 2549. Loss: 1.1300896406173706 \n",
      "Iteration: 2550. Loss: 1.141683578491211 \n",
      "Iteration: 2551. Loss: 1.140766978263855 \n",
      "Iteration: 2552. Loss: 1.0080550909042358 \n",
      "Iteration: 2553. Loss: 1.093545913696289 \n",
      "Iteration: 2554. Loss: 1.1423726081848145 \n",
      "Iteration: 2555. Loss: 1.054815649986267 \n",
      "Iteration: 2556. Loss: 1.0718705654144287 \n",
      "Iteration: 2557. Loss: 1.0291916131973267 \n",
      "Iteration: 2558. Loss: 0.994541347026825 \n",
      "Iteration: 2559. Loss: 1.0107810497283936 \n",
      "Iteration: 2560. Loss: 0.9933622479438782 \n",
      "Iteration: 2561. Loss: 1.0962249040603638 \n",
      "Iteration: 2562. Loss: 1.0960861444473267 \n",
      "Iteration: 2563. Loss: 1.0418479442596436 \n",
      "Iteration: 2564. Loss: 1.058269739151001 \n",
      "Iteration: 2565. Loss: 1.1017664670944214 \n",
      "Iteration: 2566. Loss: 1.0932576656341553 \n",
      "Iteration: 2567. Loss: 1.144789695739746 \n",
      "Iteration: 2568. Loss: 1.1319611072540283 \n",
      "Iteration: 2569. Loss: 1.0077238082885742 \n",
      "Iteration: 2570. Loss: 1.2053054571151733 \n",
      "Iteration: 2571. Loss: 1.1011470556259155 \n",
      "Iteration: 2572. Loss: 1.1908516883850098 \n",
      "Iteration: 2573. Loss: 1.0628472566604614 \n",
      "Iteration: 2574. Loss: 1.0819249153137207 \n",
      "Iteration: 2575. Loss: 1.0716586112976074 \n",
      "Iteration: 2576. Loss: 1.053658127784729 \n",
      "Iteration: 2577. Loss: 1.0941941738128662 \n",
      "Iteration: 2578. Loss: 0.9565670490264893 \n",
      "Iteration: 2579. Loss: 1.0308494567871094 \n",
      "Iteration: 2580. Loss: 1.0163166522979736 \n",
      "Iteration: 2581. Loss: 1.146233081817627 \n",
      "Iteration: 2582. Loss: 1.0635857582092285 \n",
      "Iteration: 2583. Loss: 1.0993797779083252 \n",
      "Iteration: 2584. Loss: 1.0316658020019531 \n",
      "Iteration: 2585. Loss: 1.0143523216247559 \n",
      "Iteration: 2586. Loss: 1.0731353759765625 \n",
      "Iteration: 2587. Loss: 1.0777339935302734 \n",
      "Iteration: 2588. Loss: 1.1225192546844482 \n",
      "Iteration: 2589. Loss: 1.0076930522918701 \n",
      "Iteration: 2590. Loss: 1.0974732637405396 \n",
      "Iteration: 2591. Loss: 1.0874255895614624 \n",
      "Iteration: 2592. Loss: 1.039968490600586 \n",
      "Iteration: 2593. Loss: 1.1457096338272095 \n",
      "Iteration: 2594. Loss: 0.9716666340827942 \n",
      "Iteration: 2595. Loss: 1.1159460544586182 \n",
      "Iteration: 2596. Loss: 1.033643126487732 \n",
      "Iteration: 2597. Loss: 1.063501000404358 \n",
      "Iteration: 2598. Loss: 1.138526439666748 \n",
      "Iteration: 2599. Loss: 1.1197620630264282 \n",
      "Iteration: 2600. Loss: 1.0682978630065918 \n",
      "Iteration: 2601. Loss: 1.1175988912582397 \n",
      "Iteration: 2602. Loss: 1.023352861404419 \n",
      "Iteration: 2603. Loss: 0.9703653454780579 \n",
      "Iteration: 2604. Loss: 1.086115837097168 \n",
      "Iteration: 2605. Loss: 1.0831557512283325 \n",
      "Iteration: 2606. Loss: 1.142616629600525 \n",
      "Iteration: 2607. Loss: 0.9956010580062866 \n",
      "Iteration: 2608. Loss: 1.0851198434829712 \n",
      "Iteration: 2609. Loss: 1.0735270977020264 \n",
      "Iteration: 2610. Loss: 1.0650900602340698 \n",
      "Iteration: 2611. Loss: 1.1298052072525024 \n",
      "Iteration: 2612. Loss: 1.0403786897659302 \n",
      "Iteration: 2613. Loss: 1.107652187347412 \n",
      "Iteration: 2614. Loss: 0.9946840405464172 \n",
      "Iteration: 2615. Loss: 1.0558937788009644 \n",
      "Iteration: 2616. Loss: 1.0785107612609863 \n",
      "Iteration: 2617. Loss: 1.0040358304977417 \n",
      "Iteration: 2618. Loss: 0.9715985655784607 \n",
      "Iteration: 2619. Loss: 1.0213819742202759 \n",
      "Iteration: 2620. Loss: 1.1286041736602783 \n",
      "Iteration: 2621. Loss: 1.0437498092651367 \n",
      "Iteration: 2622. Loss: 1.075241208076477 \n",
      "Iteration: 2623. Loss: 0.9797512888908386 \n",
      "Iteration: 2624. Loss: 1.0323491096496582 \n",
      "Iteration: 2625. Loss: 1.0959725379943848 \n",
      "Iteration: 2626. Loss: 0.9937668442726135 \n",
      "Iteration: 2627. Loss: 1.028342843055725 \n",
      "Iteration: 2628. Loss: 1.0374808311462402 \n",
      "Iteration: 2629. Loss: 1.0725796222686768 \n",
      "Iteration: 2630. Loss: 1.0472286939620972 \n",
      "Iteration: 2631. Loss: 1.0646575689315796 \n",
      "Iteration: 2632. Loss: 1.2358765602111816 \n",
      "Iteration: 2633. Loss: 1.019008755683899 \n",
      "Iteration: 2634. Loss: 1.0494893789291382 \n",
      "Iteration: 2635. Loss: 1.067620038986206 \n",
      "Iteration: 2636. Loss: 0.9836445450782776 \n",
      "Iteration: 2637. Loss: 1.0790891647338867 \n",
      "Iteration: 2638. Loss: 0.9625462889671326 \n",
      "Iteration: 2639. Loss: 1.1093034744262695 \n",
      "Iteration: 2640. Loss: 1.1571202278137207 \n",
      "Iteration: 2641. Loss: 0.9954107403755188 \n",
      "Iteration: 2642. Loss: 0.9533636569976807 \n",
      "Iteration: 2643. Loss: 1.0477430820465088 \n",
      "Iteration: 2644. Loss: 1.0656123161315918 \n",
      "Iteration: 2645. Loss: 1.2499579191207886 \n",
      "Iteration: 2646. Loss: 1.1651949882507324 \n",
      "Iteration: 2647. Loss: 1.0443071126937866 \n",
      "Iteration: 2648. Loss: 1.0997533798217773 \n",
      "Iteration: 2649. Loss: 1.0620896816253662 \n",
      "Iteration: 2650. Loss: 1.093564510345459 \n",
      "Iteration: 2651. Loss: 1.0565828084945679 \n",
      "Iteration: 2652. Loss: 1.0620156526565552 \n",
      "Iteration: 2653. Loss: 0.9973759651184082 \n",
      "Iteration: 2654. Loss: 1.0076478719711304 \n",
      "Iteration: 2655. Loss: 1.1820484399795532 \n",
      "Iteration: 2656. Loss: 1.1033554077148438 \n",
      "Iteration: 2657. Loss: 0.935416042804718 \n",
      "Iteration: 2658. Loss: 1.140525460243225 \n",
      "Iteration: 2659. Loss: 1.0396275520324707 \n",
      "Iteration: 2660. Loss: 1.1456559896469116 \n",
      "Iteration: 2661. Loss: 0.9801127910614014 \n",
      "Iteration: 2662. Loss: 1.1229701042175293 \n",
      "Iteration: 2663. Loss: 1.0917974710464478 \n",
      "Iteration: 2664. Loss: 1.0301305055618286 \n",
      "Iteration: 2665. Loss: 0.9646170735359192 \n",
      "Iteration: 2666. Loss: 1.1667507886886597 \n",
      "Iteration: 2667. Loss: 1.1102306842803955 \n",
      "Iteration: 2668. Loss: 1.098248839378357 \n",
      "Iteration: 2669. Loss: 1.0357985496520996 \n",
      "Iteration: 2670. Loss: 1.0162760019302368 \n",
      "Iteration: 2671. Loss: 1.0708932876586914 \n",
      "Iteration: 2672. Loss: 1.041195273399353 \n",
      "Iteration: 2673. Loss: 1.072221279144287 \n",
      "Iteration: 2674. Loss: 1.062817096710205 \n",
      "Iteration: 2675. Loss: 1.1715211868286133 \n",
      "Iteration: 2676. Loss: 1.0247055292129517 \n",
      "Iteration: 2677. Loss: 1.016344666481018 \n",
      "Iteration: 2678. Loss: 1.0776326656341553 \n",
      "Iteration: 2679. Loss: 1.0206069946289062 \n",
      "Iteration: 2680. Loss: 0.9738054871559143 \n",
      "Iteration: 2681. Loss: 1.0936315059661865 \n",
      "Iteration: 2682. Loss: 0.9547032713890076 \n",
      "Iteration: 2683. Loss: 1.0923341512680054 \n",
      "Iteration: 2684. Loss: 0.969424307346344 \n",
      "Iteration: 2685. Loss: 0.9963744878768921 \n",
      "Iteration: 2686. Loss: 1.040004849433899 \n",
      "Iteration: 2687. Loss: 1.0122594833374023 \n",
      "Iteration: 2688. Loss: 1.0438485145568848 \n",
      "Iteration: 2689. Loss: 1.0569603443145752 \n",
      "Iteration: 2690. Loss: 0.991300642490387 \n",
      "Iteration: 2691. Loss: 1.036729335784912 \n",
      "Iteration: 2692. Loss: 1.1187469959259033 \n",
      "Iteration: 2693. Loss: 1.1021389961242676 \n",
      "Iteration: 2694. Loss: 1.0902142524719238 \n",
      "Iteration: 2695. Loss: 1.0668518543243408 \n",
      "Iteration: 2696. Loss: 1.0791642665863037 \n",
      "Iteration: 2697. Loss: 1.059564471244812 \n",
      "Iteration: 2698. Loss: 0.9888299703598022 \n",
      "Iteration: 2699. Loss: 1.1456371545791626 \n",
      "Iteration: 2700. Loss: 1.0135095119476318 \n",
      "Iteration: 2701. Loss: 1.0289379358291626 \n",
      "Iteration: 2702. Loss: 0.9766438007354736 \n",
      "Iteration: 2703. Loss: 1.0631835460662842 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2704. Loss: 0.9740576148033142 \n",
      "Iteration: 2705. Loss: 1.0762046575546265 \n",
      "Iteration: 2706. Loss: 0.9743858575820923 \n",
      "Iteration: 2707. Loss: 0.9778210520744324 \n",
      "Iteration: 2708. Loss: 0.9960623979568481 \n",
      "Iteration: 2709. Loss: 1.0872492790222168 \n",
      "Iteration: 2710. Loss: 1.0674779415130615 \n",
      "Iteration: 2711. Loss: 1.0158225297927856 \n",
      "Iteration: 2712. Loss: 0.9135119915008545 \n",
      "Iteration: 2713. Loss: 1.113731026649475 \n",
      "Iteration: 2714. Loss: 1.0838032960891724 \n",
      "Iteration: 2715. Loss: 1.0433903932571411 \n",
      "Iteration: 2716. Loss: 1.0294831991195679 \n",
      "Iteration: 2717. Loss: 0.9883319139480591 \n",
      "Iteration: 2718. Loss: 1.0577312707901 \n",
      "Iteration: 2719. Loss: 1.0574767589569092 \n",
      "Iteration: 2720. Loss: 1.1033614873886108 \n",
      "Iteration: 2721. Loss: 1.056644082069397 \n",
      "Iteration: 2722. Loss: 1.0745002031326294 \n",
      "Iteration: 2723. Loss: 0.9997316002845764 \n",
      "Iteration: 2724. Loss: 1.069656252861023 \n",
      "Iteration: 2725. Loss: 1.0849417448043823 \n",
      "Iteration: 2726. Loss: 1.0690412521362305 \n",
      "Iteration: 2727. Loss: 0.9648892879486084 \n",
      "Iteration: 2728. Loss: 0.9505318403244019 \n",
      "Iteration: 2729. Loss: 0.9815772175788879 \n",
      "Iteration: 2730. Loss: 0.9784104228019714 \n",
      "Iteration: 2731. Loss: 1.0139002799987793 \n",
      "Iteration: 2732. Loss: 0.929679811000824 \n",
      "Iteration: 2733. Loss: 0.993155837059021 \n",
      "Iteration: 2734. Loss: 1.0531511306762695 \n",
      "Iteration: 2735. Loss: 1.00247323513031 \n",
      "Iteration: 2736. Loss: 1.03217351436615 \n",
      "Iteration: 2737. Loss: 1.0371965169906616 \n",
      "Iteration: 2738. Loss: 1.080188274383545 \n",
      "Iteration: 2739. Loss: 1.0756313800811768 \n",
      "Iteration: 2740. Loss: 1.024827480316162 \n",
      "Iteration: 2741. Loss: 1.0084080696105957 \n",
      "Iteration: 2742. Loss: 1.066595196723938 \n",
      "Iteration: 2743. Loss: 0.9797405004501343 \n",
      "Iteration: 2744. Loss: 1.100571870803833 \n",
      "Iteration: 2745. Loss: 1.0872275829315186 \n",
      "Iteration: 2746. Loss: 1.1047122478485107 \n",
      "Iteration: 2747. Loss: 0.9564099907875061 \n",
      "Iteration: 2748. Loss: 1.0828099250793457 \n",
      "Iteration: 2749. Loss: 1.0284608602523804 \n",
      "Iteration: 2750. Loss: 1.0097371339797974 \n",
      "Iteration: 2751. Loss: 1.0948842763900757 \n",
      "Iteration: 2752. Loss: 1.0231621265411377 \n",
      "Iteration: 2753. Loss: 1.0609663724899292 \n",
      "Iteration: 2754. Loss: 1.0603299140930176 \n",
      "Iteration: 2755. Loss: 0.9994410872459412 \n",
      "Iteration: 2756. Loss: 1.0276128053665161 \n",
      "Iteration: 2757. Loss: 1.023849606513977 \n",
      "Iteration: 2758. Loss: 1.0542171001434326 \n",
      "Iteration: 2759. Loss: 1.0927958488464355 \n",
      "Iteration: 2760. Loss: 1.0170331001281738 \n",
      "Iteration: 2761. Loss: 1.0292493104934692 \n",
      "Iteration: 2762. Loss: 1.086746096611023 \n",
      "Iteration: 2763. Loss: 1.1131281852722168 \n",
      "Iteration: 2764. Loss: 1.0878424644470215 \n",
      "Iteration: 2765. Loss: 1.035313606262207 \n",
      "Iteration: 2766. Loss: 1.0182210206985474 \n",
      "Iteration: 2767. Loss: 1.0582677125930786 \n",
      "Iteration: 2768. Loss: 1.1023461818695068 \n",
      "Iteration: 2769. Loss: 1.0430384874343872 \n",
      "Iteration: 2770. Loss: 0.9782924652099609 \n",
      "Iteration: 2771. Loss: 1.046627163887024 \n",
      "Iteration: 2772. Loss: 1.0668681859970093 \n",
      "Iteration: 2773. Loss: 1.0303819179534912 \n",
      "Iteration: 2774. Loss: 1.0178277492523193 \n",
      "Iteration: 2775. Loss: 1.0174556970596313 \n",
      "Iteration: 2776. Loss: 1.1396170854568481 \n",
      "Iteration: 2777. Loss: 1.0781893730163574 \n",
      "Iteration: 2778. Loss: 1.1035631895065308 \n",
      "Iteration: 2779. Loss: 1.0009503364562988 \n",
      "Iteration: 2780. Loss: 1.051664113998413 \n",
      "Iteration: 2781. Loss: 1.0020736455917358 \n",
      "Iteration: 2782. Loss: 1.0422437191009521 \n",
      "Iteration: 2783. Loss: 0.9787709712982178 \n",
      "Iteration: 2784. Loss: 1.053512692451477 \n",
      "Iteration: 2785. Loss: 0.9553269743919373 \n",
      "Iteration: 2786. Loss: 1.0362305641174316 \n",
      "Iteration: 2787. Loss: 0.9892436265945435 \n",
      "Iteration: 2788. Loss: 0.9769611954689026 \n",
      "Iteration: 2789. Loss: 1.0475267171859741 \n",
      "Iteration: 2790. Loss: 0.9350135922431946 \n",
      "Iteration: 2791. Loss: 1.041465401649475 \n",
      "Iteration: 2792. Loss: 1.0575201511383057 \n",
      "Iteration: 2793. Loss: 1.0929367542266846 \n",
      "Iteration: 2794. Loss: 0.9828929305076599 \n",
      "Iteration: 2795. Loss: 0.9513734579086304 \n",
      "Iteration: 2796. Loss: 1.0611159801483154 \n",
      "Iteration: 2797. Loss: 1.1176049709320068 \n",
      "Iteration: 2798. Loss: 1.024598240852356 \n",
      "Iteration: 2799. Loss: 1.0041693449020386 \n",
      "Iteration: 2800. Loss: 1.0128769874572754 \n",
      "Iteration: 2801. Loss: 0.9248526692390442 \n",
      "Iteration: 2802. Loss: 1.0828514099121094 \n",
      "Iteration: 2803. Loss: 1.0640448331832886 \n",
      "Iteration: 2804. Loss: 1.1207876205444336 \n",
      "Iteration: 2805. Loss: 1.0044562816619873 \n",
      "Iteration: 2806. Loss: 0.9539940357208252 \n",
      "Iteration: 2807. Loss: 0.9982725381851196 \n",
      "Iteration: 2808. Loss: 0.9534636735916138 \n",
      "Iteration: 2809. Loss: 1.0101900100708008 \n",
      "Iteration: 2810. Loss: 0.9955407977104187 \n",
      "Iteration: 2811. Loss: 1.0781285762786865 \n",
      "Iteration: 2812. Loss: 1.042154312133789 \n",
      "Iteration: 2813. Loss: 1.0620173215866089 \n",
      "Iteration: 2814. Loss: 1.0666141510009766 \n",
      "Iteration: 2815. Loss: 1.0263699293136597 \n",
      "Iteration: 2816. Loss: 0.9750792980194092 \n",
      "Iteration: 2817. Loss: 0.9232481122016907 \n",
      "Iteration: 2818. Loss: 1.1054714918136597 \n",
      "Iteration: 2819. Loss: 0.9923403263092041 \n",
      "Iteration: 2820. Loss: 0.940014660358429 \n",
      "Iteration: 2821. Loss: 1.005213975906372 \n",
      "Iteration: 2822. Loss: 1.0797526836395264 \n",
      "Iteration: 2823. Loss: 0.9657751321792603 \n",
      "Iteration: 2824. Loss: 1.026975154876709 \n",
      "Iteration: 2825. Loss: 1.0204499959945679 \n",
      "Iteration: 2826. Loss: 0.9922226667404175 \n",
      "Iteration: 2827. Loss: 1.0466409921646118 \n",
      "Iteration: 2828. Loss: 1.0495500564575195 \n",
      "Iteration: 2829. Loss: 0.9566800594329834 \n",
      "Iteration: 2830. Loss: 0.9971484541893005 \n",
      "Iteration: 2831. Loss: 0.9441819787025452 \n",
      "Iteration: 2832. Loss: 1.0957390069961548 \n",
      "Iteration: 2833. Loss: 1.024844765663147 \n",
      "Iteration: 2834. Loss: 1.0494039058685303 \n",
      "Iteration: 2835. Loss: 1.0488593578338623 \n",
      "Iteration: 2836. Loss: 1.0090805292129517 \n",
      "Iteration: 2837. Loss: 0.8524919152259827 \n",
      "Iteration: 2838. Loss: 1.0416350364685059 \n",
      "Iteration: 2839. Loss: 1.0005860328674316 \n",
      "Iteration: 2840. Loss: 1.0155155658721924 \n",
      "Iteration: 2841. Loss: 1.0788006782531738 \n",
      "Iteration: 2842. Loss: 1.0435587167739868 \n",
      "Iteration: 2843. Loss: 1.0499076843261719 \n",
      "Iteration: 2844. Loss: 1.002395749092102 \n",
      "Iteration: 2845. Loss: 1.094645380973816 \n",
      "Iteration: 2846. Loss: 1.0774269104003906 \n",
      "Iteration: 2847. Loss: 1.106429100036621 \n",
      "Iteration: 2848. Loss: 1.0850422382354736 \n",
      "Iteration: 2849. Loss: 1.026598334312439 \n",
      "Iteration: 2850. Loss: 1.0757219791412354 \n",
      "Iteration: 2851. Loss: 1.0160592794418335 \n",
      "Iteration: 2852. Loss: 1.0028581619262695 \n",
      "Iteration: 2853. Loss: 0.8984503149986267 \n",
      "Iteration: 2854. Loss: 0.9848125576972961 \n",
      "Iteration: 2855. Loss: 1.009756088256836 \n",
      "Iteration: 2856. Loss: 1.0985482931137085 \n",
      "Iteration: 2857. Loss: 1.0524364709854126 \n",
      "Iteration: 2858. Loss: 0.995622992515564 \n",
      "Iteration: 2859. Loss: 1.008008599281311 \n",
      "Iteration: 2860. Loss: 0.9942739009857178 \n",
      "Iteration: 2861. Loss: 1.0235912799835205 \n",
      "Iteration: 2862. Loss: 0.8866434693336487 \n",
      "Iteration: 2863. Loss: 1.140639066696167 \n",
      "Iteration: 2864. Loss: 0.9588462710380554 \n",
      "Iteration: 2865. Loss: 1.0327215194702148 \n",
      "Iteration: 2866. Loss: 1.0023432970046997 \n",
      "Iteration: 2867. Loss: 1.0285768508911133 \n",
      "Iteration: 2868. Loss: 0.9626528024673462 \n",
      "Iteration: 2869. Loss: 1.0130376815795898 \n",
      "Iteration: 2870. Loss: 0.9053637981414795 \n",
      "Iteration: 2871. Loss: 1.005419135093689 \n",
      "Iteration: 2872. Loss: 1.1273454427719116 \n",
      "Iteration: 2873. Loss: 1.0111290216445923 \n",
      "Iteration: 2874. Loss: 0.9970142841339111 \n",
      "Iteration: 2875. Loss: 1.088283658027649 \n",
      "Iteration: 2876. Loss: 1.0439634323120117 \n",
      "Iteration: 2877. Loss: 1.0778447389602661 \n",
      "Iteration: 2878. Loss: 1.016161561012268 \n",
      "Iteration: 2879. Loss: 1.0239630937576294 \n",
      "Iteration: 2880. Loss: 1.0758157968521118 \n",
      "Iteration: 2881. Loss: 1.0392223596572876 \n",
      "Iteration: 2882. Loss: 1.0254912376403809 \n",
      "Iteration: 2883. Loss: 1.0291839838027954 \n",
      "Iteration: 2884. Loss: 0.9229385256767273 \n",
      "Iteration: 2885. Loss: 0.9659401774406433 \n",
      "Iteration: 2886. Loss: 1.041314721107483 \n",
      "Iteration: 2887. Loss: 1.0564600229263306 \n",
      "Iteration: 2888. Loss: 1.0734220743179321 \n",
      "Iteration: 2889. Loss: 1.0160785913467407 \n",
      "Iteration: 2890. Loss: 1.0029841661453247 \n",
      "Iteration: 2891. Loss: 1.015152931213379 \n",
      "Iteration: 2892. Loss: 1.0557724237442017 \n",
      "Iteration: 2893. Loss: 1.0342501401901245 \n",
      "Iteration: 2894. Loss: 1.0420165061950684 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2895. Loss: 0.9911949038505554 \n",
      "Iteration: 2896. Loss: 1.037811040878296 \n",
      "Iteration: 2897. Loss: 1.0460666418075562 \n",
      "Iteration: 2898. Loss: 1.0485389232635498 \n",
      "Iteration: 2899. Loss: 1.088282585144043 \n",
      "Iteration: 2900. Loss: 1.0013628005981445 \n",
      "Iteration: 2901. Loss: 0.9849428534507751 \n",
      "Iteration: 2902. Loss: 0.9544310569763184 \n",
      "Iteration: 2903. Loss: 0.9145511388778687 \n",
      "Iteration: 2904. Loss: 1.0191256999969482 \n",
      "Iteration: 2905. Loss: 1.0095608234405518 \n",
      "Iteration: 2906. Loss: 1.0220041275024414 \n",
      "Iteration: 2907. Loss: 1.03559148311615 \n",
      "Iteration: 2908. Loss: 0.9865292310714722 \n",
      "Iteration: 2909. Loss: 0.9859837293624878 \n",
      "Iteration: 2910. Loss: 0.9048222899436951 \n",
      "Iteration: 2911. Loss: 0.9460918307304382 \n",
      "Iteration: 2912. Loss: 1.0421686172485352 \n",
      "Iteration: 2913. Loss: 1.032609462738037 \n",
      "Iteration: 2914. Loss: 1.023965835571289 \n",
      "Iteration: 2915. Loss: 1.0541398525238037 \n",
      "Iteration: 2916. Loss: 1.0694546699523926 \n",
      "Iteration: 2917. Loss: 0.9314136505126953 \n",
      "Iteration: 2918. Loss: 1.0019426345825195 \n",
      "Iteration: 2919. Loss: 1.0293747186660767 \n",
      "Iteration: 2920. Loss: 0.938212513923645 \n",
      "Iteration: 2921. Loss: 1.1174055337905884 \n",
      "Iteration: 2922. Loss: 0.9471441507339478 \n",
      "Iteration: 2923. Loss: 0.943223774433136 \n",
      "Iteration: 2924. Loss: 1.0020627975463867 \n",
      "Iteration: 2925. Loss: 1.0752928256988525 \n",
      "Iteration: 2926. Loss: 0.9351904392242432 \n",
      "Iteration: 2927. Loss: 1.0658931732177734 \n",
      "Iteration: 2928. Loss: 0.978550136089325 \n",
      "Iteration: 2929. Loss: 0.997799813747406 \n",
      "Iteration: 2930. Loss: 0.9398958086967468 \n",
      "Iteration: 2931. Loss: 0.9957995414733887 \n",
      "Iteration: 2932. Loss: 1.1650550365447998 \n",
      "Iteration: 2933. Loss: 1.0091807842254639 \n",
      "Iteration: 2934. Loss: 0.9113505482673645 \n",
      "Iteration: 2935. Loss: 1.0162264108657837 \n",
      "Iteration: 2936. Loss: 1.064344048500061 \n",
      "Iteration: 2937. Loss: 1.0067144632339478 \n",
      "Iteration: 2938. Loss: 0.9757459163665771 \n",
      "Iteration: 2939. Loss: 0.9574966430664062 \n",
      "Iteration: 2940. Loss: 1.0217455625534058 \n",
      "Iteration: 2941. Loss: 0.9882379770278931 \n",
      "Iteration: 2942. Loss: 1.062705636024475 \n",
      "Iteration: 2943. Loss: 1.0800701379776 \n",
      "Iteration: 2944. Loss: 1.0453953742980957 \n",
      "Iteration: 2945. Loss: 1.0566933155059814 \n",
      "Iteration: 2946. Loss: 1.1475533246994019 \n",
      "Iteration: 2947. Loss: 1.0794299840927124 \n",
      "Iteration: 2948. Loss: 1.0674551725387573 \n",
      "Iteration: 2949. Loss: 1.0688546895980835 \n",
      "Iteration: 2950. Loss: 1.020965814590454 \n",
      "Iteration: 2951. Loss: 1.0541534423828125 \n",
      "Iteration: 2952. Loss: 0.9357064962387085 \n",
      "Iteration: 2953. Loss: 1.1253137588500977 \n",
      "Iteration: 2954. Loss: 0.991367518901825 \n",
      "Iteration: 2955. Loss: 1.0724499225616455 \n",
      "Iteration: 2956. Loss: 1.0226373672485352 \n",
      "Iteration: 2957. Loss: 0.9152873158454895 \n",
      "Iteration: 2958. Loss: 1.0300424098968506 \n",
      "Iteration: 2959. Loss: 1.0432029962539673 \n",
      "Iteration: 2960. Loss: 0.9491870999336243 \n",
      "Iteration: 2961. Loss: 0.9771090745925903 \n",
      "Iteration: 2962. Loss: 1.0166730880737305 \n",
      "Iteration: 2963. Loss: 1.0159343481063843 \n",
      "Iteration: 2964. Loss: 0.9762299060821533 \n",
      "Iteration: 2965. Loss: 0.9645578265190125 \n",
      "Iteration: 2966. Loss: 1.014162540435791 \n",
      "Iteration: 2967. Loss: 1.0109103918075562 \n",
      "Iteration: 2968. Loss: 0.9687080383300781 \n",
      "Iteration: 2969. Loss: 0.950175940990448 \n",
      "Iteration: 2970. Loss: 1.0541247129440308 \n",
      "Iteration: 2971. Loss: 1.028801441192627 \n",
      "Iteration: 2972. Loss: 0.9425433874130249 \n",
      "Iteration: 2973. Loss: 0.9438675045967102 \n",
      "Iteration: 2974. Loss: 1.1160058975219727 \n",
      "Iteration: 2975. Loss: 0.9506348371505737 \n",
      "Iteration: 2976. Loss: 1.016400694847107 \n",
      "Iteration: 2977. Loss: 1.0422228574752808 \n",
      "Iteration: 2978. Loss: 1.066275954246521 \n",
      "Iteration: 2979. Loss: 0.9883954524993896 \n",
      "Iteration: 2980. Loss: 1.0047458410263062 \n",
      "Iteration: 2981. Loss: 1.1379345655441284 \n",
      "Iteration: 2982. Loss: 1.0379656553268433 \n",
      "Iteration: 2983. Loss: 0.9262454509735107 \n",
      "Iteration: 2984. Loss: 0.9485732913017273 \n",
      "Iteration: 2985. Loss: 0.9314708113670349 \n",
      "Iteration: 2986. Loss: 0.9559239149093628 \n",
      "Iteration: 2987. Loss: 0.9957348108291626 \n",
      "Iteration: 2988. Loss: 1.030139446258545 \n",
      "Iteration: 2989. Loss: 1.0334863662719727 \n",
      "Iteration: 2990. Loss: 0.9513362050056458 \n",
      "Iteration: 2991. Loss: 0.9818509817123413 \n",
      "Iteration: 2992. Loss: 1.0897777080535889 \n",
      "Iteration: 2993. Loss: 1.0400937795639038 \n",
      "Iteration: 2994. Loss: 0.9981691837310791 \n",
      "Iteration: 2995. Loss: 1.0271563529968262 \n",
      "Iteration: 2996. Loss: 1.0168118476867676 \n",
      "Iteration: 2997. Loss: 0.9635119438171387 \n",
      "Iteration: 2998. Loss: 1.0114527940750122 \n",
      "Iteration: 2999. Loss: 1.0058597326278687 \n",
      "Iteration: 3000. Loss: 0.9821301102638245 \n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "    \n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "    \n",
    "        \n",
    "        print('Iteration: {}. Loss: {} '.format(iter, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Iteration: 3000. Loss: 0.9821301102638245. Accuracy: 82\n"
     ]
    }
   ],
   "source": [
    "if iter %500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            # Print Loss\n",
    "            print('%Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking Down Accuracy Calcuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS\n",
      "tensor([[-0.1717, -1.2213, -0.3821, -0.1883,  0.0448, -0.3143, -1.0629,  2.7617,\n",
      "         -0.2771,  0.8775],\n",
      "        [ 0.4868, -0.1940,  1.6384,  1.0128, -1.8128,  0.6289,  1.0039, -1.6653,\n",
      "          0.3115, -1.4686],\n",
      "        [-0.8929,  2.3837,  0.2061, -0.0300, -0.5618, -0.3812, -0.3065, -0.4057,\n",
      "          0.2074, -0.3857],\n",
      "        [ 2.9092, -2.5993, -0.2225, -0.1534, -0.9367,  0.5637,  1.3026,  0.1874,\n",
      "         -0.4836, -0.2553],\n",
      "        [-0.2503, -1.9132,  0.7318, -0.6636,  1.8015, -0.3524,  0.0291,  0.4346,\n",
      "         -0.0912,  0.8104],\n",
      "        [-1.3590,  2.8067,  0.1504, -0.0048, -0.7242, -0.4630, -0.8147, -0.2668,\n",
      "          0.4203, -0.2874],\n",
      "        [-1.0666, -1.1396, -0.5288,  0.1495,  1.4067,  0.2316, -0.5936,  0.6970,\n",
      "          0.4676,  0.9475],\n",
      "        [-1.2912, -0.3283, -0.6051, -0.0755,  0.6978,  0.1998,  0.2372, -0.2120,\n",
      "          0.1070,  1.1980],\n",
      "        [ 0.0641, -0.5573,  0.9223, -1.4121,  0.7610,  0.0903,  0.4962, -0.9456,\n",
      "         -0.1065,  0.0275],\n",
      "        [-0.4448, -0.8024, -1.1345, -1.1652,  1.0376, -0.1897, -0.3415,  1.5298,\n",
      "         -0.0918,  1.4741],\n",
      "        [ 3.1407, -1.7074,  0.5249,  1.0684, -0.8436,  1.1299, -0.7621, -1.5085,\n",
      "          0.4945, -1.8566],\n",
      "        [ 0.8661, -0.4613,  0.4940, -0.0229,  0.1014, -0.4486,  0.6661, -0.8690,\n",
      "          0.0908, -0.4171],\n",
      "        [-0.8776, -1.5263, -0.8764, -1.0150,  1.5471, -0.3088, -0.2270,  1.0555,\n",
      "          0.2462,  2.0377],\n",
      "        [ 2.9795, -2.4956, -0.2533, -0.1998, -0.5507,  0.6814, -0.2199, -0.5336,\n",
      "          0.7920, -0.0256],\n",
      "        [-1.2504,  2.8041, -0.1211,  0.4450, -1.0842, -0.3445,  0.0620, -0.0597,\n",
      "          0.3976, -0.2075],\n",
      "        [ 0.6197, -0.7087, -0.0228,  1.3891, -0.3760,  0.8598, -0.5733, -0.4843,\n",
      "          0.3247, -0.8121],\n",
      "        [-0.6244, -2.1187,  0.2655, -0.6907,  1.4301, -0.7032, -0.2509,  0.9987,\n",
      "          0.2038,  1.6393],\n",
      "        [ 0.3022, -1.6722, -0.6127,  0.3990, -0.2791, -0.4069, -0.7238,  2.7278,\n",
      "         -0.5172,  0.5500],\n",
      "        [-0.5589, -0.2399,  0.0531,  0.8991, -0.6687,  0.4832,  0.6795, -0.3991,\n",
      "          0.0655, -0.2928],\n",
      "        [-1.0204, -1.3609, -0.4508, -0.0991,  1.8984,  0.0656, -0.2114,  0.1145,\n",
      "          0.0236,  1.4263],\n",
      "        [-0.8114, -0.3853, -1.4093, -0.0957,  0.6106,  0.2957, -1.1912,  1.5711,\n",
      "          0.2368,  1.3719],\n",
      "        [-0.5970, -1.4440,  0.0603,  0.2652,  0.2479,  0.5104,  2.1985, -1.4369,\n",
      "          0.0729, -0.0768],\n",
      "        [-0.4047,  0.0309,  0.3900, -0.8098,  0.8841, -1.2928,  1.0913, -0.0531,\n",
      "         -0.2211, -0.0126],\n",
      "        [-0.0423, -0.9976, -0.6100,  0.2502, -0.0104,  1.3782,  0.1782, -1.1472,\n",
      "          0.8540,  0.0619],\n",
      "        [-0.6628, -1.1251,  0.1600, -0.1264,  1.4618, -0.1252, -0.1086,  0.3507,\n",
      "         -0.2806,  1.0499],\n",
      "        [ 4.1296, -2.9326,  0.4496, -1.2362,  0.0215,  0.7982,  0.7997, -1.1705,\n",
      "         -0.3549, -1.3703],\n",
      "        [-0.2730, -1.3075, -0.2987,  0.2180,  0.2843, -0.0337, -0.5617,  1.8723,\n",
      "         -0.5558,  1.2798],\n",
      "        [-0.4533, -2.2077, -0.0843, -0.6061,  2.1611,  0.0649,  0.1001,  0.0501,\n",
      "          0.0856,  1.4573],\n",
      "        [ 2.6818, -2.5234,  0.1318,  1.0874, -1.2829,  0.8740, -0.2484, -0.5274,\n",
      "          0.8270, -0.7398],\n",
      "        [-1.1154,  1.4630, -0.0633,  0.2330, -0.5530,  0.0856,  0.3216,  0.0339,\n",
      "          0.3855, -0.2526],\n",
      "        [-0.8979, -0.2251, -0.8551,  2.2385, -1.1105,  0.8759, -0.3590,  0.5246,\n",
      "          0.2874, -0.0803],\n",
      "        [-1.0080,  1.2951, -0.2111,  0.3938, -0.4425,  0.1027, -0.0051,  0.1965,\n",
      "          0.3204,  0.1392],\n",
      "        [-1.0144, -0.8344, -0.3038,  2.5994, -0.4535,  1.4197, -0.4871, -0.8347,\n",
      "          0.5569, -0.1595],\n",
      "        [ 1.7002, -1.7602,  0.6704, -1.6012,  0.8474,  0.0583,  1.4714, -1.0767,\n",
      "         -0.4471, -0.5428],\n",
      "        [-0.8737, -0.2245,  0.8700, -0.4337, -0.0469, -0.4146, -1.4831,  1.7240,\n",
      "          0.6925,  0.4005],\n",
      "        [ 0.5083, -0.9153,  2.6042,  0.2567, -0.8742,  0.4992,  0.2308, -0.4484,\n",
      "          0.2294, -1.5306],\n",
      "        [-0.7434, -1.6652,  0.0377, -0.0580,  0.0620, -0.4863, -0.5551,  2.2086,\n",
      "         -0.3068,  1.1592],\n",
      "        [-1.3974,  2.2143, -0.2442,  0.1562, -0.6769, -0.1570,  0.0840,  0.2309,\n",
      "          0.4781,  0.0041],\n",
      "        [ 0.2488,  0.5400,  0.6594,  1.2126, -1.8753,  0.3475,  0.3781, -1.2005,\n",
      "          0.6801, -1.5389],\n",
      "        [-1.3362,  2.8342, -0.2851,  0.2834, -1.3088, -0.3130, -0.0262, -0.5122,\n",
      "          0.6227, -0.3149],\n",
      "        [-0.6392,  1.7179,  0.0320,  0.0269, -0.5729, -0.1494,  0.0876, -0.1108,\n",
      "          0.1798, -0.1079],\n",
      "        [-0.8474, -0.9798, -0.1682, -0.3962,  0.0827, -0.2294, -0.3492,  2.1818,\n",
      "         -0.3014,  1.4139],\n",
      "        [-2.1530, -0.3049, -0.2927, -0.3688,  2.1202, -0.7449, -0.8608,  0.6247,\n",
      "          0.3937,  1.5772],\n",
      "        [-0.2971,  0.7684,  1.1440, -0.2199, -0.1267, -0.4406,  0.2047, -1.0529,\n",
      "          0.3047, -0.6279],\n",
      "        [-1.1237,  0.0931,  0.2275,  1.4838, -0.8060,  0.5814,  0.4745,  0.0289,\n",
      "          0.0223, -0.2503],\n",
      "        [ 0.2181, -1.3227, -0.5118,  1.6701, -0.5398,  1.4950, -0.0971, -1.1741,\n",
      "          0.9180, -0.3541],\n",
      "        [-1.5893,  0.3678,  0.2688,  0.9401, -0.1995,  0.4596,  0.0795,  0.3062,\n",
      "          0.3294,  0.2681],\n",
      "        [-0.6692, -0.4105,  1.2875, -0.5312,  0.5810, -0.4842,  0.6844, -0.3050,\n",
      "         -0.1393,  0.1132],\n",
      "        [-1.3165, -2.6765, -1.2136, -0.0089,  2.6631,  0.3889, -0.8003,  0.3201,\n",
      "          0.7407,  2.3065],\n",
      "        [-0.3474, -2.1524,  0.6489, -0.5273,  2.3369, -0.7797,  0.1807,  0.4281,\n",
      "         -0.2390,  1.0403],\n",
      "        [-0.2002, -1.1053,  0.2615,  0.2825, -0.1584,  0.4942,  2.0573, -1.0850,\n",
      "          0.0001, -0.2865],\n",
      "        [ 0.0624, -1.0362, -0.4853,  1.6426, -0.7442,  0.6444, -0.0812, -0.2886,\n",
      "         -0.0760,  0.1360],\n",
      "        [ 0.3208, -1.1817, -1.2313,  0.1021,  0.7921,  1.2512, -0.0562, -0.4542,\n",
      "         -0.0731,  0.2554],\n",
      "        [ 0.4992, -0.8724, -0.3205,  1.0834,  0.0432,  0.6882, -0.2072, -0.6709,\n",
      "          0.3523, -0.2977],\n",
      "        [ 0.0819, -0.6495,  1.3102, -0.6148,  0.6014, -0.6280,  0.6140, -0.5579,\n",
      "          0.0271, -0.0142],\n",
      "        [ 1.4206, -2.4726, -0.4857,  0.5626, -0.7576,  0.9538,  0.4018, -0.7979,\n",
      "          1.3995, -0.2142],\n",
      "        [-0.0492, -2.7653,  0.2046, -0.5200,  2.4539, -0.0845,  0.1870, -0.2458,\n",
      "         -0.0406,  0.9643],\n",
      "        [-0.8370,  2.3450,  0.0559,  0.1460, -0.8006, -0.3916, -0.5508, -0.3409,\n",
      "          0.2519, -0.2608],\n",
      "        [-0.4666, -2.0613, -0.7708, -0.6630,  1.6866, -0.3850, -0.1239,  1.0846,\n",
      "         -0.4005,  2.0088],\n",
      "        [ 0.0751,  0.2304, -0.6990, -0.4189,  0.2807,  0.2865, -0.2550,  0.3121,\n",
      "          0.1197, -0.3702],\n",
      "        [-0.2422, -1.6466, -0.7998,  0.5575, -0.1001,  0.0270, -0.2416,  2.1369,\n",
      "         -0.5302,  0.7432],\n",
      "        [ 0.3998, -0.9981,  1.0287, -1.5830, -0.3451,  0.0653,  0.6615, -0.6160,\n",
      "          1.1515,  0.1806],\n",
      "        [-0.9376, -0.8568, -0.0665, -0.2655,  0.6038,  0.1339, -0.0305, -0.2949,\n",
      "          0.4010,  0.8512],\n",
      "        [-0.8720, -0.0873,  1.1506,  0.8811, -0.3146, -0.0826, -0.3255, -0.7292,\n",
      "          0.4169,  0.2078],\n",
      "        [-1.0404, -0.5935,  0.2681, -0.6352,  0.6219, -0.3399, -0.5630,  1.5509,\n",
      "         -0.0379,  0.4804],\n",
      "        [-1.2189, -0.6080, -0.6364,  0.4096,  0.5093,  0.4679,  0.0688, -0.3548,\n",
      "          0.2906,  0.8272],\n",
      "        [ 0.4673, -0.2531,  0.7487, -0.1024,  0.3614, -0.6165,  0.6888,  0.2991,\n",
      "         -0.2257, -0.3678],\n",
      "        [-0.6902, -1.2129,  0.5985, -0.9423,  2.2556, -0.7999, -0.3705,  0.5775,\n",
      "          0.1127,  0.6668],\n",
      "        [-1.4029, -0.5709, -0.2046,  2.7546, -0.4866,  0.9350, -1.0170, -0.7988,\n",
      "          1.1403,  0.0717],\n",
      "        [ 2.6024, -1.5113,  0.3930, -0.7268, -1.3181,  0.4522,  0.3124, -0.1501,\n",
      "         -0.4339, -0.4018],\n",
      "        [ 0.5105, -1.4645, -1.0946,  0.0157, -0.1654, -0.3371, -0.6051,  2.8413,\n",
      "         -0.3488,  0.4737],\n",
      "        [ 4.2964, -2.7148,  0.7035,  0.3142, -1.0031,  0.9873, -0.1155, -1.6627,\n",
      "          0.2422, -1.6495],\n",
      "        [ 1.4027, -1.1544,  2.0060,  1.3504, -1.2033,  0.3078,  0.1653, -0.8147,\n",
      "          0.1505, -1.8732],\n",
      "        [-1.2742,  0.8314,  0.3630, -0.2096, -0.8152, -0.1813, -0.9088,  0.9919,\n",
      "          1.1786,  0.5518],\n",
      "        [-1.2888,  2.4211, -0.3952,  0.0856, -0.9486, -0.0858,  0.1055,  0.1022,\n",
      "          0.6006, -0.1243],\n",
      "        [-1.8179,  0.5638, -0.5547, -0.5106,  0.9356, -0.6574, -0.4710,  1.8094,\n",
      "          0.1187,  0.8652],\n",
      "        [-0.3080,  0.1586, -0.1384,  2.5030, -0.8328,  0.9940, -0.4645, -1.1357,\n",
      "          0.4853, -0.9040],\n",
      "        [-0.6647, -0.2469,  0.6462, -0.7902, -0.1878, -0.4187,  0.0612,  1.6452,\n",
      "         -0.4091,  0.6830],\n",
      "        [-1.5188,  1.0395, -0.8214,  0.0567, -0.2169,  0.0393, -0.6671,  0.3690,\n",
      "          0.8439,  0.7470],\n",
      "        [-0.4702, -0.4181, -0.6683, -1.0929,  0.1025, -0.1249, -1.0682,  3.3568,\n",
      "          0.3926,  0.7826],\n",
      "        [-0.5219, -1.6973, -1.1057, -0.2157,  0.7710,  0.6945, -0.4579,  1.3161,\n",
      "         -0.5973,  1.7546],\n",
      "        [ 0.0710, -1.8444,  0.6202, -0.6205,  0.1873,  0.1284,  2.4885, -0.4570,\n",
      "         -0.2750,  0.1216],\n",
      "        [-0.6719, -1.0954,  3.9792,  0.0989,  0.0144, -0.8848,  0.6904, -0.7787,\n",
      "          0.3066, -1.2661],\n",
      "        [-0.6307, -1.6327, -0.5322, -0.0543,  0.7207,  0.0217, -1.0170,  2.0994,\n",
      "         -0.5383,  1.6863],\n",
      "        [-0.7689, -0.0944, -0.3422, -0.5647,  0.4980,  0.6254, -0.6393, -0.6814,\n",
      "          1.0659,  0.5573],\n",
      "        [-1.2116, -2.4150, -0.6632,  0.1409,  2.7871,  0.2707, -0.3529, -0.3939,\n",
      "          0.4487,  1.6118],\n",
      "        [-1.6905,  0.5356, -0.3779, -0.7953,  0.0421, -0.8041, -1.2114,  3.0319,\n",
      "          0.3127,  0.9088],\n",
      "        [ 0.0506, -1.4767, -0.7726,  1.9131, -0.3977,  1.1907,  0.7153, -0.2715,\n",
      "         -0.1837, -0.0115],\n",
      "        [-0.2341, -1.9087,  0.6353, -1.1616,  1.0659, -0.4233,  2.4676, -0.2424,\n",
      "         -0.7138,  0.1361],\n",
      "        [-1.6273,  2.9569,  0.4199, -0.0783, -0.7013, -0.4817, -0.3714, -0.4876,\n",
      "          0.4656, -0.5921],\n",
      "        [ 0.2904, -0.6242, -0.1096,  2.6985, -1.1583,  0.8994, -1.3593, -0.3714,\n",
      "          1.0084, -0.5890],\n",
      "        [-0.9193, -0.5952,  0.2472, -0.7177,  0.3525, -0.1786,  2.7163, -1.1673,\n",
      "          0.1052,  0.0564],\n",
      "        [-0.8885,  0.2205,  0.0653, -0.6210,  0.4264, -0.0707, -0.2265, -0.0979,\n",
      "          0.7127,  0.2850],\n",
      "        [-0.7923, -0.5563, -0.6513,  2.7676, -1.3001,  1.2613, -1.1231,  0.7160,\n",
      "          0.6721,  0.1586],\n",
      "        [-2.0801,  1.9923,  0.1468,  0.3185, -0.6964, -0.0002,  0.4347, -0.1284,\n",
      "          0.6715, -0.0762],\n",
      "        [-0.9678, -0.4376, -0.3019, -1.5180,  2.2595, -0.6468,  0.1264, -0.1300,\n",
      "         -0.0957,  1.1334],\n",
      "        [-1.0592,  0.6292, -0.2202,  0.4302, -0.2038,  0.2270, -0.0760,  0.2360,\n",
      "          0.3761,  0.2267],\n",
      "        [-1.8217,  1.2372, -0.6735, -0.0561, -0.2618, -0.3412, -0.0997,  0.7171,\n",
      "          0.0854,  0.5213],\n",
      "        [ 0.7455, -1.1690,  0.7739, -0.7935, -0.1152,  0.6503,  1.9150, -1.1481,\n",
      "         -0.0781, -0.5561],\n",
      "        [-1.2917, -1.9769,  0.1422, -0.6058,  1.3434, -0.8570, -0.3721,  1.0434,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0.1598,  2.2552]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0 \n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    iter_test += 1 \n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    if iter_test==1:\n",
    "        print('OUTPUTS')\n",
    "        print(outputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS\n",
      "tensor([-0.1717, -1.2213, -0.3821, -0.1883,  0.0448, -0.3143, -1.0629,  2.7617,\n",
      "        -0.2771,  0.8775], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0 \n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    iter_test += 1 \n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    if iter_test==1:\n",
    "        print('OUTPUTS')\n",
    "        print(outputs[0,:])\n",
    "    _, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0 \n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    iter_test += 1 \n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test==1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "iter_test = 0 \n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    iter_test += 1 \n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test==1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "tensor(7)\n",
      "LABEL SIZE \n",
      "torch.Size([100])\n",
      "LABEL FOR IMAGE 0 \n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# rechecking\n",
    "\n",
    "iter_test = 0 \n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    iter_test += 1 \n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test==1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted[0])\n",
    "        \n",
    "        print('LABEL SIZE ')\n",
    "        print(labels.size())\n",
    "        \n",
    "        print('LABEL FOR IMAGE 0 ')\n",
    "        print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "tensor(2)\n",
      "LABEL SIZE \n",
      "torch.Size([100])\n",
      "LABEL FOR IMAGE 0 \n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# ANOTHER IMAGE\n",
    "\n",
    "iter_test = 0 \n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    iter_test += 1 \n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if iter_test==1:\n",
    "        print('PREDICTION')\n",
    "        print(predicted[1])\n",
    "        \n",
    "        print('LABEL SIZE ')\n",
    "        print(labels.size())\n",
    "        \n",
    "        print('LABEL FOR IMAGE 0 ')\n",
    "        print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Saving a model\n",
    "\n",
    "save_model= False\n",
    "if save_model is True:\n",
    "    #saves_only parameters\n",
    "    torch.save(model.state_dict(),'logistic_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
